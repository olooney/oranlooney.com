---
title: 'ML From Scratch, Part 1: Linear Regression'
author: Oran Looney
date: 2018-11-29
slug: ml-from-scratch-part-1-linear-regression
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-1-linear-regression_files/lead.png
---



<p>To kick off this series, will start with something simple yet foundational: linear regression via <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a>.</p>
<p>While not exciting, linear regression finds widespread use both as a standalone learning algorithm and as a building block in more advanced learning algorithms. The output layer of a deep neural network trained for regression with MSE loss, simple AR time series models, and the “local regression” part of LOWESS smoothing are all examples of linear regression being used as an ingredient in a more sophisticated model.</p>
<p>Linear regression is also the “simple harmonic oscillator” of machine learning; that is to say, a pedagogical example that allows us to present deep theoretical ideas about machine learning in a context that is not too mathematically taxing.</p>
<p>There is also the small matter of it being the most widely used supervised learning algorithm in the world; although how much weight that carries I suppose depends on where you are on the “applied” to “theoretical” spectrum.</p>
<p>However, since I can already feel your eyes glazing over from such an introductory topic, we can spice things up a little bit by doing something which isn’t often done in introductory machine learning - we can present the algorithm that [your favorite statistical software here] <em>actually</em> uses to fit linear regression models: <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a>. It seems this is commonly glossed over because it involves more linear algebra than can be generally assumed, or perhaps because the exact solution we will derive doesn’t generalize well to other machine learning algorithms, not even closely related variants such as <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization">regularized regression</a> or <a href="https://en.wikipedia.org/wiki/Robust_regression">robust regression</a>.</p>
<p>The current paradigm in machine learning is to apply very powerful, very general optimization algorithms that work for a wide variety of models and scale reasonably well to vast amounts of data. This allows researchers to iterate rapidly on the structure of the model without needing to spend a lot of time coming up with a clever algorithm which solves their special case efficiently. It’s good software engineering; it avoids <a href="https://en.wikipedia.org/wiki/Program_optimization#When_to_optimize">premature optimization</a> and promotes good <a href="https://en.wikipedia.org/wiki/Separation_of_concerns">separation of concerns</a>. Still, history has shown that for any given optimization problem, there probably is a specialized algorithm that leverages the specifics of the problem to achieve an order of magnitude improvement in performance. For example, John Platt’s <a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization">Sequential Minimal Optimization</a> beat earlier, more general algorithms by such a wide margin that for a decade (1998-2009?) SVMs were one of the most promising approaches to machine learning. Today (2019), the machine learning industry is in a kind of “rapid prototyping” mode, leveraging the flexibility and composability of deep neural networks to experiment with endless numbers of novel models. However, as our understanding of which models work best for particular problems matures, the industry will likely tip back in favor of researching specialized algorithms. If we are interested in understanding machine learning from scratch we should be prepared to study specialized algorithms when and where they arise naturally.</p>
<p>And after all, what’s a little linear algebra between friends?</p>
<div id="statistical-motivation" class="section level2">
<h2>Statistical Motivation</h2>
<p>In this section we will use statistical considerations to motivate the definition of a particular mathematical optimization problem. Once we have posed this problem, we will afterwards ignore the statistics altogether and focus on numerical methods for solving the optimization problem.</p>
<p>Let’s start by deriving the so-called <a href="http://mathworld.wolfram.com/NormalEquation.html">normal equation</a> from a statistical model. Let’s say that <span class="math inline">\(X\)</span> is a random vector of length <span class="math inline">\(m\)</span> and <span class="math inline">\(Y\)</span> is a scalar random variable. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>not</em> independent, but have a joint probability distribution <span class="math inline">\(F(x, y; \Theta, \sigma)\)</span> parameterized by a non-random parameter vector <span class="math inline">\(\Theta\)</span>, a non-negative scalar <span class="math inline">\(\sigma\)</span>, and a random error term <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>. The model is:</p>
<p><span class="math display">\[ Y = X^T \Theta + \epsilon \]</span></p>
<p>Now suppose we sample <span class="math inline">\(n\)</span> independent observations from <span class="math inline">\(F\)</span>. We place these into a real-valued <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> and a real-valued vector <span class="math inline">\(\mathbf{y}\)</span>. Just to be absolutely clear, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are <em>not</em> random variables; they are the <a href="https://en.wikipedia.org/wiki/Realization_(probability)">given data used to fit the model</a>. We can then ask, what is the likelihood of obtaining the data <span class="math inline">\((\mathbf{X}, \mathbf{y})\)</span> given a parameter vector <span class="math inline">\(\Theta\)</span>? By rearranging our equation as <span class="math inline">\(Y - X\cdot\Theta = \epsilon \sim \mathcal{N}(0, \sigma^2)\)</span> and using the p.d.f. of the normal distribution, we can see that:</p>
<p><span class="math display">\[ \begin{align}
L(\Theta;\mathbf{X},\mathbf{y})
&amp; = P(\mathbf{X},\mathbf{y};\Theta) \\
&amp; = \prod_{i=1}^{n} P(\mathbf{X}_i,\mathbf{y}_i;\Theta) \\
&amp; = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma} \text{exp}\Big(\frac{-(\mathbf{y}_i - \mathbf{X}_i^T\Theta)^2}{2\sigma^2} \Big) \\
\end{align}
\]</span></p>
<p>That looks pretty awful, but there are a couple easy things we can do to make it a look a lot simpler. First, that constant term out front doesn’t matter at all: let’s just call it <span class="math inline">\(C\)</span> or something. We can also take that <span class="math inline">\(e^{-2\sigma^2}\)</span> outside the product as <span class="math inline">\(e^{-2N\sigma^2}\)</span>, which we’ll also stuff into the constant <span class="math inline">\(C\)</span> because we’re only interested in <span class="math inline">\(\Theta\)</span> right now. Finally, we can take a log to get rid of the exponential and turn the product into a sum. All together, we get the log-likelihood expression:</p>
<p><span class="math display">\[ \begin{align}
\ell(\Theta;\mathbf{X},\mathbf{y}) 
&amp; = \log L(\Theta;\mathbf{X},\mathbf{y}) \\
&amp; = C - \sum_{i=1}^N -(\mathbf{y}_i - \mathbf{X}^T_i\Theta)^2 \\
&amp; = C - \lVert\mathbf{y} - \mathbf{X}\Theta\rVert^2 \\
\end{align}
\]</span></p>
<p>Now, because log is a monotonically increasing function, maximizing <span class="math inline">\(\ell\)</span> is the same as maximizing <span class="math inline">\(L\)</span>. Furthermore, the constant <span class="math inline">\(C\)</span> has no effect whatsoever on the location of the maximum. We can also remove the minus sign and consider the problem as a minimization problem instead. Therefore our <a href="http://mathworld.wolfram.com/MaximumLikelihood.html">maximum likelihood estimate</a> of <span class="math inline">\(\Theta\)</span> for a given data set <span class="math inline">\((X, y)\)</span> is simply:</p>
<p><span class="math display">\[ \hat{\Theta} \triangleq \underset{\Theta}{\text{argmin}} \, \lVert\mathbf{y} - \mathbf{X}\Theta\rVert^2 \]</span></p>
<p>If statistics isn’t really your thing, I have some good news for you: we’re quits with it. Everything in this final equation is now a real-valued vector or matrix: there’s not a random variable or probability distribution in sight. It’s all over except for the linear algebra.</p>
<p>What we did above was essentially a short sketch of the relevant parts of the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem</a>. In particular, we’ve shown that the OLS solution to <span class="math inline">\(\mathbf{y} - \mathbf{X}\Theta\)</span> <em>is</em> the Maximum Likelihood Estimate for the parameters of the particular statistical model we started with. This isn’t general true, but it is <em>exactly</em> true for linear regression with a normally distributed error term. The full Gauss-Markov theorem proves a bunch of other nice properties: for example, it turns out this estimator is unbiased and we can even say that it’s optimal in the sense that it is the <em>best possible</em> linear unbiased estimator. But we won’t need these further theoretical results to implement a working model.</p>
<p>If there’s one thing you should remember, it’s this: the fact that the p.d.f. of the Gaussian distribution has the quadratic term <span class="math inline">\((x -\mu)^2\)</span> in the exponent is the <em>reason</em> why squared error is the “right” loss function for linear regression. If the error of our linear model had a different distribution, we’d have to <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">make a different choice</a>.</p>
<p>Our key takeaway is that if it’s true that our response variable is related to our predictor variables by a linear equation plus a certain amount of random Gaussian noise, then we can recover good, unbiased estimates of that linear equations coefficients from nothing more than a finite number of data points sampled from the underlying distribution, and the way to actually calculate those estimates is to solve the OLS problem for the data set.</p>
</div>
<div id="ordinary-least-squares" class="section level2">
<h2>Ordinary Least Squares</h2>
<p>Note: for this next section, we’re going to be doing some light vector calculus. I suggest you reference <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">the matrix cookbook</a> if any of the notation or concepts aren’t familiar.</p>
<p>Let’s call the right-hand side (the part we’re trying to minimize) <span class="math inline">\(J\)</span>. Then we have:</p>
<p><span class="math display">\[ J(\Theta) = \lVert \mathbf{y} - \mathbf{X}\Theta \rVert^2 \]</span></p>
<p>And the problem is to minimize <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(\Theta\)</span>. As optimization problems go, this one is pretty well behaved: it’s continuous, quadratic, convex, everywhere continuously differentiable, and unconstrained. That’s a fancy way of saying that it’s shaped like a big, round bowl. Because of these nice properties and a very useful set of theorems called the <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT conditions</a> we know that these properties guarentee that <span class="math inline">\(J\)</span> has a unique global minimum and that we can find the minimum - the bottom of the bowl - by finding the one place where the gradient is zero in all directions.</p>
<p>Now, it may not be obvious at first how to take the gradient of the squared norm of a vector, but recall that it is the inner product of that vector with its dual:</p>
<p><span class="math display">\[ \nabla_\Theta J = \nabla_\Theta \; (\mathbf{y} - \mathbf{X}\Theta)^T(\mathbf{y} - \mathbf{X}\Theta) \]</span></p>
<p>Expanding it out with FOIL:</p>
<p><span class="math display">\[ 
\nabla_\Theta J = \nabla_\Theta \; 
\mathbf{y}^T \mathbf{y} - (\mathbf{X}\Theta)^T \mathbf{y} - \mathbf{y}^T \mathbf{X}\Theta + \Theta^T (\mathbf{X}^T \mathbf{X}) \Theta \]</span></p>
<p>It’s obvious that <span class="math inline">\(\mathbf{y}^T \mathbf{y}\)</span> is constant with respect to <span class="math inline">\(\Theta\)</span> so the first term simply vanishes. It’s less obvious but also true that the next two terms are equal to each other - just remember that a J is a scalar, so those terms are each scalar, and the transpose of a scalar is itself. The final term is a quadratic form, and the <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/linearQuadraticGradients.pdf">general rule</a> is $ x^T A x = A^T x + A x $ but because the product of a matrix with itself is always symmetric (<span class="math inline">\(X^T X = (X^T X)^T\)</span>) we can use the simpler form <span class="math inline">\(\nabla x^T A x = 2 A x\)</span>.</p>
<p><span class="math display">\[ \nabla_\Theta J = - 2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T \mathbf{X} \Theta \]</span></p>
<p>Setting this equal to zero at the minimum, which we will call <span class="math inline">\(\hat{\Theta}\)</span>, and dividing both sides by two, we get:</p>
<p><span class="math display">\[ \mathbf{X}^T \mathbf{X} \hat{\Theta} = \mathbf{X}^T \mathbf{y} \tag{1} \]</span></p>
<p>This is the so-called <a href="http://mathworld.wolfram.com/NormalEquation.html">normal equation</a>. The importance of this step is that we’ve reduced the original optimization problem to a system of linear equations which may be solved purely by the methods of linear algegra. To see this, note that we know <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, so the right hand side is a known vector, the left-hand side is a matrix times an unknown vector, so this is just the familiar equation for solving for a particular solution to a system of equations <span class="math inline">\(Ax = b\)</span>.</p>
<p>Because <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> is square and non-singular and therefore invertible, we <em>could</em> just left-multiply both sides by its inverse to get an explicit closed form for <span class="math inline">\(\hat{\Theta}\)</span>:</p>
<p><span class="math display">\[ \hat{\Theta} = (\mathbf{X}^T \mathbf{X})^{-1} X^T \mathbf{y} \]</span></p>
<p>However, it turns out there is a faster and more numerically stable way of solving for <span class="math inline">\(\hat{\Theta}\)</span> which relies on the <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR Decomposition</a> of the matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div id="qr-decomposition" class="section level2">
<h2>QR Decomposition</h2>
<p>Since we’re going to be both <em>implementing</em> and <em>relying on</em> the QR decomposition in a minute, it’s worth making sure we understand how it works in detail. A QR decomposition of a matrix square <span class="math inline">\(A\)</span> is a product of an orthogonal matrix <span class="math inline">\(Q\)</span> and an upper-triangular matrix <span class="math inline">\(R\)</span> such that <span class="math inline">\(A = QR\)</span>. It always exists and there’s a reasonably performant algorithm for calculating it. Why is it beneficial to put a matrix in this form? In short, because it makes it very easy to compute solutions to systems of equations in matrix form <span class="math inline">\(Ax = b\)</span>; all we have to do is compute <span class="math inline">\(A = QR\)</span> and write the problem as <span class="math inline">\(R x = Q^{-1} b\)</span> which is easy to compute. Let’s examine those two steps in more detail.</p>
<p>Why is <span class="math inline">\(Q\)</span> easy to invert? Recall that <span class="math inline">\(Q\)</span> is orthogonal which implies that <span class="math inline">\(Q^{-1} = Q^T\)</span>. Most linear algebra libraries don’t even have to explicitly copy a matrix to take a transpose but simply set a flag that indicates that from now on it will operate on it row-wise instead of column-wise or vice versa. That means taking a transpose is free for all intents and purposes.</p>
<p>Why is <span class="math inline">\(Rx = Q^T b\)</span> easy to solve? Well, the right-hand side is just a vector. R is upper triangular, so we can solve this with a technique called back-substitution. Back-substitution is easiest to explain with an example. Consider this system of equations in matrix form, where the matrix is upper-triangular:</p>
<p><span class="math display">\[
 \begin{bmatrix}
   2 &amp; 1 &amp; 3 \\\
   0 &amp; 1 &amp; 1 \\
   0 &amp; 0 &amp; 4 \\
  \end{bmatrix}
 \begin{bmatrix}
   x_1 \\
   x_2 \\
   x_3 \\
  \end{bmatrix}
  =
 \begin{bmatrix}
   2 \\
   2 \\
   8 \\
  \end{bmatrix}
\]</span></p>
<p>We start on the bottom row, which is simply an equation <span class="math inline">\(4x_3 = 8\)</span>, so <span class="math inline">\(x_3 = 2\)</span>. The second row represents the equation <span class="math inline">\(x_2 + x_3 =2\)</span>, but we already know <span class="math inline">\(x_3\)</span>, so we can substitute that back in to get <span class="math inline">\(x_2 - 2 = 0\)</span>, so <span class="math inline">\(x_2 = 0\)</span>. The top row is <span class="math inline">\(2x_1 + x_2 + 3_x3 = 2x_1 + 6 = 2\)</span>, so <span class="math inline">\(x_1 = -2\)</span>. This is back-substitution, and it should be clear that we can do this quickly and efficiently for an upper-triangular matrix of any size. Furthermore, because we do at most one division per row, this method is very numerically stable. (If the matrix is <a href="https://en.wikipedia.org/wiki/Condition_number">ill-conditioned</a>, we could still run into numerical error, but this only occurs when the original data set <span class="math inline">\(X\)</span> suffers from <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a>.)</p>
<p>So hopefully you’re convinced by now that the <span class="math inline">\(QR\)</span> form is desirable. But how do we calculate <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>? There are two parts to understanding the algorithm. First, note that the product of any two orthogonal matrices is itself orthogonal. Also, the identity matrix is orthogonal. Therefore, if we have a candidate decomposition <span class="math inline">\(A = QR\)</span> where <span class="math inline">\(Q\)</span> is orthogonal (but R may not yet be square), then for any orthogonal matrix <span class="math inline">\(S\)</span> we have <span class="math inline">\(A = Q I R = Q S^T S R = (Q S^T) (S R) = Q&#39; R&#39;\)</span> where <span class="math inline">\(Q&#39; = Q S^T\)</span> and <span class="math inline">\(R&#39; = S R\)</span> is <em>also</em> a candidate decomposition! This is the general strategy behind not just QR decomposition, but behind many other decompositions in linear algebra: at each step we want to apply an orthogonal transformation designed to bring <span class="math inline">\(R\)</span> closer to the desired form, while simultaneously keeping track of all the transformations applied so far in a single matrix <span class="math inline">\(Q\)</span>.</p>
<p>That sets the rules and the goal of the game: we can apply any sequence of orthogonal transforms to a (square, non-singular) matrix <span class="math inline">\(A\)</span> that will bring it into upper triangular form. But what orthogonal transform will do that?</p>
</div>
<div id="householder-reflections" class="section level2">
<h2>Householder Reflections</h2>
<p>Let’s break it down into an even easier problem first. How would I make <em>just one column</em> of <span class="math inline">\(A\)</span> zero below the diagonal? Or even more concretely, how would I make just the <em>first column</em> of <span class="math inline">\(A\)</span> zero except for the first element?</p>
<p>Let’s take a look at the “column view” of our matrix. It looks like this:</p>
<p><span class="math display">\[
\begin{bmatrix}
    \vert &amp; \vert &amp; \vert \\
    a_1   &amp; a_2 &amp; a_3 \\
    \vert &amp; \vert &amp; \vert
\end{bmatrix}
\]</span></p>
<p>We want <span class="math inline">\(a_1\)</span> to be zero except for the first element. What does that <em>mean</em>? Let’s call our basis vectors <span class="math inline">\(e_1 = [1\, 0\, 0]^T\)</span>, <span class="math inline">\(e_2 = [0\, 1\, 0]^T\)</span>, <span class="math inline">\(e_3 = [0\, 0\, 1]^T\)</span>. Every vector in our space is a linear combination of these basis vectors. So what it means for <span class="math inline">\(a_1\)</span> to be zero except for the first element is that <span class="math inline">\(a_1\)</span> is co-linear (in the same line) as <span class="math inline">\(e_1\)</span>: $ H a_i = e_i$.</p>
<p>We’re going to do this with an orthogonal transformation. But orthogonal transformations are <em>length preserving</em>. That means <span class="math inline">\(\alpha = ||a_1||\)</span>. Therefore we need to find an orthogonal matrix that sends the vector <span class="math inline">\(a_1\)</span> to the vector <span class="math inline">\(||a_1|| e_1\)</span>. Note that any two vectors lie in a plane. We could either <em>rotate</em> by the angle between the vectors:</p>
<p><span class="math display">\[
\cos^{-1} \frac{a_1 \dot e_1}{||a_1||}
\]</span></p>
<p>or we can <em>reflect</em> across the line which bisects the two vectors in their plane. These two strategies are called the <a href="https://en.wikipedia.org/wiki/Givens_rotation">Givens rotation</a> and the <a href="https://en.wikipedia.org/wiki/Householder_transformation">Householder reflection</a> respectively. The rotation matrix is slightly less stable, so we will use the Householder reflection.</p>
<p>Let’s say that <span class="math inline">\(v\)</span> is the unit normal vector of a plane; how would I reflect an arbitrary vector <span class="math inline">\(x\)</span> across that plane? Well, if we subtracted <span class="math inline">\(\langle x, v \rangle v\)</span> from <span class="math inline">\(x\)</span>, that would be a projection into the plane, right? So, if we just keep going the same direction and for the same distance again, we’ll end up a point on the other side of the plane <span class="math inline">\(x&#39;\)</span>. Both <span class="math inline">\(x\)</span> and <span class="math inline">\(x&#39;\)</span> project to the same point on the plane, and furthermore both are a distance <span class="math inline">\(\langle x, v \rangle\)</span> from the plane. In other words, this operation is a reflection.</p>
<p>This diagram from Wikipedia illustrates this beautifully. Stare at it until you can <em>see</em> that reflecting about the dotted plane sends <span class="math inline">\(x\)</span> to <span class="math inline">\(||x||e_1\)</span>, and <em>believe</em> that <span class="math inline">\(v\)</span> is a unit vector orthogonal to the dotted plane of reflection.</p>
<p><a title="Bruguiea [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Householder.svg"><img width="256" alt="Householder" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Householder.svg/256px-Householder.svg.png"></a></p>
<p>Because a reflection is a linear transformation, we can express it as a matrix, which we will call <span class="math inline">\(H\)</span>. Here is how we go from our geometric intuition to a matrix:</p>
<p><span class="math display">\[
    \begin{align}
    H x &amp; \triangleq x - 2 \langle x, v \rangle v \\
        &amp; = x - 2v \langle x, v \rangle \\
        &amp; = Ix - 2 v (v^T x) \\
        &amp; = Ix - 2 (v v^T) x \\
        &amp; = (I - 2 (v v^T)) x 
    \end{align}
\]</span></p>
<p>Here, note that <span class="math inline">\(v v^T\)</span> is the <em>outer</em> product of <span class="math inline">\(v\)</span> with itself so is a square matrix with elements <span class="math inline">\(v_i v_j\)</span>. For example, if <span class="math inline">\(v = [\frac{1}{\sqrt{2}} \, \frac{1}{\sqrt{2}} \, 0]^T\)</span> (the 45° line in the xy-plane) we get:</p>
<p><span class="math display">\[
 H = I - 2 v v^T =
 \begin{bmatrix}
   1 &amp; 0 &amp; 0 \\\
   0 &amp; 1 &amp; 0 \\
   0 &amp; 0 &amp; 1 \\
 \end{bmatrix} - 2
 \begin{bmatrix}
   \frac{1}{2} &amp; \frac{1}{2} &amp; 0 \\\
   \frac{1}{2} &amp; \frac{1}{2} &amp; 0 \\
   0 &amp; 0 &amp; 0 \\
 \end{bmatrix} =
 \begin{bmatrix}
   0 &amp; -1 &amp; 0 \\\
   -1 &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 1 \\
 \end{bmatrix} 
\]</span></p>
<p>We now know how to define reflections which zero out the subdiagonal of a target columns, and we know how to construct orthogonal matrices which perform that reflection.</p>
</div>
<div id="implementing-the-lemmas" class="section level2">
<h2>Implementing the Lemmas</h2>
<p>Given the above theoretical presentation, and copious inline comments, I hope you will now be able to read and understand the following code:</p>
<pre><code>def householder_reflection(a, e):
    &#39;&#39;&#39;
    Given a vector a and a unit vector e,
    (where a is non-zero and not collinear with e)
    returns an orthogonal matrix which maps a
    into the line of e.
    &#39;&#39;&#39;
    
    # better safe than sorry.
    assert a.ndim == 1
    assert np.allclose(1, np.sum(e**2))
    
    # a and norm(a) * e are of equal length so
    # form an isosceles triangle. Therefore the third side
    # of the triangle is perpendicular to the line
    # that bisects the angle between a and e. This third
    # side is given by a - ||a|| e, which we will call u.
    # Since u lies in the plane spanned by a and e
    # its clear that u is actually orthogonal to a plane
    # equadistant to both a and ||a|| e. This is our
    # plane of reflection. We normalize u to v to 
    # because a unit vector is required in the next step.
    u = a - np.sign(a[0]) * norm(a) * e  
    v = u / norm(u)
    
    # derivation of the matrix form of a reflection:
    # x - 2&lt;x, v&gt;v ==
    # x - 2v&lt;x, v&gt; ==
    # Ix - 2 v (v^T x) ==
    # Ix - 2 (v v^T) x ==
    # (I - 2v v^T) x == H x
    H = np.eye(len(a)) - 2 * np.outer(v, v)
    
    return H</code></pre>
<p>With the householder reflection in hand, we can implement an iterative version of the QR decomposition algorithm, using the Householder reflection on each column in turn to transform <code>A</code> into an upper triangular matrix.</p>
<pre><code>def qr_decomposition(A):
    &#39;&#39;&#39;
    Given an n x m invertable matrix A, returns the pair:
        Q an orthogonal n x m matrix
        R an upper triangular m x m matrix
    such that QR = A.
    &#39;&#39;&#39;
    
    n, m = A.shape
    assert n &gt;= m
    
    # Q starts as a simple identity matrix.
    # R is not yet upper-triangular, but will be.
    Q = np.eye(n)
    R = A.copy()
    
    # if the matrix is square, we can stop at m-1
    # because there are no elements below the pivot
    # in the last column to zero out. Otherwise we
    # need to do all m columns.
    for i in range(m - int(n==m)):
        # we don&#39;t actually need to construct it,
        # but conceptually we&#39;re working to update
        # the minor matrix R[i:, i:] during the i-th
        # iteration. 
        
        # the first column vector of the minor matrix.
        r = R[i:, i]
        
        # if r and e are already co-linear then we won&#39;t
        # be able to construct the householder matrix,
        # but the good news is we won&#39;t need to!
        if np.allclose(r[1:], 0):
            continue
            
        # e is the i-th basis vector of the minor matrix.
        e = np.zeros(n-i)
        e[0] = 1  
        
        # The householder reflection is only
        # applied to the minor matrix - the
        # rest of the matrix is left unchanged,
        # which we represent with an identity matrix.
        # Note that means H is in block diagonal form
        # where every block is orthogonal, therefore H
        # itself is orthogonal.
        H = np.eye(n)
        H[i:, i:] = householder_reflection(r, e)

        # QR = A is invariant. Proof:
        # QR = A, H^T H = I =&gt; 
        # Q H^T H R = A =&gt;
        # Q&#39; = Q H^T, R&#39; = H R =&gt;
        # Q&#39; R&#39; = A. QED.
        #
        # By construction, the first column of the 
        # minor matrix now has zeros for all
        # subdiagonal matrix. By induction, we 
        # have that all subdiagonal elements in
        # columns j&lt;=i are zero. When i=N, R
        # is upper triangular. 
        Q = Q @ H.T
        R = H @ R
    
    return Q, R</code></pre>
<p>The last piece of the puzzle is back-substitution. This is straightforward and available in standard libraries, but to comply with the letter-of-law of the “from scratch” challenge we’ll implement our own version.</p>
<pre><code>def solve_triangular(A, b):
    &#39;&#39;&#39;
    Solves the equation Ax = b when A is an upper-triangular square matrix
    and b is a one dimensional vector by back-substitution. The length of b
    and the number of rows must match. Returns x as a one-dimensional numpy.ndarray
    of the same length as b.
    
    This isn&#39;t as micro-optimized as scipy.linalg.solve_triangular() but the
    algorithm is the same, and the asymptotic time complexity is the same.  
    &#39;&#39;&#39;
    
    # starting at the bottom, the last row is just a_N_N * x = b_N
    n, m = A.shape
    x = b[(m-1):m] / A[m-1, m-1]
    
    for i in range(m - 2, -1, -1):
        back_substitutions = np.dot(A[i, (i+1):], x)
        rhs = b[i] - back_substitutions
        x_i = rhs / A[i, i]  # possible ill-conditioning warning?
        x = np.insert(x, 0, x_i)
  
    return x</code></pre>
<p>I won’t lie - that was a ton of linear algebra we just ploughed through. If you got through it, or if you had the good sense to skim ahead until you found something that made sense, congratulations.</p>
<p>Before we move on to actually <em>using</em> our new functions, let’s spend some time making sure everything up to this point is correct.</p>
<pre><code>class QRTestCase(unittest.TestCase):
    &#39;&#39;&#39;
    Unit tests for QR decomposition and its dependencies. 
    &#39;&#39;&#39;
    
    def test_2d(self):
        A = np.array([[1,1], [0,1]])
        b = np.array([2,3])
        x = solve_triangular(A, b)
        assert_allclose(x, np.array([-1, 3]))
    
    def test_solve_triangular(self):
        for N in range(1, 20):
            A = np.triu(np.random.normal(size=(N, N)))
            x = np.random.normal(size=(N,))
            b = A @ x
            x2 = solve_triangular(A, b)
            assert_allclose(x, x2, atol=1e-5)

    def test_solve_rect_triangular(self):
        for N in range(1, 20):
            for N2 in [1, 5, 100]:
                A = np.triu(np.random.normal(size=(N+N2, N)))
                x = np.random.normal(size=(N,))
                b = A @ x
                x2 = solve_triangular(A, b)
                assert_allclose(x, x2, atol=1e-5)
     
    def test_reflection(self):
        x = np.array([1,1,1])
        e1 = np.array([1,0,0])
        H = householder_reflection(x, e1)
        assert_allclose(H @ (sqrt(3)* np.array([1, 0, 0])), x, atol=1e-5)
        assert_allclose(H @ np.array([1,1,1]), sqrt(3) * e1, atol=1e-5)
    
    def test_square_qr(self):
        # already upper triangular
        A = np.array([[2,1], [0, 3]])
        Q, R = qr_decomposition(A)
        assert_allclose(Q, np.eye(2))
        assert_allclose(R, A)
        
        N = 3
        Q = ortho_group.rvs(N) # generates random orthogonal matrices
        R = np.triu(np.random.normal(size=(N, N)))
        A = Q @ R
        Q2, R2 = qr_decomposition(Q @ R)
        # note that QR is not quite unique, so we can&#39;t
        # just test Q == Q2, unfortunately.
        assert_allclose(Q2 @ R2, Q @ R, atol=1e-5)
        assert_allclose(np.abs(det(Q2)), 1.0, atol=1e-5)
        assert_allclose(R2[2, 0], 0, atol=1e-5)
        assert_allclose(R2[2, 1], 0, atol=1e-5)
        assert_allclose(R2[1, 0], 0, atol=1e-5)
        
    def test_rect_qr(self):
        A = np.array([
            [2,1],
            [0,3],
            [4,5],
            [1,1],
        ])
        Q, R = qr_decomposition(A)
        assert_allclose(R[1:, 0], np.zeros(A.shape[0]-1), atol=1e-5)
        assert_allclose(R[2:, 0], np.zeros(A.shape[0]-2), atol=1e-5)
        assert_allclose(Q @ R, A, atol=1e-5)

unittest.main(argv=[&#39;&#39;], exit=False)</code></pre>
<p>With our trusty tools in hand, we’re ready to tackle linear regression properly.</p>
</div>
<div id="implementing-linear-regression" class="section level2">
<h2>Implementing Linear Regression</h2>
<p>Recall that our problem was to solve the normal equation:</p>
<p><span class="math display">\[ X^T X \hat{\Theta} = X^T y \]</span></p>
<p>If we now let <span class="math inline">\(QR\)</span> be the QR-decomposition of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[ (R^T Q^T)(Q R) \hat{\Theta} = R^T Q^T y \]</span></p>
<p>Since <span class="math inline">\(Q\)</span> is orthogonal, <span class="math inline">\(Q^T Q = I\)</span> and we can simplify this to:</p>
<p><span class="math display">\[ R^T R \hat{\Theta} = R^T Q^T y \]</span></p>
<p>For the next step, we have to assume the <span class="math inline">\(R\)</span> is invertible. This is always the case if our original <span class="math inline">\(X\)</span> was free of multicollinearity. It is also equivalent to <span class="math inline">\(X^T X\)</span> being invertible so the naive approach of taking <span class="math inline">\((X^T X)^{-1}\)</span> isn’t any stronger. Even gradient descent has issues with singular matrices because the problem is no longer strongly convex. There is a method based on SVD (singular value decomposition) which can handle linear regression in the presence of multicollinearity but it’s slower and in general the whole problem is better handled by removing redundant features or adding regularization, neither of which are in scope for this article.</p>
<blockquote>
<p>A guy goes to the doctor and says, “Doctor, it hurts when I perform linear regression on a dataset with strong or perfect multicollinearity.” The doctor says, “don’t do that.”</p>
</blockquote>
<p>In any case, let’s just assume that <span class="math inline">\(R^{-1}\)</span> exists. We don’t actually have to calculate it, though! The mere fact of its existence lets us left multiply both sides of the equation by <span class="math inline">\((R^T)^{-1}\)</span> and cancel the <span class="math inline">\(R^T\)</span> on both sides, leaving only:</p>
<p><span class="math display">\[ R \hat{\Theta} = Q^T y \]</span></p>
<p>Because <span class="math inline">\(R\)</span> is an upper triangular matrix, we can use our <code>solve_triangular()</code> function to solve this equation very quickly.</p>
<p>The final algorithm is deceptively simple. Compare the normal equations derived above to the final two lines of the <code>fit()</code> method.</p>
<pre><code>class LinearRegression:
    def __init__(self, add_intercept=True):
        self.add_intercept = bool(add_intercept)

    def _design_matrix(self, X):
        if self.add_intercept:
            X = np.hstack([ np.ones((X.shape[0], 1)), X])
        return X

    def fit(self, X, y):
        X = self._design_matrix(X)
        Q, R = qr_decomposition(X)
        self.theta_hat = solve_triangular(R, Q.T @ y)
    
    def predict(self, X):
        X = self._design_matrix(X)
        return X @ self.theta_hat</code></pre>
</div>
<div id="testing" class="section level2">
<h2>Testing</h2>
<p>Note that while we follow the scikit-learn naming conventions, up to this point we haven’t imported anything from sklearn. That’s in keeping with the “from scratch” challenge. However, to <em>test</em> the code, we are going to use a few sklearn and scipy dependencies.</p>
<p>Let’s first grab a bunch of test-only dependencies and also grab a copy of the famous <a href="https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html">Boston</a> data set so we have a simple regression problem to play with.</p>
<pre><code># testing purposes only
from sklearn.datasets import load_boston
import matplotlib
from matplotlib import pyplot as plt
%matplotlib inline
from numpy.linalg import det
from scipy.stats import ortho_group
import unittest
from numpy.testing import assert_allclose

boston = load_boston()
X_raw = boston.data
y_raw = boston.target

# shuffle the data to randomize the train/test split
shuffle = np.random.permutation(len(y_raw))
X_full = X_raw[shuffle].copy()
y_full = y_raw[shuffle].copy()

# 80/20 train/test split. 
train_test_split = int(0.8 * len(y_full))
X_train = X_full[:train_test_split, :]
y_train = y_full[:train_test_split]
X_test = X_full[train_test_split:, :]
y_test = y_full[train_test_split:]</code></pre>
<p>The model is fit to the training set only. If it fits the training set pretty well we know it has learned the examples we gave it; if it <em>also</em> fits the test set pretty well, we know it’s done more than just memorize the examples given but has also learned a more general lesson that it can apply to novel data that it’s never seen before.</p>
<p>A good way to visualize model performance is to plot <span class="math inline">\(y\)</span> vs. <span class="math inline">\(\hat{y}\)</span> - in other words, actual vs predicted. A perfect predictor would be a 45° diagonal through the origin; random guessing would be a shapeless or circular cloud of points.</p>
<pre><code>model = LinearRegression()
model.fit(X_train, y_train)

def goodness_of_fit_report(label, model, X, y):
    y_hat = model.predict(X)
    
    # predicted-vs-actual plot
    plt.scatter(x=y, y=y_hat, label=label, alpha=0.5)
    plt.title(&quot;Predicted vs. Actual&quot;)
    plt.xlabel(&quot;Actual&quot;)
    plt.ylabel(&quot;Predictions&quot;)
    plt.legend()
    
    mse = np.mean( (y - y_hat)**2 )
    y_bar = np.mean(y)
    r2 = 1 - np.sum( (y-y_hat)**2 ) / np.sum( (y-y_bar)**2 )
    print(&quot;{label: &lt;16} mse={mse:.2f}     r2={r2:.2f}&quot;.format(**locals()))
        
plt.figure(figsize=(16,6))
plt.subplot(1, 2, 1)
goodness_of_fit_report(&quot;Training Set&quot;, model, X_train, y_train)
plt.subplot(1, 2, 2)
goodness_of_fit_report(&quot;Test Set&quot;, model, X_test, y_test)</code></pre>
<blockquote>
<p>Training Set mse=21.30 r2=0.73</p>
<p>Test Set mse=25.12 r2=0.75</p>
</blockquote>
<p>And in point of fact this linear regression model does reasonably well on both the train and test set, with correlation scores around 75%. That means it’s able to explain about three-quarters of the variation that it finds in <span class="math inline">\(y\)</span> from what it was able to learn about the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>It’s also a good idea to visualize actual responses <span class="math inline">\(y\)</span> and predictions <span class="math inline">\(\hat{y}\)</span> as a function of the independent variables <span class="math inline">\(X\)</span>. In this case <span class="math inline">\(X\)</span> is 13-dimensional so hard to visualized fully, so we will simply choose a few random pairs of dimensions dimensions so we can work in 2D. If the model has learned anything real about the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>, we should see two similar clouds of points for actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="figure">
<img src="/post/ml-from-scratch-part-1-linear-regression_files/predicted_vs_actual.png" alt="Prediction vs. Actual Scatterplot, training set and test set" />
<p class="caption">Prediction vs. Actual Scatterplot, training set and test set</p>
</div>
<p>We can also plot the actual and predicted response as a function of various predictors to get a sense of whether or not our function is truly fitting the data:</p>
<pre><code>y_hat = model.predict(X_train)
plt.figure(figsize=(16,32))
for i in range(4, 8):
    plt.subplot(6, 2, i+1)
    plt.scatter(x=X_train[:, i], y=y_train, alpha=0.2, label=&#39;Actual&#39;)
    plt.scatter(x=X_train[:, i], y=y_hat, alpha=0.2, label=&#39;Predicted&#39;)
    plt.legend()
    plt.xlabel(boston.feature_names[i])
    plt.ylabel(&quot;Response Variable&quot;)</code></pre>
<div class="figure">
<img src="/post/ml-from-scratch-part-1-linear-regression_files/scatter.png" alt="Predicted vs. Actual over pairs of independent variables" />
<p class="caption">Predicted vs. Actual over pairs of independent variables</p>
</div>
<p>The eyeball test confirms that this model is fitting the data rather well, just as we’d expect when <span class="math inline">\(r^2 = 0.75\)</span>.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>That was linear regression from scratch. There’s a lot more that could be said about linear regression even as a black box predictive model: polynomial and interaction terms, L1 and L2 regularization, and linear constraints on coefficients come to mind.</p>
<p>There’s also a whole universe of techniques for doing statistical modeling and inference with linear regression: testing residuals for homoscedasticity, normality, autocorrelation, variance inflation factors, orthogonal polynomial regression, Cook’s distance, leverage, Studentized residuals, ANOVA, AIC, BIC, Omnibus F-tests on nested models, etc., etc. Just to be clear, these aren’t <em>variations</em> or <em>generalization</em> of linear regression (although there are tons of those too) these are just standard techniques for analyzing and understanding linear regression models of the exact same form we calculated above. The topic is very mature and a huge amount of auxiliary mathematical machinery has been built up over the centuries (Gauss was studying OLS around 1800, and the problem is older than that.)</p>
<p>However, if we go too deeply into linear regression, we won’t get a chance to explore the rest of machine learning. So for the next part of the series, we will switch our attention to <a href="/post/ml-from-scratch-part-2-logistic-regression/">logistic regression</a> and use that as an excuse to explore <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a> in some detail. That will then serve as a jumping off point for our first “true” machine learning algorithm in part 3: <a href="/post/ml-from-scratch-part-3-backpropagation/">neural networks and backpropagation.</a></p>
</div>
