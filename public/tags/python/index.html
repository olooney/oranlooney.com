<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Python</title>
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link href="/tags/python/index.xml" rel="alternate" type="application/rss+xml" title="OranLooney.com" />
  <link href="/tags/python/index.xml" rel="feed" type="application/rss+xml" title="OranLooney.com" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/search/" title="Site Search"><i class="fa fa-search"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//x.com/oranlooney" target="_blank"><i class="fab fa-twitter"></i></a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>
  </ul>

  </nav>
</header>


  <div class="breadcrumb">
    <a href="/tags/">Tags</a> / Python
  </div>
  <div class="main" role="main">
    <section class="list taxonomy-list">
      <article class="article">
        <a href="/post/grifters-skeptics-marks/" class="article-titles">
          <h2 class="article-title">Grifters, Skeptics, and Marks</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>January 8, 2026</time></li>
          <li class="article-meta-tags">
            <a href="/tags/computer-science/">
              <i class="fas fa-tag"></i>
              Computer Science
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/visualization/">
              <i class="fas fa-tag"></i>
              Visualization
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/grifters-skeptics-marks/"><img src="/post/grifters-skeptics-marks_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          We’re in the golden age of grift. Where adventurers once flocked to California and the Yukon because &ldquo;there was gold in them thar hills,&rdquo; the most exploitable resource in 2026 are suckers. Great Grift Rush of ’06. &ldquo;crime is legal now&rdquo;, smart money and retail investors. CAT bonds for retail investors, etc.
This is hardly the first time. The Great Depression brought with it a wave of con artists, as portrayed in movies like Paper Moon or The Sting.
        </div>
        <div class="article-readmore"><a href="/post/grifters-skeptics-marks/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/genji-ko/" class="article-titles">
          <h2 class="article-title">The Art and Mathematics of Genji-Kō</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 26, 2024</time></li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/visualization/">
              <i class="fas fa-tag"></i>
              Visualization
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/history/">
              <i class="fas fa-tag"></i>
              History
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/genji-ko/"><img src="/post/genji-ko_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          You might think it&rsquo;s unlikely for any interesting mathematics to arise from incense appreciation, but that&rsquo;s only because you&rsquo;re unfamiliar with the peculiar character of Muromachi (室町) era Japanese nobles.
There has never been a group of people, in any time or place, who were so driven to display their sophistication and refinement. It wouldn&rsquo;t do to merely put out a few sticks of incense; no, you would have to prove that your taste was more exquisite, your judgment more refined, your etiquette more oblique.
        </div>
        <div class="article-readmore"><a href="/post/genji-ko/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/jeopardy/" class="article-titles">
          <h2 class="article-title">Let&#39;s Play Jeopardy! with LLMs</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>May 12, 2024</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/llm/">
              <i class="fas fa-tag"></i>
              LLM
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/jeopardy/"><img src="/post/jeopardy_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          How good are LLMs at trivia? I used the Jeopardy! dataset from Kaggle to benchmark ChatGPT and the new Llama 3 models. Here are the results:
There you go. You&rsquo;ve already gotten 90% of what you&rsquo;re going to get out of this article. Some guy on the internet ran a half-baked benchmark on a handful of LLM models, and the results were largely in line with popular benchmarks and received wisdom on fine-tuning and RAG.
        </div>
        <div class="article-readmore"><a href="/post/jeopardy/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/kaprekar/" class="article-titles">
          <h2 class="article-title">Kaprekar&#39;s Magic 6174</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 25, 2024</time></li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/visualization/">
              <i class="fas fa-tag"></i>
              Visualization
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/kaprekar/"><img src="/post/kaprekar_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Kaprekar&rsquo;s routine is a simple arithmetic procedure which, when applied to four digit numbers, rapidly converges to the fixed point 6174, known as the Kaprekar constant. Unlike other famous iterative procedures such as the Collatz function, the somewhat arbitrary nature of the Kaprekar routine doesn&rsquo;t hint at fundamental mathematical discoveries yet to be made; rather, its charm lies in its intuitive definition (requiring no more than elementary mathematics,) its oddly off-center fixed point of 6174, and its surprisingly rapid convergence (which requires only five iterations on average and never more than seven.
        </div>
        <div class="article-readmore"><a href="/post/kaprekar/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/playfair/" class="article-titles">
          <h2 class="article-title">Cracking Playfair Ciphers</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>September 13, 2023</time></li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/visualization/">
              <i class="fas fa-tag"></i>
              Visualization
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/playfair/"><img src="/post/playfair_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In 2020, the Zodiac 340 cipher was finally cracked after more than 50 years of trying by amateur code breakers. While the effort to crack it was extremely impressive, the cipher itself was ultimately disappointing. A homophonic substitution cipher with a minor gimmick of writing diagonally, the main factor that prevented it from being solved much earlier was the several errors the Zodiac killer made when encoding it.
Substitution ciphers, which operate at the level of a single character, are children&rsquo;s toys, the kind of thing you might get a decoder ring for from the back of a magazine.
        </div>
        <div class="article-readmore"><a href="/post/playfair/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-6-pca/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 6: Principal Component Analysis</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>September 16, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-6-pca/"><img src="/post/ml-from-scratch-part-6-pca_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In the previous article in this series we distinguished between two kinds of unsupervised learning (cluster analysis and dimensionality reduction) and discussed the former in some detail. In this installment we turn our attention to the later.
In dimensionality reduction we seek a function \(f : \mathbb{R}^n \mapsto \mathbb{R}^m\) where \(n\) is the dimension of the original data \(\mathbf{X}\) and \(m\) is less than or equal to \(n\). That is, we want to map some high dimensional space into some lower dimensional space.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-6-pca/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/slow-fibonacci/" class="article-titles">
          <h2 class="article-title">A Seriously Slow Fibonacci Function</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>July 6, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/computer-science/">
              <i class="fas fa-tag"></i>
              Computer Science
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/slow-fibonacci/"><img src="/post/slow-fibonacci_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          I recently wrote an article which was ostensibly about the Fibonacci series but was really about optimization techniques. I wanted to follow up on its (extremely moderate) success by going in the exact opposite direction: by writing a Fibonacci function which is as slow as possible.
This is not as easy as it sounds: any program can trivially be made slower, but this is boring. How can we make it slow in a fair and interesting way?
        </div>
        <div class="article-readmore"><a href="/post/slow-fibonacci/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-5-gmm/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 5: Gaussian Mixture Models</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>June 5, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-5-gmm/"><img src="/post/ml-from-scratch-part-5-gmm_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Consider the following motivating dataset:
Unlabled Data
 It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. The question is: is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-5-gmm/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/adaptive-basis-functions/" class="article-titles">
          <h2 class="article-title">Adaptive Basis Functions</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>May 21, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/adaptive-basis-functions/"><img src="/post/adaptive-basis-functions_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Today, let me be vague. No statistics, no algorithms, no proofs. Instead, we’re going to go through a series of examples and eyeball a suggestive series of charts, which will imply a certain conclusion, without actually proving anything; but which will, I hope, provide useful intuition.
The premise is this:
 For any given problem, there exists learned featured representations which are better than any fixed/human-engineered set of features, even once the cost of the added parameters necessary to also learn the new features into account.
        </div>
        <div class="article-readmore"><a href="/post/adaptive-basis-functions/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-4-decision-tree/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 4: Decision Trees</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>March 1, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-4-decision-tree/"><img src="/post/ml-from-scratch-part-4-decision-tree_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          So far in this series we’ve followed one particular thread: linear regression -&gt; logistic regression -&gt; neural network. This is a very natural progression of ideas, but it really represents only one possible approach. Today we’ll switch gears and look at a model with completely different pedigree: the decision tree, sometimes also referred to as Classification and Regression Trees, or simply CART models. In contrast to the earlier progression, decision trees are designed from the start to represent non-linear features and interactions.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-4-decision-tree/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/fibonacci/" class="article-titles">
          <h2 class="article-title">A Fairly Fast Fibonacci Function</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 19, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/c&#43;&#43;/">
              <i class="fas fa-tag"></i>
              C&#43;&#43;
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/computer-science/">
              <i class="fas fa-tag"></i>
              Computer Science
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/fibonacci/"><img src="/post/fibonacci_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          A common example of recursion is the function to calculate the \(n\)-th Fibonacci number:
def naive_fib(n): if n &lt; 2: return n else: return naive_fib(n-1) + naive_fib(n-2) This follows the mathematical definition very closely but it’s performance is terrible: roughly \(\mathcal{O}(2^n)\). This is commonly patched up with dynamic programming. Specifically, either the memoization:
from functools import lru_cache @lru_cache(100) def memoized_fib(n): if n &lt; 2: return n else: return memoized_fib(n-1) + memoized_fib(n-2) or tabulation:
        </div>
        <div class="article-readmore"><a href="/post/fibonacci/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-3-backpropagation/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 3: Backpropagation</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 3, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-3-backpropagation/"><img src="/post/ml-from-scratch-part-3-backpropagation_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In today’s installment of Machine Learning From Scratch we’ll build on the logistic regression from last time to create a classifier which is able to automatically represent non-linear relationships and interactions between features: the neural network. In particular I want to focus on one central algorithm which allows us to apply gradient descent to deep neural networks: the backpropagation algorithm. The history of this algorithm appears to be somewhat complex (as you can hear from Yann LeCun himself in this 2018 interview) but luckily for us the algorithm in its modern form is not difficult - although it does require a solid handle on linear algebra and calculus.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-3-backpropagation/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-2-logistic-regression/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 2: Logistic Regression</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>December 27, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-2-logistic-regression/"><img src="/post/ml-from-scratch-part-2-logistic-regression_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In this second installment of the machine learning from scratch we switch the point of view from regression to classification: instead of estimating a number, we will be trying to guess which of 2 possible classes a given input belongs to. A modern example is looking at a photo and deciding if its a cat or a dog.
In practice, its extremely common to need to decide between \(k\) classes where \(k &gt; 2\) but in this article we’ll limit ourselves to just two classes - the so-called binary classification problem - because generalizations to many classes are usually both tedious and straight-forward.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-2-logistic-regression/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-1-linear-regression/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 1: Linear Regression</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 29, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-1-linear-regression/"><img src="/post/ml-from-scratch-part-1-linear-regression_files/lead.192x128.png" class="article-image" /></a>
        <div class="article-content">
          To kick off this series, will start with something simple yet foundational: linear regression via ordinary least squares.
While not exciting, linear regression finds widespread use both as a standalone learning algorithm and as a building block in more advanced learning algorithms. The output layer of a deep neural network trained for regression with MSE loss, simple AR time series models, and the “local regression” part of LOWESS smoothing are all examples of linear regression being used as an ingredient in a more sophisticated model.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-1-linear-regression/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-0-introduction/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 0: Introduction</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 11, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-0-introduction/"><img src="/post/ml-from-scratch-part-0-introduction_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Motivation  “As an apprentice, every new magician must prove to his own satisfaction, at least once, that there is truly great power in magic.” - The Flying Sorcerers, by David Gerrold and Larry Niven
 How do you know if you really understand something? You could just rely on the subjective experience of feeling like you understand. This sounds plausible - surely you of all people should know, right? But this runs head-first into in the Dunning-Kruger effect.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-0-introduction/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/craps-game-variants/" class="article-titles">
          <h2 class="article-title">Craps Variants</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>July 11, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/craps-game-variants/"><img src="/post/craps-game-variants_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Craps is a suprisingly fair game. I remember calculating the probability of winning craps for the first time in an undergraduate discrete math class: I went back through my calculations several times, certain there was a mistake somewhere. How could it be closer than $\frac{1}{36}$?
(Spoiler Warning If you haven&rsquo;t calculated these odds for yourself then you may want to do so before reading further. I&rsquo;m about to spoil it for you rather thoroughly in the name of exploring a more general case.
        </div>
        <div class="article-readmore"><a href="/post/craps-game-variants/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/semantic-code/" class="article-titles">
          <h2 class="article-title">Semantic Code</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 30, 2008</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/semantic-code/"><img src="/post/semantic-code_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          se-man-tic (si-man&rsquo;tik) adj. &nbsp; &nbsp; 1. Of or relating to meaning, especially meaning in language.
 Programming destroys meaning. When we program, we first replace concepts with symbols and then replace those symbols with arbitrary codes &mdash; that&rsquo;s why it&rsquo;s called coding.
At its worst programming is write-only: the program accomplishes a task, but is incomprehensible to humans. See, for example, the story of Mel. Such a program is correct, yet at the same time meaningless.
        </div>
        <div class="article-readmore"><a href="/post/semantic-code/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
    </section>
    

  </div>



<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2026 Oran Looney
  </div>
  <ul class="site-footer-items">
    <li class="site-footer-item-rsslink">
      <a href="/tags/python/index.xml" type="application/rss+xml" target="_blank" title="RSS">
        <i class="fas fa-rss"></i>
      </a>
    </li>
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
