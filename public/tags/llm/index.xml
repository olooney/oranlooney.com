<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llm on OranLooney.com</title>
    <link>https://www.oranlooney.com/tags/llm/</link>
    <description>Recent content in Llm on OranLooney.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; Copyright {year} Oran Looney</copyright>
    <lastBuildDate>Sat, 11 May 2024 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.oranlooney.com/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Jeopardy! Benchmark</title>
      <link>https://www.oranlooney.com/post/jeopardy/</link>
      <pubDate>Sat, 11 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/jeopardy/</guid>
      <description>I used the Kaggle Jeopardy! to benchmark OpenAI and LLama3 models including some fine-tuning and RAG. In accordance with the principle of BLUF (bottom-line-up-front,) here are my results:
There you go. You&amp;rsquo;ve already gotten 90% of what you&amp;rsquo;re going to get out of this article. Some rando on the internet used the Jeopardy! dataset to benchmark a scattered handful of LLM models and approaches, and those results largely match popular benchmarks and received wisdom on fine-tuning and RAG.</description>
    </item>
    
    <item>
      <title>My Dinner with ChatGPT</title>
      <link>https://www.oranlooney.com/post/my-dinner-with-chatgpt/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/my-dinner-with-chatgpt/</guid>
      <description>It&#39;s hard to talk about ChatGPT without cherry-picking. It&#39;s too easy to try a dozen different prompts, refresh each a handful of times, and report the most interesting or impressive thing from those sixty trials. While this problem plagues a lot of the public discourse around generative models, cherry-picking is particularly problematic for ChatGPT because it&#39;s actively using the chat history as context. (It might be using a $\mathcal{O}(n \log{} n)$ attention model like reformer or it might just be brute forcing it, but either it has an impressively long memory; about 2048 &#34;</description>
    </item>
    
  </channel>
</rss>