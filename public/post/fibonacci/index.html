<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A Fairly Fast Fibonacci Function - OranLooney.com</title>
  <meta property="og:title" content="A Fairly Fast Fibonacci Function - OranLooney.com" />
  <meta name="twitter:title" content="A Fairly Fast Fibonacci Function - OranLooney.com" />
  <meta name="description" content="A common example of recursion is the function to calculate the \(n\)-th Fibonacci number:
def naive_fib(n):if n &lt; 2:return nelse:return naive_fib(n-1) &#43; naive_fib(n-2)This follows the mathematical definition very closely but it’s performance isterrible: roughly \(\mathcal{O}(2^n)\). This is commonly patched up with dynamicprogramming. Specifically, either the memoization:
from functools import lru_cache@lru_cache(100)def memoized_fib(n):if n &lt; 2:return nelse:return memoized_fib(n-1) &#43; memoized_fib(n-2)or tabulation:">
  <meta property="og:description" content="A common example of recursion is the function to calculate the \(n\)-th Fibonacci number:
def naive_fib(n):if n &lt; 2:return nelse:return naive_fib(n-1) &#43; naive_fib(n-2)This follows the mathematical definition very closely but it’s performance isterrible: roughly \(\mathcal{O}(2^n)\). This is commonly patched up with dynamicprogramming. Specifically, either the memoization:
from functools import lru_cache@lru_cache(100)def memoized_fib(n):if n &lt; 2:return nelse:return memoized_fib(n-1) &#43; memoized_fib(n-2)or tabulation:">
  <meta name="twitter:description" content="A common example of recursion is the function to calculate the \(n\)-th Fibonacci number:
def naive_fib(n):if n &lt; 2:return nelse:return naive_fib(n-1) &#43; naive_fib(n-2)This follows the …">
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>

  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-2535855-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-2535855-1');
  </script>

 
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/search/" title="Site Search"><i class="fa fa-search"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//honeycode.tumblr.com/" target="_blank" title="Honeycode Microblog">
      <svg class="svg-inline--fa fa-w-12" aria-hidden="true" data-prefix="fab" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="20 50 350 450" data-fa-i2svg="">
<path fill="currentColor" d="M193 97l86.6 50v100l-86.6 50l-86.6 -50v-100zM285 255l86.6 50v100l-86.6 50l-86.6 -50v-100zM100 255l86.6 50v100l-86.6 50l-86.6 -50v-100z"></path>
</svg>
    </a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>

    <li>
      <a href="https://www.librarything.com/catalog.php?view=olooney&amp;offset=0&amp;shelf_rows=10&amp;previousOffset=0&amp;shelf=shelf" target="_blank" title="LibraryThing">
	    <i class="fas fa-book-reader"></i>
	  </a>
	</li>
    
  </ul>

  </nav>
</header>


  <div class="main" role="main">
    <article class="article">
      <img src="/post/fibonacci_files/lead.jpg" class="article-image" />
      
      <h1 class="article-title">A Fairly Fast Fibonacci Function</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-date"><time>February 19, 2019</time></li>
        <li class="article-meta-tags">
          <a href="/tags/python/">
            <i class="fas fa-tag"></i>
            Python
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/c&#43;&#43;/">
            <i class="fas fa-tag"></i>
            C&#43;&#43;
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/math/">
            <i class="fas fa-tag"></i>
            Math
          </a>&nbsp;
        </li>
      </ul>
      
<aside class="toc">
  
</aside>
      <p>A common example of recursion is the function to calculate the <span class="math inline">\(n\)</span>-th Fibonacci number:</p>
<pre><code>def naive_fib(n):
    if n &lt; 2:
        return n
    else:
        return naive_fib(n-1) + naive_fib(n-2)</code></pre>
<p>This follows the mathematical definition very closely but it’s performance is
terrible: roughly <span class="math inline">\(\mathcal{O}(2^n)\)</span>. This is commonly patched up with <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic
programming</a>. Specifically, either the <a href="https://en.wikipedia.org/wiki/Memoization">memoization</a>:</p>
<pre><code>from functools import lru_cache

@lru_cache(100)
def memoized_fib(n):
    if n &lt; 2:
        return n
    else:
        return memoized_fib(n-1) + memoized_fib(n-2)</code></pre>
<p>or <a href="https://www.geeksforgeeks.org/tabulation-vs-memoizatation/">tabulation</a>:</p>
<pre><code>def table_fib(n):
    if n &lt; 2:
        return n
    table = [-1] * (n+1)
    table[0] = 0
    table[1] = 1
    for i in range(_2, n+1):
        table[i] = table[i-1] + table[i-2]
    return table[n]</code></pre>
<p>Observing that we only ever have to use the two most recent Fibonacci numbers,
the tabular solution can easily be made iterative, resulting in a large space
savings:</p>
<pre><code>def iterative_fib(n):
    previous, current = (0, 1)
    for i in range(2, n+1):
        previous, current = (current, previous + current)
    return current</code></pre>
<p>And that, oddly enough, is often where it stops. For example, this presentation
of solving the Fibonacci sequence as an <a href="https://medium.com/quick-code/fibonacci-sequence-javascript-interview-question-iterative-and-recursive-solutions-6a0346d24053">interview question</a> presents the
above two solutions and then… nothing. Not so much as an off-hand mention
that better solutions might exist. Googling around, I got the impression this
is a fairly common (but by no means universal) misconception, perhaps because
teachers use the Fibonacci function to illustrate the <em>idea</em> of dynamic
programming but are not interested in spending too much time going too far into
the specifics of the mathematics.</p>
<p>Which is a shame, because it only gets more interesting the deeper we go.</p>
<p>I should also clarify that we are particularly interested in calculating
<strong>large</strong> Fibonacci numbers - say, the one-millionth or one-billionth.</p>
<p>Fair warning: this is a bit of rabbit hole, with no other purpose than to
optimize the hell out something for which there is frankly no practical use.
But we get to do a bit of linear algebra and try out some pretty interesting
optimization techniques; that’s what I call a good time!</p>
<div id="matrix-form" class="section level2">
<h2>Matrix Form</h2>
<p>There exist several <a href="https://en.wikipedia.org/wiki/Fibonacci_number#Closed-form_expression">closed-form solutions</a> to Fibonacci sequence which gives
us the false hope that there might be an <span class="math inline">\(\mathcal{O}(1)\)</span> solution. Unfortunately
they all turn out to be non-optimal if you want an exact solution for a large <span class="math inline">\(n\)</span>.
We will use to so-called “matrix form” instead, which we will now describe in some detail.</p>
<p>Recall that the <span class="math inline">\(n\)</span>-th Fibonacci number is given by the recurrence relation:</p>
<p><span class="math display">\[
    \begin{align}
        F_0 &amp;= 0 \\
        F_1 &amp;= 1 \\
        F_n &amp;= F_{n-1} + F_{n-2}
    \end{align}
\]</span></p>
<p>Define the first Fibonacci matrix to be:</p>
<p><span class="math display">\[
    \mathbf{F}_1 = \begin{bmatrix}
        1 &amp; 1 \\
        1 &amp; 0 
    \end{bmatrix}
\]</span></p>
<p>And define the <span class="math inline">\(n\)</span>-th Fibonacci matrix to be the <span class="math inline">\(n\)</span>-th power:</p>
<p><span class="math display">\[
    \mathbf{F}_n = \mathbf{F}_1^n
\]</span></p>
<p>I didn’t just pluck this out of thin air - there’s a general way
to turn <em>any</em> <a href="http://mathworld.wolfram.com/LinearRecurrenceEquation.html">linear recurrence relation</a> into a matrix which I’ll
describe in a moment. But first let’s prove the following theorem, which
justifies our definition:</p>
<p><span class="math display">\[
    \forall n \in \mathbb{N}, \mathbf{F}_n = \begin{bmatrix}
        F_{n+1} &amp; F_n \\
        F_n &amp; F_{n-1}
    \end{bmatrix}
\]</span></p>
<p>We proceed by induction. For the case of <span class="math inline">\(n = 1\)</span>, the theorem is true by inspection because we know <span class="math inline">\(F_0 = 0\)</span> and <span class="math inline">\(F_1 = F_2 = 1\)</span>.</p>
<p>Suppose it is true for <span class="math inline">\(n-1\)</span>. Then we have:</p>
<p><span class="math display">\[
    \mathbf{F}_n = \mathbf{F}_1^{n} = \mathbf{F}_1^{n-1} \mathbf{F}_1 = 
    \begin{bmatrix}
        F_n &amp; F_{n-1} \\
        F_{n-1} &amp; F_{n-2}
    \end{bmatrix}
    \begin{bmatrix}
        1 &amp; 1 \\
        1 &amp; 0 
    \end{bmatrix}
\]</span></p>
<p>Multiplying these two matrices, we have:</p>
<p><span class="math display">\[
    \mathbf{F}_n = 
    \begin{bmatrix}
        F_n + F_{n-1} &amp; F_{n} \\
        F_{n-1} + F_{n-2} &amp; F_{n-1} 
    \end{bmatrix}
\]</span></p>
<p>We can use the Fibonacci definition twice (once for each element of the first column) to
get:</p>
<p><span class="math display">\[
    \mathbf{F}_n = 
    \begin{bmatrix}
        F_{n+1} &amp; F_{n} \\
        F_{n} &amp; F_{n-1} 
    \end{bmatrix}
\]</span></p>
<p>Therefore if the theorem is true for <span class="math inline">\(n-1\)</span>, it is also true for <span class="math inline">\(n\)</span>. We have
already shown it is true for <span class="math inline">\(n = 1\)</span>, so by mathematical induction it is true
for all <span class="math inline">\(n \geq 1\)</span>. Q.E.D.</p>
<p>A brief word about where this matrix representation came from. Wikipedia has a
<a href="https://en.wikipedia.org/wiki/Recurrence_relation#Solving_via_linear_algebra">good explanation</a> for how any linear recurrence relation can be
expressed in matrix form and I’ve described it myself in <a href="/post/complex-r-part-2/">a prior
article</a>. Essentially, we use the first dimension to store the current
value, and the rest of the vector as <a href="https://en.wikipedia.org/wiki/Shift_register">shift registers</a> to “remember”
previous states. The recurrence relation is encoded along the first row and the
ones along the <a href="http://mathworld.wolfram.com/Subdiagonal.html">subdiagonal</a> roll the history forward. It’s actually easier
to see in higher dimensions, so here’s an example of encoding a linear
recurrence relationship which uses the four most recent numbers instead of just
two:</p>
<p><span class="math display">\[
  y_{n+1} = c_0 y_n + c_1 y_{n-1} + c_2 y_{n-2} + c_3 y_{n-3} \\  
  \iff \\
  \begin{bmatrix}
  y_{n+1} \\
  y_{n} \\
  y_{n-1} \\
  y_{n-2} \\
  y_{n-3} \\
  \end{bmatrix} = 
    \begin{bmatrix}
  c_0 &amp; c_0 &amp; c_1 &amp; c_2 &amp; c_3 \\
  1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
  \end{bmatrix}
  \begin{bmatrix}
  y_n \\
  y_{n-1} \\
  y_{n-2} \\
  y_{n-3} \\
  y_{n-4} \\
  \end{bmatrix}
\]</span></p>
<p>If we squint at <span class="math inline">\(\mathbf{F}_1\)</span>, we can see it has this form too:
the first row is <span class="math inline">\([ 1 \,\, 1 ]\)</span> because recurrence relation is simply the
sum of the previous two, while the second row <span class="math inline">\([ 1 \,\, 0 ]\)</span> contains the <span class="math inline">\(1\)</span> on the
<a href="http://mathworld.wolfram.com/Subdiagonal.html">subdiagonal</a> which “remembers” the previous value. The effect is
to advance the state of the algorithm in almost the exact same way as the
<code>interative_fib()</code> above:</p>
<p><span class="math display">\[
    \begin{bmatrix}
        1 &amp; 1 \\
        1 &amp; 0 
    \end{bmatrix}
    \begin{bmatrix}
        F_n \\
        F_{n-1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        F_n + F_{n-1} \\
        F_{n}
    \end{bmatrix}
    =
    \begin{bmatrix}
        F_{n+1} \\
        F_{n}
    \end{bmatrix}
\]</span></p>
<p>At first this may not seem at all helpful. But by framing the problem as taking
the exponent of a matrix instead of repeated addition, we can derive two much
faster algorithms: a constant time <span class="math inline">\(\mathcal{O}(n)\)</span> approximate solution using
eigenvalues, and a fast <span class="math inline">\(\mathcal{O}(n \log n)\)</span> exact solution.</p>
</div>
<div id="eigenvalue-solution" class="section level2">
<h2>Eigenvalue Solution</h2>
<p>Note that the matrix <span class="math inline">\(\mathbf{F}_1\)</span> is symmetric and real-valued. Therefore it
has real eigenvalues which we’ll call <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. The eigenvalue
decomposition allows us to diagonalize <span class="math inline">\(\mathbf{F}_1\)</span> like so:</p>
<p><span class="math display">\[
    \mathbf{F}_1 = 
    \mathbf{Q} 
    \mathbf{\Lambda}
    \mathbf{Q}^T
    =
    \mathbf{Q} 
    \begin{bmatrix}
        \lambda_1 &amp; 0 \\
        0 &amp; \lambda_2
    \end{bmatrix}
    \mathbf{Q}^T
\]</span></p>
<p>Writing <span class="math inline">\(\mathbf{F}_1\)</span> in this form makes it easy to square it:</p>
<p><span class="math display">\[
    \begin{align}
     \mathbf{F}_1^2 &amp; = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T \\
                     &amp; = \mathbf{Q} \mathbf{\Lambda}^2 \mathbf{Q}^T \\
                     &amp; = \mathbf{Q}  
                    \begin{bmatrix}
                        \lambda_1^2 &amp; 0 \\
                        0 &amp; \lambda_2^2
                    \end{bmatrix}
                    \mathbf{Q}^T
    \end{align}
\]</span></p>
<p>or to raise it to an arbitrary power:</p>
<p><span class="math display">\[
     \mathbf{F_n} 
     = \mathbf{F}_1^n 
     = \mathbf{Q} \mathbf{\Lambda}^n \mathbf{Q}^T 
     = 
        \mathbf{Q}  
        \begin{bmatrix}
            \lambda_1^n &amp; 0 \\
            0 &amp; \lambda_2^n
        \end{bmatrix}
        \mathbf{Q}^T
\]</span></p>
<p>We can calculate the two eigenvalues analytically by solving the
characteristic equation <span class="math inline">\((1-\lambda)\lambda - 1 = 0\)</span>. Since this is a quadratic
polynomial, we can use the quadratic equation to obtain both solutions in closed form:</p>
<p><span class="math display">\[
    \begin{align}
    \lambda_1 &amp; = \frac{1 + \sqrt{5}}{2} \\
    \lambda_2 &amp; = \frac{1 - \sqrt{5}}{2} 
    \end{align}
\]</span></p>
<p>Where the largest eigenvalue is in fact <span class="math inline">\(\phi\)</span>, the <a href="https://en.wikipedia.org/wiki/Golden_ratio">golden ratio</a>. The
matrix formulation is an easy way to see <a href="https://www.quickanddirtytips.com/education/math/what-is-the-golden-ratio-and-how-is-it-related-to-the-fibonacci-sequence">famous connection</a> between the
Fibonacci numbers and <span class="math inline">\(\phi\)</span>. To calculate <span class="math inline">\(F_n\)</span> for large values of <span class="math inline">\(n\)</span>, it
suffices to calculate <span class="math inline">\(\phi^n\)</span> and then do some constant time <span class="math inline">\(\mathcal{O}(1)\)</span>
bookkeeping, like so:</p>
<pre><code>import numpy as np

def eigen_fib(n):
    F1 = np.array([[1, 1], [1, 0]])
    eigenvalues, eigenvectors = np.linalg.eig(F1)
    Fn = eigenvectors @ np.diag(eigenvalues ** n) @ eigenvectors.T
    return int(np.rint(Fn[0, 1]))</code></pre>
<p>So there you have it – a <span class="math inline">\(\mathcal{O}(1)\)</span> algorithm for any Fibonacci number.
There’s just one tiny little problem with it: <span class="math inline">\(\phi\)</span>, being irrational, is not
particularly convenient for numerical analysis. If we run the
above Python program, it will use 64-bit floating point arithmetic and will
never be able to precisely represent more than 15 decimal digits. That only
lets us calculate up to <span class="math inline">\(F_{93}\)</span> before we no longer have enough precision to exactly represent it. Past <span class="math inline">\(F_{93}\)</span>, our
clever little “exact” eigenvalue algorithm is good for nothing but a rough
approximation!</p>
<p>Now, we <em>could</em> use a high precision rational numbers, but that approach turns out
to always require strictly more space and time that just sticking to integers.
So, abandoning the eigenvalue approach on the garbage heap of ivory tower
theory, let’s turn our attention to simply calculating the powers of an integer
matrix.</p>
</div>
<div id="fast-exponentiation" class="section level2">
<h2>Fast Exponentiation</h2>
<p>So far, all we’ve done is reformulate our problem so that instead of calculating
<span class="math inline">\(n\)</span> terms in a sequence using simple addition, we now have to multiply <span class="math inline">\(n\)</span>
matrices together. We’ve made things worse! Multiplication is slower than
addition, especially for large numbers, and computing the production of two <span class="math inline">\(2 \times 2\)</span> matrices requires <em>eight</em> multiplications!</p>
<p>Remain calm. There’s a <a href="https://en.wikipedia.org/wiki/Exponentiation_by_squaring">trick</a> to calculating large powers quickly. Imagine
we want to calculate <span class="math inline">\(x^n\)</span> where <span class="math inline">\(n\)</span> is a power of two: <span class="math inline">\(n = 2^m\)</span>. If
we square <span class="math inline">\(x\)</span>, then square it again, and keep doing that <span class="math inline">\(m\)</span> times, we get</p>
<p><span class="math display">\[ ((x^2)^2...)^2 = x^{2^m} = x^n \]</span></p>
<p>In other words, we only need to perform <span class="math inline">\(m = \log_2 n\)</span> matrix multiplications to
calculate <span class="math inline">\(x^n\)</span>.</p>
<p>We can generalize this to calculate any large power <span class="math inline">\(n\)</span> (not necessary a power
of two) by first finding the largest power of two less than <span class="math inline">\(n\)</span> and factoring
it out:</p>
<p><span class="math display">\[ x^n = x^{2^m} x^{n-2^m} \]</span></p>
<p>The left factor can be calculated by repeated squaring and the right factor by
can calculated by recursively applying the same trick. However, we will never
need to do that more than <span class="math inline">\(\log_2 n\)</span> times and each time the power of two gets
smaller.</p>
<p>The upshot is that we can calculate <span class="math inline">\(x^n\)</span> in <span class="math inline">\(\mathcal{O}(\log n)\)</span>
multiplications. This is mostly commonly seen in cryptography such as the <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">RSA
algorithm</a> and <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange">Diffie-Hellman key exchange</a> where it is done modulo
some large but fixed sized integer, making all the multiplications roughly
equal cost. Here, we are using multiple precision integers which are doubling
in size with each multiplication. That means abstract “multiplications” are the
wrong thing to count. We won’t get <span class="math inline">\(\mathcal{O}(\log n)\)</span> runtime
performance because the top multiplications keep getting more expensive.
Nevertheless, the squaring by exponentiation trick hugely reduces the amount of
work we have to do relative to the naive iterative solution.</p>
</div>
<div id="matrix-implementation" class="section level2">
<h2>Matrix Implementation</h2>
<p>Fun fact: Python has multiple precision baked in. If if an arithmetic operation
on Python’s <code>int()</code> type exceed the normal limits of a 64-bit integer, Python
will transparently substitute a high precision type. This makes Python a
convenient language for working with very large numbers.</p>
<p>Now, we <em>could</em> just rely on <a href="https://www.numpy.org/">NumPy</a>’s matrix multiplication, like so:</p>
<pre><code>F1 = numpy.array([[1,1],[1,]], dtype=&#39;object&#39;) 
numpy.linalg.matrix_power(F1, n)</code></pre>
<p>This works. (Although strangely enough matrix multiplication with the <code>@</code>
operator <em>doesn’t</em> work when <code>dtype='object'</code>.) As much as love numpy though,
I don’t think we need to drag it in as a dependency just to multiply <span class="math inline">\(2 \times 2\)</span> matrices when we’re not even using native integer types.</p>
<p>Plus, we’ll see see in a second that there are some optimizations we can make
that wouldn’t be possible if we let NumPy handle everything for us. So for
now, let’s implement the naive matrix algorithm in native Python; we’ll come
back and refactor in the next section.</p>
<p>First, for testing and benchmarking purposes, we’ll write a non-optimized
version that just implements matrix powers in a straightforward way:</p>
<pre><code>def matrix_multiply(A, B):
    a, b, c, d = A
    x, y, z, w = B
    
    return (
        a*x + b*z,
        a*y + b*w,
        c*x + d*z,
        c*y + d*w,
    )

def naive_matrix_power(A, m):
    if m == 0:
        return [1, 0, 0, 1]
    B = A
    for _ in range(m-1):
        B = matrix_multiply(B, A)
    return B

def naive_matrix_fib(n):
    return naive_matrix_power(F1, n)[1]</code></pre>
<p>But we’ll immediately want to move on to a version which implements
the fast exponentiation by repeated squares described above:</p>
<pre><code>def matrix_power(A, m):
    if m == 0:
        return [1, 0, 0, 1]
    elif m == 1:
        return A
    else:
        B = A
        n = 2
        while n &lt;= m:
            # repeated square B until n = 2^q &gt; m
            B = matrix_multiply(B, B)
            n = n*2
        # add on the remainder
        R = matrix_power(A, m-n//2)
        return matrix_multiply(B, R)

F1 = [1, 1, 
      1, 0]

def matrix_fib(n):
    return matrix_power(F1, n)[1]</code></pre>
</div>
<div id="implicit-matrix-form" class="section level2">
<h2>Implicit Matrix Form</h2>
<p>The above has reasonably good asymptotic performance but it bothers me that
it’s doing 8 multiplications each time. Luckily, because all Fibonacci matrices
are of a special form, we really only need to keep track of two elements in the
right-hand column of the matrix. I call this this the “implicit matrix form.”
Here is a Fibonacci matrix described with just two numbers, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
        \mathbf{F}_n 
        = \begin{bmatrix}
            \color{lightgrey} a + b &amp; a \\
            \color{lightgrey} a     &amp; b
        \end{bmatrix}
\]</span></p>
<p>We can easily work out closed forms for multiplying and squaring matrices
in this form. While the full expressions are a little complex - we never
actually need to explicitly calculate the left-hand column, a fact I
will indicate by graying those columns out:</p>
<p><span class="math display">\[
    \begin{align}
        \begin{bmatrix}
            \color{lightgrey} a + b &amp; a \\
            \color{lightgrey} a     &amp; b
        \end{bmatrix}
        &amp;
        \begin{bmatrix}
            \color{lightgrey} x + y &amp; x \\
            \color{lightgrey} x     &amp; y
        \end{bmatrix} 
        &amp; =
        \begin{bmatrix}
            \color{lightgrey} a(2x+y) + b(x+y) &amp;  a(x+y) + bx \\
            \color{lightgrey} a(x+y) + bx      &amp;  ax + by
        \end{bmatrix} \\
        &amp;
        \begin{bmatrix}
            \color{lightgrey} a + b &amp; a \\
            \color{lightgrey} a     &amp; b
        \end{bmatrix}^2
        &amp; =
        \begin{bmatrix}
            \color{lightgrey} 2a^2 + 2ab + b^2 &amp;  a^2 + 2ab \\
            \color{lightgrey} a^2 + 2ab        &amp;  a^2 + b^2 
        \end{bmatrix}
    \end{align}
\]</span></p>
<p>Using the implicit matrix form, we can multiply two
different Fibonacci matrices with just four multiplications, and we can
squaring a matrix with only three! It’s only a constant time speed-up but every
little bit helps.</p>
<pre><code>def multiply(a, b, x, y):
    return x*(a+b) + a*y, a*x + b*y

def square(a, b):
    a2 = a * a
    b2 = b * b
    ab = a * b
    return a2 + (ab &lt;&lt; 1), a2 + b2

def power(a, b, m):
    if m == 0:
        return (0, 1)
    elif m == 1:
        return (a, b)
    else:
        x, y = a, b
        n = 2
        while n &lt;= m:
            # repeated square until n = 2^q &gt; m
            x, y = square(x, y)
            n = n*2
        # add on the remainder
        a, b = power(a, b, m-n//2)
        return multiply(x, y, a, b)

def implicit_fib(n):
    a, b = power(1, 0, n)
    return a</code></pre>
<p>It would of course be possible to derive these relationships without ever
introducing the Fibonacci matrices, but I think they provides a valuable
foundation for intuition. Without that foundation, the above program seems a
little arbitrary.</p>
<p>You may be wondering why I square numbers as <code>a*a</code> instead of <code>a**2</code> or
<code>pow(a, 2)</code>, and why I use <code>ab&lt;&lt;1</code> instead of <code>2*ab</code> or <code>ab+ab</code> to double
them. The answer is simple - I benchmarked the various forms and found these
expressions to be very slightly faster, at least when using large <code>mpz()</code>
objects (which we’ll get to in a moment.)</p>
</div>
<div id="cython" class="section level2">
<h2>Cython</h2>
<p>Another thing to try – something which <em>usually</em> helps a lot –
is to try converting our program to <a href="https://cython.org/">Cython</a>.</p>
<p>Unfortunately, the one type that we want to use, Python’s native <code>int()</code> type, is
represented by Cython as a C-style int - fixed precision signed integer. It
doesn’t have Python’s ability to transparently handle large numbers. We can
either use the native C <code>long</code> in which case we run into precision problems after <span class="math inline">\(F_{93}\)</span>,
or we can continue to use the Python <code>int()</code> type in which case we gain only a modest
speed up.</p>
<pre><code>%%cython

cdef cython_multiply(a, b, x, y):
    return x*(a+b) + a*y, a*x + b*y

cdef cython_square(a, b):
    a2 = a * a
    b2 = b * b
    ab = a * b
    return a2 + (ab &lt;&lt; 1), a2 + b2

cdef cython_power(a, b, int m):
    cdef int n = 2
    if m == 0:
        return (0, 1)
    elif m == 1:
        return (a, b)
    else:
        x, y = a, b
        while n &lt;= m:
            # repeated square until n = 2^q &gt; m
            x, y = cython_square(x, y)
            n = n*2
        # add on the remainder
        a, b = cython_power(a, b, m-n//2)
        return cython_multiply(x, y, a, b)
    
cpdef cython_fib(n):
    a, b = cython_power(1, 0, n)
    return a

print(cython_fib(103))</code></pre>
<p>We still get a good boost for small numbers, but
the benefit of this quickly becomes irrelevant for large numbers.</p>
<p>Never fret, though, because we can use something even <em>better</em>.</p>
</div>
<div id="the-gnu-multiple-precision-arithmetic-library" class="section level2">
<h2>The GNU Multiple Precision Arithmetic Library</h2>
<p>The GNU Multiple Precision Arithmetic Library, or <a href="https://gmplib.org/">GMP</a> for short, is
nothing short of a work of art. Often used for calculating <span class="math inline">\(\pi\)</span> to a number
of decimal places described as “silly” by their <a href="https://gmplib.org/pi-with-gmp.html">own documentation</a>, GMP
is able to add, multiply, divide and perform arithmetic on larger and larger
numbers until your computer runs out of RAM. The multiplication algorithm used
<em>starts</em> with <a href="https://en.wikipedia.org/wiki/Karatsuba_algorithm">Karatsuba</a> - and then they get <em><a href="https://gmplib.org/manual/Multiplication-Algorithms.html">serious</a></em>.</p>
<p>It’s almost embarrassingly easy to convert our algorithm to use GMP because the
<code>mpz()</code> type is a drop-in replacement for <code>int()</code>:</p>
<pre><code>import gmpy2
from gmpy2 import mpz

def gmp_fib(n):
    a, b = power(mpz(1), mpz(0), mpz(n))
    return a</code></pre>
<p>Note that we didn’t have to define the <code>power()</code> or <code>multiply()</code> functions
again: this implementation re-uses the exact same functions we wrote for Python
native types when implementing <code>implicit_fib()</code> above. Every Python function is
a type-agnostic template function.</p>
<p>You may also wonder why the large integer type is called <code>mpz</code>: the “mp” is for
“multiple precision”, just like the “MP” in “GMP,” while the “z” stands for
<span class="math inline">\(\mathbb{Z}\)</span>, the conventional name for the set of integers. There is also <code>mpq</code>
for the set of rationals <span class="math inline">\(\mathbb{Q}\)</span> and so on.</p>
</div>
<div id="dynamic-programming-redux" class="section level2">
<h2>Dynamic Programming Redux</h2>
<p>The GMP version is really quite extraordinarily fast, but if we look at the
call graph we can still see some redundant effort. It turns out that we
are recalculating each power of two every time we need it, resulting
in this ever widening tree-shaped DFG:</p>
<div class="figure">
<img src="/post/fibonacci_files/fib_103_dfg_bad.png" title="naïve DFG for fib(103)" alt="naive DFG for fib(103)" />
<p class="caption">naive DFG for fib(103)</p>
</div>
<p>We can fix this with - you guessed it - dynamic programming! With dynamic
programming, it’s a good idea to only cache the results of sub-problems which
are likely to be re-used. Here, we can be reasonably certain that the only
results worth caching are the powers of two, so we refactor that to its
own function and apply memoization there.</p>
<pre><code># improve the algorithm slightly by caching
# and re-using powers of two. 
@lru_cache(100)
def dynamic_repeated_squares(a, b, n):
    # n must be a power of two. 
    if n == 0:
        return (0, 1)
    elif n == 1:
        return (a, b)
    return square(*dynamic_repeated_squares(a, b, n//2))
    
def dynamic_power(a, b, m):
    if m == 0:
        return (0, 1)
    elif m == 1:
        return (a, b)
    else:
        # hit the cache for powers of 2
        n = 2
        while n &lt;= m:
            n = n*2
        n = n // 2
        x, y = dynamic_repeated_squares(a, b, n)

        # add on the remainder
        a, b = dynamic_power(a, b, m-n)
        return multiply(x, y, a, b)
    
def dynamic_fib(n):
    a, b = dynamic_power(mpz(1), mpz(0), mpz(n))
    return a</code></pre>
<p>With the caching added for powers of two, we get a much smaller DFG, now an acyclic graph
with no duplicate effort at all:</p>
<div class="figure">
<img src="/post/fibonacci_files/fib_103_dfg_dynamic_programming.png" title="DFG for fib(103) with dynamic programming" alt="DFG for fib(103) with dynamic programming" />
<p class="caption">DFG for fib(103) with dynamic programming</p>
</div>
<p>It should be clear from graph that in the worst case scenario, where <span class="math inline">\(n = 2^m -1\)</span>, the cached algorithm performs a maximum of <span class="math inline">\(2m\)</span> multiplications, compared to the
<span class="math inline">\(m(m-1)/2\)</span> needed for the algorithm without caching. Despite this, the benefit
of the cache is surprisingly minor: maybe 10% in practice. That’s because
almost all the time is spent in a
handful of very large multiplications – the smaller ones just don’t matter as
much. “Logical multiplications” just isn’t the right operation to count. When
dealing with multiple precision numbers we need to look at the number of bytes
multiplied, and the number of bytes doubling with each multiplication. I’ve heard those
two effects more or less cancel out and the final algorithm is <span class="math inline">\(\mathcal{O}(n \log n)\)</span>
but won’t venture to prove it myself. It seems to roughly hold empirically:
every time <span class="math inline">\(n\)</span> goes up by a factor of 10, time increases by about 20. (See the
benchmarks below.)</p>
</div>
<div id="c-fibonacci" class="section level2">
<h2>C++ Fibonacci</h2>
<p>Now that we’ve exhausted my ideas for algorithmic optimizations, there’s really
only one thing approach left: micro-optimization. So far we’ve been working in
Python, but Python has a reputation for being slow and we did see a small
speed-up when we started using Cython. The GMP library is native to C; maybe a
C or C++ program would eliminate all the Python overhead?</p>
<p>To find out, I ported the above logic pretty faithfully to C++, almost line-for-line:</p>
<pre><code>// memoized version
ImplicitMatrix repeatedSquares(int n)
{
    // 0 squares means the original basis matrix f1
    static std::vector&lt;ImplicitMatrix&gt; cache = { {1, 0} };

    // repeatedly square as often as necessary.
    while (n &gt;= cache.size() ) {
        cache.push_back( square(cache.back()) );
    }

    // the n-th element is always f1^n.
    return cache[n];
}

ImplicitMatrix power(
    const ImplicitMatrix&amp; x,
    const bigint&amp; m)
{
    if ( m == 0 ) {
        return {0, 1};
    } else if ( m == 1 ) {
        return x;
    }

    // powers of two by iterated squaring
    // ImplicitMatrix powerOfTwo = x;
    bigint n = 2;
    int n_squares_needed = 0;
    while ( n &lt;= m ) {
        n = n*2;
        n_squares_needed++;
    //powerOfTwo = square(powerOfTwo);
    }
    ImplicitMatrix powerOfTwo = repeatedSquares(n_squares_needed);

    // recurse for remainder
    ImplicitMatrix remainder = power(x, m-n/2);

    return multiply(powerOfTwo, remainder);
}</code></pre>
<p>I installed these libraries on Debian/Ubuntu like so:</p>
<pre><code>sudo apt install libboost-all-dev libgmp-dev</code></pre>
<p>The above program was built like so:</p>
<pre><code>g++ -std=c++17 -O3 -o fib main.cpp -lgmp</code></pre>
<p>Note that <code>-O3</code> tells the compiler to apply maximum optimization
to the program. That’s also why we need the <code>volatile</code> keyword -
the optimizer notices my program doesn’t actually <em>do</em> anything
and optimizes the whole thing away!</p>
<p>The results we mildly disappointing:</p>
<pre><code>~/fib$ time ./fib 10000003

real    0m0.427s
user    0m0.360s
sys     0m0.060s

~/fib$ time ./fib 1000000003

real    1m24.088s
user    1m22.550s
sys     0m1.430s</code></pre>
<p>If this is any faster than the Python version, it can’t be be measured. This
result isn’t actually too surprising - at this point, 99.9% of computation time
is spent in the GMP multiplication routines, and only a few microseconds are
spent in Python. So we’re not going to squeeze any more performance out that way.</p>
</div>
<div id="final-python-fibonacci" class="section level2">
<h2>Final Python Fibonacci</h2>
<p>Our performance testing has revealed something interesting - there is no one
implementation which <a href="https://en.wikipedia.org/wiki/Strategic_dominance">strictly dominates</a> all the others over all possible
inputs. The simple algorithms tend to win when <span class="math inline">\(n\)</span> is small, while more complex
algorithms are able to pull ahead when <span class="math inline">\(n\)</span> is large.</p>
<p>A common way to squeeze as much performance as possible across all possible
inputs is to use a <a href="https://en.wikipedia.org/wiki/Hybrid_algorithm">hybrid algorithm</a> which selects an algorithm from a family
based on heuristics that estimate which should perform best in which regions.
A hybrid solution is the <a href="https://en.wikipedia.org/wiki/Anything_You_Can_Do_(I_Can_Do_Better)">Annie Oakley</a> solution: “Anything you can do I can do better; I can do anything better than you.”
Probably the most famous hybrid algorithm in use today is <a href="https://en.wikipedia.org/wiki/Timsort">Timsort</a>.</p>
<p>We will use earlier benchmarks to define three regions:</p>
<table>
<thead>
<tr class="header">
<th align="center">Region</th>
<th align="center">Name</th>
<th align="center">Algorithm</th>
<th align="center">Implementation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">n &lt;= 92</td>
<td align="center">Small</td>
<td align="center">Table Lookup</td>
<td align="center">Python</td>
</tr>
<tr class="even">
<td align="center">92 &lt; n &lt;= <span class="math inline">\(2^{12}\)</span></td>
<td align="center">Medium</td>
<td align="center">Implicit Matrix</td>
<td align="center">Cython</td>
</tr>
<tr class="odd">
<td align="center">n &gt; <span class="math inline">\(2^{12}\)</span></td>
<td align="center">Large</td>
<td align="center">Implicit Matrix</td>
<td align="center">GMP</td>
</tr>
</tbody>
</table>
<p>For the first region, we introduce a pre-calculated table indexed at zero which
stores every Fibonacci number small enough to fit into 64-bits.</p>
<pre><code>small_fib = [
    0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597,
    2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418,
    317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465,
    14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296,
    433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976,
    7778742049, 12586269025, 20365011074, 32951280099, 53316291173,
    86267571272, 139583862445, 225851433717, 365435296162, 591286729879,
    956722026041, 1548008755920, 2504730781961, 4052739537881, 6557470319842,
    10610209857723, 17167680177565, 27777890035288, 44945570212853,
    72723460248141, 117669030460994, 190392490709135, 308061521170129,
    498454011879264, 806515533049393, 1304969544928657, 2111485077978050,
    3416454622906707, 5527939700884757, 8944394323791464, 14472334024676221,
    23416728348467685, 37889062373143906, 61305790721611591, 99194853094755497,
    160500643816367088, 259695496911122585, 420196140727489673,
    679891637638612258, 1100087778366101931, 1779979416004714189,
    2880067194370816120, 4660046610375530309, 7540113804746346429
]</code></pre>
<p>Past that, we will use either the Cython or GMP implementation depending on
whether a better constant or better asymptotic performance is more beneficial.</p>
<pre><code>def hybrid_fib(n):
    if n &lt;= len(small_fib):
        return small_fib[n]
    elif n &lt;= 2**12:
        return cython_fib(n)
    else:
        return gmp_fib(n)</code></pre>
<p>And that, as they say, is my final answer.</p>
</div>
<div id="benchmarking-results" class="section level2">
<h2>Benchmarking Results</h2>
<p>As I’ve been implementing these, I’ve been informally testing and benchmarking them with
<a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">IPython’s %timeit magic.</a> But now that we have a large number of candidate
implementations, <em>ad hoc</em> testing is becoming tiresome. Let’s benchmark all of our
functions across a wide range of inputs to see which emerges as the leader. All of
these are measured at <span class="math inline">\(2^k-1\)</span> to force worst-case performance from the main algorithms.</p>
<div class="figure">
<img src="/post/fibonacci_files/competing_fibonacci_implementations.png" alt="competing Fibonacci implementations" />
<p class="caption">competing Fibonacci implementations</p>
</div>
<p>We can make a few observations:</p>
<ul>
<li>The naive implementation’s <span class="math inline">\(\mathcal{O}(2^n)\)</span> performance hits a wall around
100, after which it’s no longer practical.</li>
<li>The table based method actually runs out of memory before its runtime
performance becomes a problem.</li>
<li>The <code>eigen_fib()</code> implementation is basically constant time - until it starts
overflowing once it can no longer represent its solution as a 64-bit floating
point number.</li>
<li>The best asymptotic performance is from the version using both GMP and the
dynamic programming cache.</li>
<li>By construction, the “hybrid” algorithm traces out lower bound - constant
until 92, then hugs the Cython curve for a while, then switches to the
dynamic GMP solution for large numbers.</li>
</ul>
</div>
<div id="feynman-fuse-problem" class="section level2">
<h2>Feynman Fuse Problem</h2>
<p>We’ve made a lot of progress, and we’ve hit what I call a “fuse problem,” after
this anecdote from <a href="https://books.google.com/books?id=7papZR4oVssC&amp;lpg=PA103&amp;ots=euSVb9oLXZ&amp;dq=feynman%20%22altitude%22%20problem&amp;pg=PA103#v=onepage&amp;q=feynman%20%22altitude%22%20problem&amp;f=false">Surely You’re Joking, Mr. Feynman!</a>:</p>
<blockquote>
<p>The problem was to design a machine like the other one - what they called a
director - but this time I thought the problem was easier, because the
gunner would be following behind in another machine at the same altitude.
The gunner would set into my my machine his altitude and an estimate of his
disance behind the other airplane. My machine would automatically tilt the
gun up at the correct angle and set the fuse.</p>
<p>As director of this project, I would be making trips down to Aberdeen to
get the firing tables. However, they already had some preliminary data and
it turned out that the fuses they were going to use were not clock fuses,
but powder-train fuses, which didn’t work at those altitudes - they fizzled
out in the thin air.</p>
<p>I thought I only had to correct for the air resistance at different
altitudes. Instead my job was to invent a machine that would make the
shell explode at the right moment, when the fuse won’t burn!</p>
<p>I decided that was too hard for me and went back to Princeton.</p>
</blockquote>
<p>Work on a problem long enough, and every problem is a fuse problem; that is to
say, it becomes apparent that a fundamental shift in approach and a completely
different skill set is necessary to make any further progress.</p>
<p>In our case, the problem is no longer to calculate Fibonacci numbers – the
problem is now to find a way to multiply large integers together efficiently.
As far as I can tell, GMP is already <a href="https://gmplib.org/manual/Multiplication-Algorithms.html">state-of-the-art</a> when it comes to
that, and tends to come out ahead on most benchmarks.</p>
<p>In fact, it’s recently come to my attention that GMP in fact has a dedicated
<a href="https://gmplib.org/manual/Number-Theoretic-Functions.html#index-mpz_005ffib_005fui-396">Fibonacci benchmark</a>. I can’t compete with that! So I think we’ve taken
it as far as we can reasonably go.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>When I started this project, I would not have believed that my laptop could
calculate the millionth Fibonacci number in a fraction of a second. Certainly the first few
algorithms we looked at couldn’t come close to that. But my surprise should
come as no surprise.</p>
<p>New algorithms are being discovered all the time. When I graduated,
<a href="https://en.wikipedia.org/wiki/Quicksort">quicksort</a> was considered state-of-the-art. Since then, <a href="https://en.wikipedia.org/wiki/Timsort">Timsort</a> has
supplanted it in a number of standard libraries such as <a href="https://stackoverflow.com/questions/4018332/is-java-7-using-tim-sort-for-the-method-arrays-sort">Java’s.</a> Some
people believe <a href="https://agtb.wordpress.com/2010/12/23/progress-in-algorithms-beats-moore%E2%80%99s-law/">improvements in algorithms are outpacing Moore’s law.</a>
Sometimes an algorithm comes along and just blows everything else out of the water,
like John Platt’s <a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization">Sequential Minimal Optimization</a>. For a decade after
that was invented, SVM’s were considered one of the best off-the-shelf
classifiers, until <a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">even better</a> algorithms came along. Even today, the
best way to fit an <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization">ElasticNet</a> model is to reduce it to an SVM and use a
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">fast solver</a> based on SMO.</p>
<p>New algorithms take even dedicated professionals by surprise. <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Kolmogorov</a>
– perhaps one of the greatest mathematicians of all time – actually stated
the conjecture that multiplication was necessarily <span class="math inline">\(\mathcal{O}(n^2)\)</span> in a
lecture and a few weeks later his student <a href="https://en.wikipedia.org/wiki/Anatoly_Karatsuba">Karatsuba</a> showed how a few
simple additions and subtractions would allow three multiplications to do the
work of four, decreasing the bound to <span class="math inline">\(\mathcal{O}(n^{\log_2 3})\)</span>. So simple,
yet until 1960 not one mathematician had ever thought of it. And yet it is this
trick (and later more complicated versions in the same vein) that account for
the almost magical speed of the GMP library. It’s also closely related to the
same <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">divide-and-conquer strategy for matrix multiplication</a> that makes linear
algebra libraries like <a href="https://en.wikipedia.org/wiki/OpenBLAS">OpenBLAS</a> so fast.</p>
<p>The lower bounds on good algorithms can often seem impossible. It doesn’t sound
possible to search a length <span class="math inline">\(n\)</span> string for a substring of length <span class="math inline">\(m\)</span> in less
than <span class="math inline">\(\mathcal{O}(nm)\)</span>, but <a href="https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm">Rabin-Karp</a> and other similar algorithms do it
in <span class="math inline">\(\mathcal{O}(n+m)\)</span> through the clever use of a rolling hash. It doesn’t
sound possible to store and retrieve items in less than <span class="math inline">\(\mathcal{O}(\log n)\)</span>,
but hash tables do it in amortized <span class="math inline">\(\mathcal{O}(1)\)</span>. Obviously, there’s
absolutely no way to estimate the cardinality of the union of two possibly
overlapping sets in less than <span class="math inline">\(\mathcal{O}(n \log n)\)</span>… unless you use
HyperLogLog. Bloom filters let you (for the price of a small change of a false
positive but no chance of a false negative) test set membership while using
only <span class="math inline">\(\mathcal{O}(\log n)\)</span> space. How can it possibly do that? Hash functions
again. In my own work, I frequently rely on <a href="/post/ml-from-scratch-part-2-logistic-regression/">sophisticated gradient descent
algorithms</a> to fit models that would take hours or days to fit on the
same hardware if naive algorithms were used. All of these algorithms are
somewhere between magical and impossible. Yet they all work, both in theory and
practice.</p>
<p>As good as today’s hardware is, it’s often the algorithm that makes the
impossible possible.</p>
<p>The code for today’s article is available as a <a href="/post/fibonacci_files/Fibonacci.ipynb">Jupyter notebook</a> if you’d
like to hack on it. You will need to install <a href="https://gmplib.org/">GMP</a>, its Python wrapper
<a href="https://pypi.org/project/gmpy2/">gmpy2</a>, and <a href="https://cython.readthedocs.io/en/latest/src/quickstart/install.html#install">Cython</a>. I am sure there is another order of magnitude
of performance to be found somewhere.</p>
</div>

    </article>

    <hr>


    <ul class="pager article-pager">
      <li class="pager-newer">
          <a href="/post/ml-from-scratch-part-4-decision-tree/" data-toggle="tooltip" data-placement="top" title="ML From Scratch, Part 4: Decision Trees">&lt; Newer</a>
      </li>
      <li class="pager-older">
        <a href="/post/ml-from-scratch-part-3-backpropagation/" data-toggle="tooltip" data-placement="top" title="ML From Scratch, Part 3: Backpropagation">Older &gt;</a>
      </li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2023 Oran Looney
  </div>
  <ul class="site-footer-items">
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
