<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML From Scratch, Part 5: Gaussian Mixture Models - OranLooney.com</title>
  <meta property="og:title" content="ML From Scratch, Part 5: Gaussian Mixture Models - OranLooney.com" />
  <meta name="twitter:title" content="ML From Scratch, Part 5: Gaussian Mixture Models - OranLooney.com" />
  <meta name="description" content="Consider the following motivating dataset:
Unlabled Data
 It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. The question is: is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?">
  <meta property="og:description" content="Consider the following motivating dataset:
Unlabled Data
 It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. The question is: is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?">
  <meta name="twitter:description" content="Consider the following motivating dataset:
Unlabled Data
 It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple …">
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>
 
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/search/" title="Site Search"><i class="fa fa-search"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//honeycode.tumblr.com/" target="_blank" title="Honeycode Microblog">
      <svg class="svg-inline--fa fa-w-12" aria-hidden="true" data-prefix="fab" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="20 50 350 450" data-fa-i2svg="">
<path fill="currentColor" d="M193 97l86.6 50v100l-86.6 50l-86.6 -50v-100zM285 255l86.6 50v100l-86.6 50l-86.6 -50v-100zM100 255l86.6 50v100l-86.6 50l-86.6 -50v-100z"></path>
</svg>
    </a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>
  </ul>

  </nav>
</header>


  <div class="main" role="main">
    <article class="article">
      <img src="/post/ml-from-scratch-part-5-gmm_files/lead.jpg" class="article-image" />
      
      <h1 class="article-title">ML From Scratch, Part 5: Gaussian Mixture Models</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-author">
            by <a href="/about/">Oran Looney</a>
        </li>
        <li class="article-meta-date"><time>June 5, 2019</time></li>
        <li class="article-meta-tags">
          <a href="/tags/python/">
            <i class="fas fa-tag"></i>
            Python
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/statistics/">
            <i class="fas fa-tag"></i>
            Statistics
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/from-scratch/">
            <i class="fas fa-tag"></i>
            From Scratch
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/machine-learning/">
            <i class="fas fa-tag"></i>
            Machine Learning
          </a>&nbsp;
        </li>
      </ul>
      
<aside class="toc">
  
</aside>
      <p>Consider the following motivating dataset:</p>
<div class="figure">
<img src="/post/ml-from-scratch-part-5-gmm_files/unlabled.png" alt="Unlabled Data" />
<p class="caption">Unlabled Data</p>
</div>
<p>It is apparent that these data have some kind of structure; which is to say,
they certainly are not drawn from a uniform or other simple distribution. In
particular, there is at least one cluster of data in the lower right which is
clearly separate from the rest. The question is: is it possible for a machine
learning algorithm to automatically discover and model these kinds of
structures without human assistance?</p>
<p>Every model we’ve look at so far has assumed that we have a clear definition of
the thing we are trying to predict <strong>and</strong> that we already know the correct
answer for every example in the training set. A problem of the form “just find
me <em>some</em> kind interesting relationships or structure, any will do” does not
fit into this framework because no “true” labels are known in advance. More
formally: every problem so far has been a “supervised” learning problem, where
the training set consists of <em>labeled</em> pairs <span class="math inline">\((X, Y)\)</span> and the task was to
predict <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span>. The problem of discovering interesting structure or
relationships from <em>unlabeled</em> examples <span class="math inline">\(X\)</span> is called the “unsupervised”
learning problem, and calls for a different set of techniques and algorithms
entirely.</p>
<div id="types-of-unsupervised-learning" class="section level2">
<h2>Types of Unsupervised Learning</h2>
<p>There are two broad approaches to unsupervised learning: dimensionality
reduction and cluster analysis.</p>
<p>In dimensionality reduction we seek a function <span class="math inline">\(f : \mathbb{R}^a \mapsto \mathbb{R}^b\)</span> where <span class="math inline">\(a\)</span> is the dimension of the original data <span class="math inline">\(\mathbf{X}\)</span> and
<span class="math inline">\(b\)</span> is usually much smaller than <span class="math inline">\(a\)</span>. The classic example of a dimensionality
reduction algorithm is <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> but there are many others, including
non-linear techniques like <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>, topic models like <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA</a>, and
most examples of representation learning such as <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>. The basic
idea is that by reducing to a lower dimensional space we somehow capture the
essential characteristics of each data point while getting rid of noise,
multicollinearity, and non-essential features. Furthermore, it should be
possible to approximately reconstruct the original data point in the original
<span class="math inline">\(a\)</span>-dimensional space from just its compressed <span class="math inline">\(b\)</span>-dimensional representation
with minimal loss. Depending on the specific technique used, the lower
dimensional space may also be designed to have desirable properties like an
isotropic/spherical covariance matrix or a meaningful distance function where
data points that a human would agree are “similar” are close together. We will
return to dimensionality reduction in some future article.</p>
<p>The second approach to unsupervised learning is called clustering and is
characterized by seeking a function <span class="math inline">\(f : \mathbb{R}^a \mapsto \{1,2, ..., k\}\)</span>
which maps each data point to exactly one of <span class="math inline">\(k\)</span> possible classes. The classic
example of a clustering algorithm is <a href="https://en.wikipedia.org/wiki/K-means_clustering"><span class="math inline">\(k\)</span>-means</a>. Reducing rich,
multivariate data to a small finite number of possibilities seems extreme, but
for that same reason it can be extremely clarifying as well. In this article we
will implement on particular clustering model called the <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Gaussian mixture
model</a>, or just GMM for short.</p>
</div>
<div id="gaussian-mixture-models" class="section level2">
<h2>Gaussian Mixture Models</h2>
<p>The Gaussian mixture model is simply a “mix” of Gaussian distributions. In this
case, “Gaussian” means the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal distribution</a>
<span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}, \Sigma)\)</span> and “mixture” means that several
different gaussian distributions, all with different mean vectors
<span class="math inline">\(\boldsymbol{\mu}_j\)</span> and different covariance matrices <span class="math inline">\(\Sigma_j\)</span>, are combined
by taking the weighted sum of the probability density functions:</p>
<p><span class="math display">\[ \begin{align}
    f_{GMM}(\mathbf{x}) = \sum^k_{j=1} \phi_j f_{\mathcal{N}(\boldsymbol{\mu}_j, \Sigma_j)}(\mathbf{x}) \tag{1}
   \end{align}
\]</span></p>
<p>subject to:</p>
<p><span class="math display">\[
  \sum_{j=1}^k \phi_j = 1 \tag{2}
\]</span></p>
<p>A single multivariate normal distribution has a single “hill” or “bump” located
at <span class="math inline">\(\boldsymbol{\mu}_i\)</span>; in contrast, a GMM is a multimodal distribution with
on distinct bump per class. (Sometimes you get fewer than <span class="math inline">\(k\)</span> distinct local
maxima in the p.d.f., if the bumps are sufficiently close together or if the
weight of one class is zero or nearly so, but in general you get <span class="math inline">\(k\)</span> distinct
bumps.) This makes it well suited to modeling data like that seen in our
motivating example above, where there seems to be more than one region on high
density.</p>
<p><a href="https://mathematica.stackexchange.com/questions/15055/finding-distribution-parameters-of-a-gaussian-mixture-distribution"><img src="/post/ml-from-scratch-part-5-gmm_files/gmm_pdf.png" alt="GMM p.d.f." /></a></p>
<p>We can view this is as a two-step generative process. To generate the <span class="math inline">\(i\)</span>-th example:</p>
<ol style="list-style-type: decimal">
<li>Sample a random class index <span class="math inline">\(C_i\)</span> from the categorical distribution parameterized by <span class="math inline">\(\boldsymbol{\phi} = (\phi_1, ... \phi_k)\)</span>.</li>
<li>Sample a random vector <span class="math inline">\(\mathbf{X}_i\)</span> from the multivariate distribution associated to the <span class="math inline">\(C_i\)</span>-th class.</li>
</ol>
<p>The <span class="math inline">\(n\)</span> independent samples <span class="math inline">\(\mathbf{X}_i\)</span> are the row vectors of the matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Symbolically, we write:</p>
<p><span class="math display">\[ \begin{align}
    C_i &amp; \sim \text{Categorical}(k, \boldsymbol{\phi}) \tag{3} \\
    \mathbf{X}_i &amp; \sim \mathcal{N}(\boldsymbol{\mu}_{C_i}, \Sigma_{C_i}) \tag{4} \\
   \end{align}
\]</span></p>
<p>To fit a GMM model to a particular dataset, we attempt to find the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a> of the parameters <span class="math inline">\(\Theta\)</span>:</p>
<p><span class="math display">\[\Theta = \{ \mathbf{\mu}_1, \Sigma_1, ..., \mathbf{\mu}_k, \Sigma_k \} \tag{5} \]</span></p>
<p>Because the <span class="math inline">\(n \times m\)</span> example matrix <span class="math inline">\(\mathbf{X}\)</span>
is assumed to be a realization of <span class="math inline">\(n\)</span> i.i.d. samples from <span class="math inline">\(f_{GMM}(\mathbf{x})\)</span>, we can write down our likelihood function as</p>
<p><span class="math display">\[
  \mathcal{L}(\Theta; \mathbf{X}) = P(\mathbf{X};\Theta) = \prod_{i=1}^n \sum_{j=1}^k P(C_i=j) P(\mathbf{X}_i|C_i=j) \tag{6}
\]</span></p>
<p>We know that <span class="math inline">\(\mathbf{X}_i\)</span> has a multivariate normal distribution with
parameters determined by the class, so the conditional probability
<span class="math inline">\(P(\mathbf{X}_i|C_i=j)\)</span> can be written down pretty much directly from the
definition:</p>
<p><span class="math display">\[
  P(\mathbf{X}_i|C_i=j) = \frac{1}{\sqrt{(2\pi)^k |\Sigma_j|}} \text{exp}\Bigg( 
    - \frac{(\mathbf{X}_i - \boldsymbol{\mu}_j)^T \Sigma_j^{-1} (\mathbf{X}_i - \boldsymbol{\mu}_j) }
         {2} 
  \Bigg) \tag{7}
\]</span></p>
<p>Obtaining a formula for <span class="math inline">\(P(C_i=j|\mathbf{X}_i)\)</span> requires a little more work. We
know that the unconditional probability is given by the parameter vector
<span class="math inline">\(\boldsymbol{\phi}\)</span>:</p>
<p><span class="math display">\[
P(C_i = j) = \phi_j \tag{8}
\]</span></p>
<p>So using Bayes’ theorem, we can write this in terms of equation (7):</p>
<p><span class="math display">\[
\begin{align}
P(C_i=j|\mathbf{X}_i) 
  &amp; = \frac{P(C_i=j) P(\mathbf{X}_i|C_i=j)}
           {P(\mathbf{X}_i)} \\
  &amp; = \frac{ \phi_j P(\mathbf{X}_i|C_i=j)}
           {\sum_{l=1}^k P(\mathbf{X}_i|C_i=l)} \\
\end{align} \tag{9}
\]</span></p>
<p>If we substituted equation (7) into (9) we could get a more explicit but very
ugly formula, so I leave that to the reader’s imagination.</p>
<p>Equations (6), (7), and (9), when taken together, constitute the complete
likelihood function <span class="math inline">\(\mathcal{L}(\Theta;\mathbf{X})\)</span>. However, these equations
have a problem - they depend on the unknown random variable <span class="math inline">\(C_i\)</span>. This
variable tells us which class each <span class="math inline">\(\mathbf{X}_i\)</span> was drawn from and makes it
much easier to reason about the distribution, but we don’t actually know what
<span class="math inline">\(C_i\)</span> is for any <span class="math inline">\(i\)</span>. This is called a <a href="https://en.wikipedia.org/wiki/Latent_variable">latent random variable</a> and its
presence in our model causes a kind of chicken-and-egg problem. If we knew
<span class="math inline">\(\boldsymbol{\mu}_j\)</span> and <span class="math inline">\(\Sigma_j\)</span> for <span class="math inline">\(j = (1, 2, ..., k)\)</span> then we could make
a guess about what <span class="math inline">\(C_i\)</span> is by looking at which <span class="math inline">\(\boldsymbol{\mu}_j\)</span> is closest
to <span class="math inline">\(\mathbf{X}_i\)</span>. If we knew <span class="math inline">\(C_i\)</span>, we could estimate <span class="math inline">\(\boldsymbol{\mu}_j\)</span> and
<span class="math inline">\(\Sigma_j\)</span> by simply taking the mean and covariance over all <span class="math inline">\(X_i\)</span> where <span class="math inline">\(C_i = j\)</span>. But how can we estimate these two sets of parameters together, if we don’t
know either when we start?</p>
</div>
<div id="the-em-algorithm" class="section level2">
<h2>The EM Algorithm</h2>
<p>The solution to our chicken-and-egg dilemma is an iterative algorithm called
the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation-maximization algorithm</a>, or EM algorithm for short. The EM
algorithm is actually a meta-algorithm: a very general strategy that can be
used to fit many different types of latent variable models, most famously
<a href="https://en.wikipedia.org/wiki/Factor_analysis">factor analysis</a> but also the <a href="https://courses.cs.washington.edu/courses/cse590q/04au/papers/WinklerEM.pdf">Fellegi-Sunter</a> record linkage
algorithm, <a href="https://en.wikipedia.org/wiki/Item_response_theory">item response theory</a>, and of course <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Gaussian mixture
models</a>.</p>
<p>The EM algorithm requires us to introduce a pseudo-parameter to model the
unknown latent variables <span class="math inline">\(C_i\)</span>. Because <span class="math inline">\(C_i\)</span> can take on <span class="math inline">\(k\)</span> discrete values,
this new parameter will be a <span class="math inline">\(n \times k\)</span> matrix where each element <span class="math inline">\(w_{ij}\)</span> is
an estimate of <span class="math inline">\(P(C_i = j|\mathbf{X}_i;\theta)\)</span>. Each element of this matrix
represents the probability that the <span class="math inline">\(i\)</span>-th data point came from cluster <span class="math inline">\(j\)</span>.
This pseudo-parameter is only used when fitting the model and will be discarded
afterwards; in that sense it is not a true parameter of the model.</p>
<p>The EM algorithm then proceeds iteratively, with each iteration being divided
into two steps: the E-step and the M-step. I will describe these in broad
strokes first, so you can get a feel for the overall intent of the algorithm,
then we will study each in more detail in the following sections.</p>
<p>In the E-step, we use our current best knowledge of the centers and shapes of
each cluster to update our estimates of which data point came from which class.
Concretely, we hold <span class="math inline">\(\boldsymbol{\mu}_j\)</span> and <span class="math inline">\(\Sigma_j\)</span> fixed and update
<span class="math inline">\(w_{ij}\)</span> and <span class="math inline">\(\boldsymbol{\phi}\)</span>.</p>
<p>In the M-step, we use our current best knowledge of which class each point
belongs to to update and improve our estimates for the center and shape of each
cluster. Concretely, we use <span class="math inline">\(w_{ij}\)</span> as <em>sample weights</em> when updating
<span class="math inline">\(\boldsymbol{\mu}_j\)</span> and <span class="math inline">\(\Sigma_j\)</span> by taking weighted averages over <span class="math inline">\(X\)</span>. For
example, if <span class="math inline">\(w_11 = 0.01\)</span> and <span class="math inline">\(w_11 = 0.99\)</span> we know that the data point <span class="math inline">\(X_1\)</span>
is unlikely to be in class 1, but very likely to be in class 2. Therefore, when
estimating the center of the first class <span class="math inline">\(\boldsymbol{\mu}_1\)</span> we give <span class="math inline">\(X_1\)</span>
almost negligible weight, but when estimating the center of the second class
<span class="math inline">\(\boldsymbol{\mu}_2\)</span> we give <span class="math inline">\(X_1\)</span> almost full weight. This “pulls” the center
of each cluster towards those data points which are considered likely to be
part of that cluster.</p>
<p>Visually, the iterative process looks something like this:</p>
<p><a title="Chire [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif"><img width="256" alt="EM Clustering of Old Faithful data" src="https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif"></a></p>
<p>With each iteration, the algorithm improves its estimate of where the clusters
are, which in turn allows it to make better guesses about which points are from
which clusters, which in turn allows it to further refine its estimate of the
center and shape of each cluster, and so on <em>ad infinitum</em>.</p>
<p>This process is guaranteed to converge a (local) maximum likelihood because of
the ratchet principle: at each step, likelihood can only increase and never
decrease. This can be viewed as a type of <a href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate ascent</a>. These
maxima are not unique, and GMM will tend to converge to different final
solutions depending on initial conditions.</p>
<p>One resource on GMM and the EM algorithm I used was this <a href="https://youtu.be/ZZGTuAkF-Hw?t=2108">Stanford lecture by
Andrew Ng</a>. I’ve linked to the part of the
lecture where he shows this update step because that is most relevant to
implementing the algorithm but the whole lecture is worth watching if you want
to understand the concepts. Another good resource on the fundamentals of the EM
algorithm is this <a href="http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/em-baumwelch.pdf">slide deck</a>; it provides a simple example that can be
worked by hand which I found to be a great way to build intuition before
tackling the much more complicated problem of applying the EM algorithm to GMM.</p>
<p>We will now treat the E-step and M-step for the particular case of the GMM in
detail.</p>
</div>
<div id="e-step" class="section level2">
<h2>E-step</h2>
<p>Given the that centroid <span class="math inline">\(\boldsymbol{\mu}_j\)</span> and covariance matrix <span class="math inline">\(\Sigma_j\)</span>
for class <span class="math inline">\(j\)</span> is fixed, we can update <span class="math inline">\(w_{ij}\)</span> by simply calculating the
probability that <span class="math inline">\(X_i\)</span> came from each class and normalizing:</p>
<p><span class="math display">\[ w_{ij} = \frac{ P(X_i|K=j) }{ P(K_i) } = \frac{ P(X_i|K=j) }{ \sum_{l=1}^k P(X_i|K=l) } \tag{10} \]</span></p>
<p>The conditional probablity <span class="math inline">\(P(\mathbf{X}_i|K=j)\)</span> is simply the multivariate
normal distribution <span class="math inline">\(\mathbf{X}_i ~ \mathcal{N}(\mu_i, \Sigma_i)\)</span> so we can use
equation (4) above to calculate the probability density for each class, and
then divide through by the total to normalize each row of <span class="math inline">\(\mathbf{X}\)</span> to 1.
This gives us a concrete formula for the update to <span class="math inline">\(w_ij\)</span>:</p>
<p><span class="math display">\[ w_{ij} = \frac{ f_{\mathcal{N}(\mu_i, \Sigma_i)}(\mathbf{X}_i) }
                 { \sum_{l=1}^k f_{\mathcal{N}(\mu_l, \Sigma_l)}(\mathbf{X}_i) } \tag{11}
\]</span></p>
<p>The probability of each class <span class="math inline">\(\phi\)</span> can then be estimated by averaging over
all examples in the training set:</p>
<p><span class="math display">\[ \phi_j = \sum_{i=1}^n w_{ij} \tag{12} \]</span></p>
</div>
<div id="m-step" class="section level2">
<h2>M-step</h2>
<p>Forget about the past estimates we had for <span class="math inline">\(\boldsymbol{\mu}_j\)</span> or <span class="math inline">\(\Sigma\)</span>.
Unlike gradient descent, the EM algorithm does not proceed by making small
changes to the previous iteration’s parameter estimates - instead, it makes a
bold leap all the way to the <em>exact</em> estimate - but only in certain dimensions.
In the M-step, we will calculate the ML estimates for <span class="math inline">\(\boldsymbol{\mu}_j\)</span> or
<span class="math inline">\(\Sigma\)</span> assuming that <span class="math inline">\(w_{ij}\)</span> is held constant.</p>
<p>How can we make such a leap? Well, we have a matrix of <span class="math inline">\(n\)</span> observations
<span class="math inline">\(\mathbf{X}_i\)</span> with weights <span class="math inline">\(w_i\)</span> which we believe came from a multivariate
distribution <span class="math inline">\(\mathcal{N}(\vec{\mu}, \mathbb{\Sigma})\)</span>. That means we can use
the familiar formulas:</p>
<p><span class="math display">\[ \boldsymbol{\mu}_j = {1 \over {n}}\sum_{i=1}^n w_{ij} \mathbf{X}_i \tag{13} \]</span></p>
<p><span class="math display">\[ \Sigma_j = \frac{1}{n} \sum_{i=1}^n w_{ij} ( \mathbf{X}_i - \boldsymbol{\mu}_j )( \mathbf{X}_i - \boldsymbol{\mu}_j )^T \tag{14} \]</span></p>
<p>These are in fact the <a href="https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/gaussian.pdf">ML estimate</a> for these parameters for the
multivariate normal distribution. As such, we don’t need to worry about
learning rate or gradients as we would with gradient descent because these
estimates are already maximal! This is one of the neatest things about this
algorithm.</p>
</div>
<div id="implementation" class="section level2">
<h2>Implementation</h2>
<p>Turning the above mathematics into a working implementation is straight
forward. The below program corresponds almost one-to-one (one line of code for
one equation) with the above mathematics. The equations (11), (12) are used in
the <code>e_step()</code> method and equations (13) and (14) are used in the <code>m_step()</code>
method.</p>
<p>One detail I did not treat above is initialization - while <span class="math inline">\(\boldsymbol{\phi}\)</span>
and <span class="math inline">\(w_{ij}\)</span> can use simple uniform initialization, for <span class="math inline">\(\boldsymbol{\mu}\)</span> it
is better to choose a random index <span class="math inline">\(i_j\)</span> uniformly from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span> for each
class and then initialize <span class="math inline">\(\boldsymbol{\mu}_j = X_{i_j}\)</span>. This ensures that
each cluster centroid is inside the support of the underlying distribution and
that they are initially spread out randomly throughout the space.</p>
<pre><code>import numpy as np
from scipy.stats import multivariate_normal

class GMM:
    def __init__(self, k, max_iter=5):
        self.k = k
        self.max_iter = int(max_iter)

    def initialize(self, X):
        self.shape = X.shape
        self.n, self.m = self.shape

        self.phi = np.full(shape=self.k, fill_value=1/self.k)
        self.weights = np.full( shape=self.shape, fill_value=1/self.k)
        
        random_row = np.random.randint(low=0, high=self.n, size=self.k)
        self.mu = [  X[row_index,:] for row_index in random_row ]
        self.sigma = [ np.cov(X.T) for _ in range(self.k) ]

    def e_step(self, X):
        # E-Step: update weights and phi holding mu and sigma constant
        self.weights = self.predict_proba(X)
        self.phi = self.weights.mean(axis=0)
    
    def m_step(self, X):
        # M-Step: update mu and sigma holding phi and weights constant
        for i in range(self.k):
            weight = self.weights[:, [i]]
            total_weight = weight.sum()
            self.mu[i] = (X * weight).sum(axis=0) / total_weight
            self.sigma[i] = np.cov(X.T, 
                aweights=(weight/total_weight).flatten(), 
                bias=True)

    def fit(self, X):
        self.initialize(X)
        
        for iteration in range(self.max_iter):
            self.e_step(X)
            self.m_step(X)
            
    def predict_proba(self, X):
        likelihood = np.zeros( (self.n, self.k) )
        for i in range(self.k):
            distribution = multivariate_normal(
                mean=self.mu[i], 
                cov=self.sigma[i])
            likelihood[:,i] = distribution.pdf(X)
        
        numerator = likelihood * self.phi
        denominator = numerator.sum(axis=1)[:, np.newaxis]
        weights = numerator / denominator
        return weights
    
    def predict(self, X):
        weights = self.predict_proba(X)
        return np.argmax(weights, axis=1)</code></pre>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model Evaluation</h2>
<p>We’ll use the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris</a> dataset as a test case. This is the same
dataset used as a motivating example at the beginning of the article, although
I did not name it at that time. The iris dataset has labels, but we won’t
expose them to the GMM model. However, we will use these labels in the next
section to discuss the question, “were we able to discover the class labels
through unsupervised learning?”</p>
<pre><code>from scipy.stats import mode
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data</code></pre>
<p>Fit a model:</p>
<pre><code>np.random.seed(42)
gmm = GMM(k=3, max_iter=10)
gmm.fit(X)</code></pre>
<p>Plot the clusters. Each color is a cluster found by GMM:</p>
<pre><code>def jitter(x):
    return x + np.random.uniform(low=-0.05, high=0.05, size=x.shape)

def plot_axis_pairs(X, axis_pairs, clusters, classes):
    n_rows = len(axis_pairs) // 2
    n_cols = 2
    plt.figure(figsize=(16, 10))
    for index, (x_axis, y_axis) in enumerate(axis_pairs):
        plt.subplot(n_rows, n_cols, index+1)
        plt.title(&#39;GMM Clusters&#39;)
        plt.xlabel(iris.feature_names[x_axis])
        plt.ylabel(iris.feature_names[y_axis])
        plt.scatter(
            jitter(X[:, x_axis]), 
            jitter(X[:, y_axis]), 
            #c=clusters, 
            cmap=plt.cm.get_cmap(&#39;brg&#39;),
            marker=&#39;x&#39;)
    plt.tight_layout()
    
plot_axis_pairs(
    X=X,
    axis_pairs=[ 
        (0,1), (2,3), 
        (0,2), (1,3) ],
    clusters=permuted_prediction,
    classes=iris.target)</code></pre>
<div class="figure">
<img src="/post/ml-from-scratch-part-5-gmm_files/gmm_clusters.png" alt="GMM Clusters" />
<p class="caption">GMM Clusters</p>
</div>
<p>Well, the model certainly found <em>something.</em></p>
<p>One thing we can say for sure is that the GMM model does find clusters of
related points. It does a particularly good job placing the visually separate
points in their own (blue) cluster, but the story with the other two clusters
in the upper right is less clear-cut.</p>
</div>
<div id="comparing-to-true-class-labels" class="section level2">
<h2>Comparing to True Class Labels</h2>
<p>Are the clusters discovered by the GMM model <em>meaningful</em>? Are they <em>correct</em>?
For a real-world unsupervised learning problem, these questions can be hard to
answer.</p>
<p>However, it so happens that the iris dataset we used <em>is</em> actually labeled.
True, we didn’t make use of these labels when training the GMM model.
Furthermore, those classes <em>are</em> associated with different distributions in the
4 observed variables in a way that closely matches the assumptions of the GMM.
So even if we can’t ask about “meaning” and “correctness”, we can at least ask
a closely related question: “did this unsupervised learning algorithm
(re-)discover the known structure of this (iris) data set?”</p>
<p>First, a bit of book-keeping. The cluster indexes found by the model are in
random order. For convenience when comparing them to true class labels, we will
permute them to be as similar as possible to true class labels. All this is
doing is swapping, say, 0 for 2 so that 0 means the same thing for both the
clusters and for the original class labels. It’s not important, but it does
make comparisons a little bit easier.</p>
<pre><code>permutation = np.array([
    mode(iris.target[gmm.predict(X) == i]).mode.item() 
    for i in range(gmm.k)])
permuted_prediction = permutation[gmm.predict(X)]
print(np.mean(iris.target == permuted_prediction))
confusion_matrix(iris.target, permuted_prediction)

0.96
array([[50,  0,  0],
       [ 0, 44,  6],
       [ 0,  0, 50]])</code></pre>
<p>For the random seed 42 (used above when the trained the GMM model) this results
in the very promising 96% agreement! However, if we 1,000 random trials,
varying the seed each time, we can see that cluster-to-labels agreement
actually varies at random from 0.52 to 0.99 with a mean of 0.74.</p>
<div class="figure">
<img src="/post/ml-from-scratch-part-5-gmm_files/accuracy_histogram.png" alt="Accuracy Histogram" />
<p class="caption">Accuracy Histogram</p>
</div>
<p>This is a little disappointing. We started from a dataset which really was the
aggregation of three different classes, and while our unsupervised learning
algorithm was discover three clusters, the agreement between reality and our
model is only around 3/4. That means we can’t reliably reconstruct the true
structure of this dataset using this technique. In contrast, a supervised
learning algorithm could have easily found a class boundary with an accuracy of
99%. That suggests that if we run an unsupervised learning algorithm on a
real-world data set and it finds some clusters for us, we should be suspicious
that they represent “true” classes in the real world. In fact, unsupervised
learning algorithms are subject to a large number of caveats and limitations
which I’ll digress briefly to enumerate.</p>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<p>All unsupervised learning methods known today share certain limitations.</p>
<p>First, they tend to rely on the researcher choosing certain arbitrary
complexity parameters such as the number of clusters <span class="math inline">\(k\)</span>. Worse still, while
there are techniques for picking these complexity parameters, they are
heuristic and often unsatisfying in practice. It can be very hard to tell if an
unsupervised learning method is “overfitting”, because “overfit” doesn’t even
have a precise definition for unsupervised learning problems.</p>
<p>Second, there are no hard metrics like accuracy or AUC that let you compare
models across different families. While each unsupervised learning algorithm
will have its own internal metrics which they try to optimize such as variance
explained or perplexity, these usually can’t be meaningful compare two models
that use two different algorithms or with different complexity parameters. This
makes model selection a fundamentally subjective task - to decide that, say,
t-SNE is doing a better job than <span class="math inline">\(k\)</span>-means on a given data set, the modeler is
often reduced to eyeballing the output.</p>
<p>Third and finally, the factors and/or clusters discovered by unsupervised
learning algorithms are often unsatisfying or counter-intuitive and don’t
necessarily line up with human intuition. Another way of saying the same thing
is that if a human goes through and creates labels <span class="math inline">\(\mathbf{Y}\)</span> for the
training set <span class="math inline">\(\mathbf{X}\)</span> after the unsupervised learning algorithm has been
applied to it, they are not very likely to come up with the same factors or
clusters. In general, humans tend to come up with rules that “make sense” but
don’t explain as much variance as possible, while algorithms tend to find
“deep” features that do explain a lot of variance but have complicated
definitions that are hard to make sense of.</p>
<p>These seem like serious criticisms; does this mean we shouldn’t use
unsupervised learning? Well, I won’t tell you that you <em>categorically</em> should
never use it, but you should know what you’re getting into. By default, it
tends to produce low-quality, hard-to-interpret models that cannot really be
defended due to number of subjective decisions needed to make them work at all.</p>
<p>On the other hand, unsupervised learning can be extremely helpful during
exploratory research; also, in the form of representation learning, it can
sometimes accelerate learning or improve performance, or allow models to
generalize from an extremely limited labeled training set. For example, a
sentiment analysis model trained on only a few hundred reviews may only see the
word “sterling” once, but if it uses a word embedding model like
<a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>, it will understand that “stupendous” is broadly a synonym for
“good” or “great”, and will therefore be able to correctly classify a future
example with the word “stupendous” - which did not appear even once in the
training set - as likely having positive sentiment. While success stories like
this are possible, in general unsupervised learning requires more expertise,
more manual tuning, and more input from domain experts in order to create
value, when compared to supervised learning projects.</p>
<p>Unfortunately, we do not always have the labels necessary for supervised
learning, and the datasets available may be too large, too high dimensional, or
too sparse to be amenable to traditional techniques; it is in these situations
where the benefits of unsupervised learning can outweigh the negatives.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this article, we have seen how unsupervised learning differs from supervised
learning and the challenges that come along with that. We discussed a method
for posing an unsupervised learning problem as an maximum likelihood
optimization, and described and implemented the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM algorithm</a> often used
to solve these otherwise intractable problems. We made the EM algorithm
concrete by implementing one particular latent variable model, the <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Gaussian
mixture model</a>, a powerful unsupervised clustering algorithm. We’ve seen
first hand that the clusters identified by GMMs don’t always line up with what
we believe the true structure to be; this lead to a broader discussion of the
limitations of unsupervised learning algorithms and the difficulty getting
value out of them.</p>
<p>In the <a href="/post/ml-from-scratch-part-6-pca/">next article</a> in this series, we’ll continue our discussion of
unsupervised learning algorithms by implementing the <em>other</em> kind of
unsupervised learning algorithm besides clustering: a dimensionality reduction
algorithm.</p>
</div>

    </article>

    <hr>


    <ul class="pager article-pager">
      <li class="pager-newer">
          <a href="/post/slow-fibonacci/" data-toggle="tooltip" data-placement="top" title="A Seriously Slow Fibonacci Function">&lt; Newer</a>
      </li>
      <li class="pager-older">
        <a href="/post/adaptive-basis-functions/" data-toggle="tooltip" data-placement="top" title="Adaptive Basis Functions">Older &gt;</a>
      </li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2024 Oran Looney
  </div>
  <ul class="site-footer-items">
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
