<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A Picture is Worth 227 Words - Speculating About GPT-4o Internals - OranLooney.com</title>
  <meta property="og:title" content="A Picture is Worth 227 Words - Speculating About GPT-4o Internals - OranLooney.com" />
  <meta name="twitter:title" content="A Picture is Worth 227 Words - Speculating About GPT-4o Internals - …" />
  <meta name="description" content="DRAFT DO NOT PUBLISH
Problem Statement TODO: punchy intro
GPT-4o charges 170 tokens to process each $512 \times 512$ tile used in high-res mode. (Ignoring the 85 token for the low-res &ldquo;master thumbnail&rdquo;.) At 0.75 tokens/word, this suggests a picture is worth about 227 words - only a factor of four off from the traditional saying.
OK, but why 170? It&rsquo;s an oddly specific number. It&rsquo;s not power of 2 or a multiple of 5 the way most &ldquo;round&rdquo; numbers are.">
  <meta property="og:description" content="DRAFT DO NOT PUBLISH
Problem Statement TODO: punchy intro
GPT-4o charges 170 tokens to process each $512 \times 512$ tile used in high-res mode. (Ignoring the 85 token for the low-res &ldquo;master thumbnail&rdquo;.) At 0.75 tokens/word, this suggests a picture is worth about 227 words - only a factor of four off from the traditional saying.
OK, but why 170? It&rsquo;s an oddly specific number. It&rsquo;s not power of 2 or a multiple of 5 the way most &ldquo;round&rdquo; numbers are.">
  <meta name="twitter:description" content="DRAFT DO NOT PUBLISH
Problem Statement TODO: punchy intro
GPT-4o charges 170 tokens to process each $512 \times 512$ tile used in high-res mode. (Ignoring the 85 token for the low-res &ldquo;master …">
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>
 
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/search/" title="Site Search"><i class="fa fa-search"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//honeycode.tumblr.com/" target="_blank" title="Honeycode Microblog">
      <svg class="svg-inline--fa fa-w-12" aria-hidden="true" data-prefix="fab" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="20 50 350 450" data-fa-i2svg="">
<path fill="currentColor" d="M193 97l86.6 50v100l-86.6 50l-86.6 -50v-100zM285 255l86.6 50v100l-86.6 50l-86.6 -50v-100zM100 255l86.6 50v100l-86.6 50l-86.6 -50v-100z"></path>
</svg>
    </a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>
  </ul>

  </nav>
</header>


  <div class="main" role="main">
    <article class="article">
      <img src="/post/gpt-cnn_files/lead.jpg" class="article-image" />
      
      <h1 class="article-title">A Picture is Worth 227 Words - Speculating About GPT-4o Internals</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-author">
            by <a href="/about/">Oran Looney</a>
        </li>
        <li class="article-meta-date"><time>May 27, 2024</time></li>
        <li class="article-meta-tags">
          <a href="/tags/machine-learning/">
            <i class="fas fa-tag"></i>
            Machine Learning
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/llm/">
            <i class="fas fa-tag"></i>
            LLM
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/cnn/">
            <i class="fas fa-tag"></i>
            CNN
          </a>&nbsp;
        </li>
      </ul>
      
<aside class="toc">
  <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#problem-statement">Problem Statement</a></li>
<li><a href="#making-assumptions">Making Assumptions</a>
<ul>
<li><a href="#strategy-1-raw-pixels">Strategy 1: Raw Pixels</a></li>
<li><a href="#strategy-2-cnn">Strategy 2: CNN</a></li>
</ul></li>
<li><a href="#semantics">Semantics</a></li>
<li><a href="#ocr">OCR</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#postscript-alpha-channel-shenanigans">Postscript: Alpha Channel Shenanigans</a></li>
</ul></li>
</ul>
</nav>
</aside>
      

<p><strong>DRAFT DO NOT PUBLISH</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>TODO: punchy intro</p>

<p>GPT-4o charges 170 tokens to process each $512 \times 512$ tile used in high-res mode.
(Ignoring the 85 token for the low-res &ldquo;master thumbnail&rdquo;.)
At 0.75 tokens/word, this suggests a picture is worth about 227 words - only
a factor of four off from the traditional saying.</p>

<p>OK, but why 170? It&rsquo;s an oddly specific number. It&rsquo;s not power of 2 or a
multiple of 5 the way most &ldquo;round&rdquo; numbers are. Numbers that are just dropped
into the codebase without explanation are called &ldquo;<a href="https://en.wikipedia.org/wiki/Magic_number_(programming)">magic numbers</a>&rdquo; in
programming, and 170 is a pretty glaring magic number.</p>

<p>And why are image costs even converted to tokens anyway? If it were just for
billing purposes, wouldn&rsquo;t it be less confusing to simply list the cost per tile?</p>

<p>What if OpenAI chose 170, not as part of some arcane pricing strategy, but
simply because it&rsquo;s literally true? What if image tiles are represented
internally as 170 consecutive tokens? And if so, how?</p>

<h2 id="making-assumptions">Making Assumptions</h2>

<p>OpenAI seems to likes powers of 2, sometimes with a factor of 3 mixed in.
For example, 1,536 (ada-002) or 3,072 (text-embedding-3-large)</p>

<p>Alternatively, image tiles are square, so are likely represented by a square grid of tokens.
Following the &ldquo;powers of 2s and 3s&rdquo; logic, 170 might be <sup>512</sup>&frasl;<sub>3</sub>.
Following the square logic, 170 is very close to $13 \times 13$.</p>

<p>TODO: recall that &ldquo;tokens&rdquo; are represented internally in transformer models
as vector embeddings.</p>

<p>It&rsquo;s likely that a the internal vector dimension used to represent tokens
inside of GPT-4o is one of these:</p>

<div style="width: 50%; margin: auto">
    <table>
        <thead>
            <tr>
                <th align="center">Dimension</th>
                <th align="center">Prime Factors</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td align="center">$1{,}536$</td>
                <td align="center">$3 \cdot 2^9$</td>
            </tr>
            <tr>
                <td align="center">$2{,}024$</td>
                <td align="center">$2^{11}$</td>
            </tr>
            <tr>
                <td align="center">$3{,}072$</td>
                <td align="center">$3 \cdot 2^{10}$</td>
            </tr>
            <tr>
                <td align="center">$4{,}048$</td>
                <td align="center">$2^{12}$</td>
            </tr>
        </tbody>
    </table>
</div>

<p>Let&rsquo;s say it&rsquo;s 3,072 for the sake of argument.
How do we go from $ 512 \times 512 \times 3 $ to $170 \times 3{,}072$?</p>

<h3 id="strategy-1-raw-pixels">Strategy 1: Raw Pixels</h3>

<p>Here&rsquo;s an extremely simple way to stuff an image into the embedding space:</p>

<ol>
<li>Divide the $512 \times 512$ into a $16 \times 16$ grid of &ldquo;mini-tiles.&rdquo;</li>
<li>Each mini-tiles is $32 \times 32 \times 3$, for a total size of $3{,}072$.</li>
<li>Each mini-tile can be embedded a a single token.</li>
<li>Represent the image as 256 consecutive tokens.</li>
</ol>

<p>There are two main problems with this approach:</p>

<ol>
<li>256 is larger than 170, and</li>
<li>it&rsquo;s extremely stupid.</li>
</ol>

<p>By &ldquo;extremely stupid&rdquo; I mean that it doesn&rsquo;t make much sense to embed using raw
RGB values and hope the transformer can handle it. The transformer architecture
is optimized for text, not images, and this brute force way of stuffing data in
gives us no way to map into the same &ldquo;semantic space&rdquo; used by token embeddings.</p>

<h3 id="strategy-2-cnn">Strategy 2: CNN</h3>

<p>TODO Discuss AlexNet and YOLOv3</p>

<p><img src="/post/gpt-cnn_files/layers_alexnet.png">
Note: AlexNet was designed to work on $227 \times 227$ pixels. I&rsquo;ve adjusted it to match the $512 \times 512$ input.</p>

<p><img src="/post/gpt-cnn_files/layers_yolo_v3.png">
Note: YOLOv3 was designed to work on $416 \times 416$ pixels. I&rsquo;ve adjusted it to match the $512 \times 512$ input.</p>

<p>TODO: explain why YOLO ends in a grid instead of a single output, bounding boxes, etc.</p>

<p>Suggest 3x3 and 5x5 versions</p>

<p><img src="/post/gpt-cnn_files/layers_3x3_speculative.png"></p>

<p>If you use $5 \times 5$ convolution layers alternating with $2 \times 2$
max pool layers, all the math works beautifully without any special cases:</p>

<p><img src="/post/gpt-cnn_files/layers_5x5_speculative.png"></p>

<p>The $5 \times 5$ version seems to fit extremely neatly, reaching our target
size of $13 \times 13$ and target embedding dimension of 3,072 without any
special cases. At 16-bit precision, this uses a maximum of about 12 GB of VRAM
so can be run quickly on an inexpensive, commodity GPU.</p>

<p>However, $5 \times 5$ convolutions aren&rsquo;t very common in best-practice CNN
Architectures, so maybe it&rsquo;s using two back-to-back $3 \times 3$ convolutions,
similar to the VGG16 architecture:</p>

<!-- <img src="/post/gpt-cnn_files/layers_3x3_vgg_speculative.png"> -->

<p><img src="/post/gpt-cnn_files/layers_3x3_vgg_short_speculative.png"></p>

<p>This gets us roughly the same effect, but at the cost of more layers.</p>

<p>Once we&rsquo;ve transformed the image tensor from $512 \times 512 \times 3$
to $13 \times 13 \times 3{,}072$, we can represent them to the transfomer
as 169 consecutive embedding vectors.</p>

<p>Add one token for flag the start of an image block:</p>

<pre><code>&lt;| Image Tile Start |&gt;
3,072 channels from (0,0)
3,072 channels from (1,0)
...skipping 166 similar embedding vectors...
3,072 channels from (13,13)
</code></pre>

<p>(Alternatively, perhaps its $12 \times 12$ with special tokens for start, end, and &ldquo;row&rdquo; delimiter.)
Note that RoPE (Rotary Position Embedding) should help GPT-4o understand the approximate positions of tokens within the image, but not perfectly.</p>

<h2 id="semantics">Semantics</h2>

<p>TODO</p>

<p>Problem: Aligning image semantics with token semantics</p>

<p>Possible Solution: Train the CNN to predict the embedding vector for the tagged token
For example, if there&rsquo;s a cat in the upper left, train it to predict the 3,072-dimension embedding vector for the token &ldquo;cat&rdquo;.</p>

<p>Possible Solution 2: End-to-End training</p>

<h2 id="ocr">OCR</h2>

<p>This does <em>NOT</em> explain how it does OCR.
YOLO can&rsquo;t really do OCR, especially not the high-quality OCR GPT-4o exhibits.
I have another theory for that: I think they&rsquo;re running tesseract (or their own in-house OCR) and feeding the identified text in alongside the image data.
I think that is why the early versions could be easily confused by text hidden in images; from its POV, that text <em>was</em> part of the prompt.
 (This is fixed now; GPT-4o is good at ignoring malicious embedded prompts.)
Malicious prompt rejection example.
However, this does not explain why there&rsquo;s no charge per token for the text found in an image.</p>

<p>Interestingly enough, this seems to suggest its actually <em>more efficient</em> to
send text as images:A $512 \times 512$ image with a small but readable font
can easily fit 400-500 tokens worth of text, yet you&rsquo;re only charged for 170
input tokens plus the 85 for the master thumbnail for a grand total of 255
tokens - far less than the number of words on the image.</p>

<p>This theory explains why there is additional latency when processing images.
The CNN would be essentially instaneous, but OCR takes a bit of time.BTW,
and I&rsquo;m not saying this proves anything, but the Python environment used by the
OpenAI code interpreter has pytesseract installed. You can literally just ask
it to run pytesseract on any image you&rsquo;ve uploaded if you want to get a second
opinion.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Well, we&rsquo;ve made a lot of speculitive hay out of what is essentially only one
morsel of hard fact: that OpenAI used the magic number 170.</p>

<p>However, there does seem to be a complete plausible approach, very much in line
with other best practice CNN architectures such as YOLO, for mapping from
image tiles to embedding vectors.</p>

<p>So, I don&rsquo;t think 170 tokens is just an approximation used to bill for roughly
the amount of compute it takes to process an image. And I don&rsquo;t think they&rsquo;re
concatinating layers to join image and text data the way some other multi-modal
models do.</p>

<p>No, I think GPT-4o is <em>literally</em> representing $512 \times 512$ images as 170
tokens, using an CNN architecture very similar to YOLO to embed the image
directly into the transformers semantic embedding space.</p>

<p>The architectures that best &ldquo;fits&rdquo; is 5 layers of $5 \times 5$ convolutions alternating with 5 layers of $2 \times 2$ max pooling,
or a similar architecture with two back-to-back $3 \times 3$ convolution layers.
It&rsquo;s impossible to be that precise, but I think it&rsquo;s roughly in that ballpark.
(Not least because <em>most</em> state-of-the-art image classifiers are roughly in the same ballpark.)</p>

<p>This explains how its able to handle multiple images, and tasks like comparing two images, for example.
It explains how its able to see multiple objects in the same image, but gets overwhelmed when there are too many objects in a busy scene.
It also explains why GPT-4o seems extremely vague and the absolute and relative positions of separate objects within the scene; RoPE
can only do so much.</p>

<!-- https://unsplash.com/photos/black-and-gray-camera-on-white-table-Y5dd6hLkn-8 -->

<h2 id="postscript-alpha-channel-shenanigans">Postscript: Alpha Channel Shenanigans</h2>

<p>One curious thing I noticed while working on this project is that GPT-4o
<em>ignores</em> the alpha channel, resulting in occasionally suprising behavior.</p>

<p>We can illustate this with four carefully prepared images. For convenience,
These images are displayed on top of a checkboard pattern - the images themselves
have flat, transparent backgrounds.</p>

<p>What do I mean by &ldquo;transparent black&rdquo; or &ldquo;transparent white?&rdquo; Well, when
we represent an RGBA color with four bytes, the RGB byes are still there
even when alpha is 100%. <code>(0, 0, 0, 255)</code> and <code>(255, 255, 255, 255)</code> are
in some sense different colors, even though there&rsquo;s no situation where a
correct renderer would display them differently.</p>

<p><style>
    .grid-container {
        display: grid;
        grid-template-columns: auto auto;
        gap: 10px;
        text-align: center;
    }
    .grid-item {
        padding: 10px;
    }
    .image-container {
        display: flex;
        flex-direction: column;
        align-items: center;
    }
    .image-container img {
        margin: 0px;
        padding: 0px;
        max-width: 100%;
        height: auto;
        background: repeating-conic-gradient(#888888 0% 25%, #cccccc 0% 50%) 0 / 20px 20px;
    }
</style>
<div class="grid-container">
    <div class="grid-item">
        <div class="image-container">
            <img src="/post/gpt-cnn_files/black_on_transparent_black.png" alt="MODICUM">
            <div>Black Text on Transparent Black Background</div>
            <div>GPT-4o Reads: &ldquo;&rdquo;</div>
        </div>
    </div>
    <div class="grid-item">
        <div class="image-container">
            <img src="/post/gpt-cnn_files/black_on_transparent_white.png" alt="ENORMOUS">
            <div>Black Text on Transparent White Background</div>
            <div>GPT-4o Reads: &ldquo;ENORMOUS&rdquo;</div>
        </div>
    </div>
    <div class="grid-item">
        <div class="image-container">
            <img src="/post/gpt-cnn_files/white_on_transparent_black.png" alt="SCINTILLA">
            <div>White Text on Transparent Black Background</div>
            <div>GPT-4o Reads: &ldquo;SCINTILLA&rdquo;</div>
        </div>
    </div>
    <div class="grid-item">
        <div class="image-container">
            <img src="/post/gpt-cnn_files/white_on_transparent_white.png" alt="GIGANTIC">
            <div>White Text on Transparent White Background</div>
            <div>GPT-4o Reads: &ldquo;&rdquo;</div>
        </div>
    </div>
</div></p>

<p>What&rsquo;s going on here? The pattern that emerges is that GPT-4o can read the
text if and only if the text color is different than the &ldquo;color&rdquo; of the
transparent background.</p>

<p>This tells us that GPT-4o <em>disregards</em> the alpha channel and only looks at the
RGB channels. To it, transparent black is black, transparent white is white.</p>

<p>We can see this even more clearly if we mess with an image to preserve the
three RGB channels while setting the alpha channel to 100%. These two images
have identical RGB data, and only differ in the alpha channel:</p>

<div class="grid-container">
    <div class="grid-item">
        <div class="image-container">
            <img src="/post/gpt-cnn_files/platypus.png" alt="Visible Platypus">
            <div>Alpha Channel = 255</div>
        </div>
    </div>
    <div class="grid-item">
        <div class="image-container">
            <img src="/post/gpt-cnn_files/platypus_hidden.png" alt="Hidden Platypus">
            <div>Alpha Channel = 0</div>
        </div>
    </div>
</div>

<p>GPT-4o has no trouble seeing the hidden platypus:</p>

<p><img src="/post/gpt-cnn_files/chatgpt_hidden_platypus_test.png" alt="ChatGPT passes the hidden platypus test."></p>

<p>If you don&rsquo;t believe me, download the
<a href="/post/gpt-cnn_files/platypus_hidden.png" download><code>hidden_platypus.png</code></a>
image and paste it into ChatGPT yourself; it will correctly describe it. (DON&rsquo;T
right-click copy-and-paste - the transparency will be lost as it passes
through the clipboard.) You may also note the image is 39.3 KB, the same
size as
<a href="/post/gpt-cnn_files/platypus.png" download><code>platypus.png</code></a>
even though PNG compression should have made it much smaller if it was really
a perfectly blank, transparent image. <em>You</em> can&rsquo;t see it, because your browser correctly respects the alpha channel,
but GPT-4o can, because it ignores it completely.</p>

<p>I&rsquo;m not sure if this is bug but its certainly suprising behavior; In fact,
it feels like something a malicous user could use to smuggle information past
humans and directly to GPT-4o. It&rsquo;s true that GPT-4o is <em>much</em> better at
detecting and ignoring malicious prompts hidden in images:</p>

<p><img src="/post/gpt-cnn_files/gpt4o_malicious_test1.png"></p>

<p>You can see several other examples of GPT-4o successfully detecting and
ignoring malicious prompts hidden in images in my <a href="https://olooney.github.io/image_tagger/gallery/index.html">gallery of GPT-4o test
images</a>.</p>

<p>So even if it is a bug, it&rsquo;s not obvious it can be exploited. Still, it would
be less suprising in GPT-4o &ldquo;saw&rdquo; the same thing that a human would in a
browser.</p>

    </article>

    <hr>


    <ul class="pager article-pager">
      <li class="pager-newer pager-noitem">&lt; Newer</li>
      <li class="pager-older">
        <a href="/post/jeopardy/" data-toggle="tooltip" data-placement="top" title="Let&#39;s Play Jeopardy! with LLMs">Older &gt;</a>
      </li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2024 Oran Looney
  </div>
  <ul class="site-footer-items">
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
