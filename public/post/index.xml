<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on OranLooney.com</title>
    <link>https://www.oranlooney.com/post/</link>
    <description>Recent content in Posts on OranLooney.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; Copyright {year} Oran Looney</copyright>
    <lastBuildDate>Sat, 24 Jan 2026 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.oranlooney.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Grifters, Skeptics, and Marks</title>
      <link>https://www.oranlooney.com/post/grifters-skeptics-marks/</link>
      <pubDate>Sat, 24 Jan 2026 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/grifters-skeptics-marks/</guid>
      <description>We are in a golden age of grift. Where adventurers once flocked to California or the Yukon because &amp;ldquo;there was gold in them thar hills,&amp;rdquo; the best way to get rich today is to fleece suckers. We&amp;rsquo;ve got crypto rug pulls, meme stocks, and a seemingly endless stream of financial products engineered for nothing more than plausible deniability. Things have gotten so bad that financial professionals frequently joke, &amp;ldquo;crime is legal now.</description>
    </item>
    
    <item>
      <title>A Modest Definition of Human Consciousness</title>
      <link>https://www.oranlooney.com/post/em-dash/</link>
      <pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/em-dash/</guid>
      <description>I bring you news of the single most important intellectual discovery of our generation: the hard problem of human consciousness has now been solved once and for all.
For a long time, the ability to select squares containing traffic lights was our best working definition of what it meant to be truly, deeply, authentically human, but this was never quite satisfactory.
Now, in 2025, research philosophers have expanded that definition to include the crucial missing ingredient: an inability to use an em dash.</description>
    </item>
    
    <item>
      <title>The Prehistory of Computing, Part II</title>
      <link>https://www.oranlooney.com/post/history-of-computing-2/</link>
      <pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/history-of-computing-2/</guid>
      <description>In part I of this two-part series we covered lookup tables and simple devices with at most a handful of moving parts. This time we&amp;rsquo;ll pick up in the 17th centuries, when computing devices started to became far more complex and the groundwork for later theoretical work began to be laid.
Pascal We enter the era of mechanical calculators in 1642 when Pascal invented a machine, charmingly called the pascaline, which could perform addition and subtraction:</description>
    </item>
    
    <item>
      <title>The Prehistory of Computing, Part I</title>
      <link>https://www.oranlooney.com/post/history-of-computing/</link>
      <pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/history-of-computing/</guid>
      <description>What is a computer, really? Where did it come from? When did we realize we could trick rocks into doing our math homework for us?
In this two-part series, I&amp;rsquo;ll cover the origin and early history of computing and computer science, starting in prehistoric Africa and ending in Victorian-era England. Not exhaustively (because that would require an entire book) but selectively, highlighting the most interesting innovations and focusing on the untold (or at least less well known) stories.</description>
    </item>
    
    <item>
      <title>The Art and Mathematics of Genji-Kō</title>
      <link>https://www.oranlooney.com/post/genji-ko/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/genji-ko/</guid>
      <description>You might think it&amp;rsquo;s unlikely for any interesting mathematics to arise from incense appreciation, but that&amp;rsquo;s only because you&amp;rsquo;re unfamiliar with the peculiar character of Muromachi (室町) era Japanese nobles.
There has never been a group of people, in any time or place, who were so driven to display their sophistication and refinement. It wouldn&amp;rsquo;t do to merely put out a few sticks of incense; no, you would have to prove that your taste was more exquisite, your judgment more refined, your etiquette more oblique.</description>
    </item>
    
    <item>
      <title>A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images?</title>
      <link>https://www.oranlooney.com/post/gpt-cnn/</link>
      <pubDate>Wed, 05 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/gpt-cnn/</guid>
      <description>Here&amp;rsquo;s a fact: GPT-4o charges 170 tokens to process each 512x512 tile used in high-res mode. At ~0.75 tokens/word, this suggests a picture is worth about 227 words&amp;mdash;only a factor of four off from the traditional saying.
(There&amp;rsquo;s also an 85 tokens charge for a low-res &amp;lsquo;master thumbnail&amp;rsquo; of each picture and higher resolution images are broken into many such 512x512 tiles, but let&amp;rsquo;s just focus on a single high-res tile.</description>
    </item>
    
    <item>
      <title>Let&#39;s Play Jeopardy! with LLMs</title>
      <link>https://www.oranlooney.com/post/jeopardy/</link>
      <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/jeopardy/</guid>
      <description>How good are LLMs at trivia? I used the Jeopardy! dataset from Kaggle to benchmark ChatGPT and the new Llama 3 models. Here are the results:
There you go. You&amp;rsquo;ve already gotten 90% of what you&amp;rsquo;re going to get out of this article. Some guy on the internet ran a half-baked benchmark on a handful of LLM models, and the results were largely in line with popular benchmarks and received wisdom on fine-tuning and RAG.</description>
    </item>
    
    <item>
      <title>Stacking Triangles for Fun and Profit</title>
      <link>https://www.oranlooney.com/post/angle-addition/</link>
      <pubDate>Mon, 08 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/angle-addition/</guid>
      <description>One thing you may have noticed about the trigonometric functions sine and cosine is that they seem to have no agreed upon definition. Or rather, different authors choose different definitions as the starting point, mainly based on convenience. This isn&amp;rsquo;t problematic or even particularly unusual in mathematics - as long as we can derive any of the other forms from any starting point, it makes little theoretical difference which we start from since they&amp;rsquo;re all equivalent anyway.</description>
    </item>
    
    <item>
      <title>Kaprekar&#39;s Magic 6174</title>
      <link>https://www.oranlooney.com/post/kaprekar/</link>
      <pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/kaprekar/</guid>
      <description>Kaprekar&amp;rsquo;s routine is a simple arithmetic procedure which, when applied to four digit numbers, rapidly converges to the fixed point 6174, known as the Kaprekar constant. Unlike other famous iterative procedures such as the Collatz function, the somewhat arbitrary nature of the Kaprekar routine doesn&amp;rsquo;t hint at fundamental mathematical discoveries yet to be made; rather, its charm lies in its intuitive definition (requiring no more than elementary mathematics,) its oddly off-center fixed point of 6174, and its surprisingly rapid convergence (which requires only five iterations on average and never more than seven.</description>
    </item>
    
    <item>
      <title>Cracking Playfair Ciphers</title>
      <link>https://www.oranlooney.com/post/playfair/</link>
      <pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/playfair/</guid>
      <description>In 2020, the Zodiac 340 cipher was finally cracked after more than 50 years of trying by amateur code breakers. While the effort to crack it was extremely impressive, the cipher itself was ultimately disappointing. A homophonic substitution cipher with a minor gimmick of writing diagonally, the main factor that prevented it from being solved much earlier was the several errors the Zodiac killer made when encoding it.
Substitution ciphers, which operate at the level of a single character, are children&amp;rsquo;s toys, the kind of thing you might get a decoder ring for from the back of a magazine.</description>
    </item>
    
    <item>
      <title>My Dinner with ChatGPT</title>
      <link>https://www.oranlooney.com/post/my-dinner-with-chatgpt/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/my-dinner-with-chatgpt/</guid>
      <description>It&#39;s hard to talk about ChatGPT without cherry-picking. It&#39;s too easy to try a dozen different prompts, refresh each a handful of times, and report the most interesting or impressive thing from those sixty trials. While this problem plagues a lot of the public discourse around generative models, cherry-picking is particularly problematic for ChatGPT because it&#39;s actively using the chat history as context. (It might be using a $\mathcal{O}(n \log{} n)$ attention model like reformer or it might just be brute forcing it, but either it has an impressively long memory; about 2048 &#34;</description>
    </item>
    
    <item>
      <title>A History of Encabulation</title>
      <link>https://www.oranlooney.com/post/encabulation/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/encabulation/</guid>
      <description>To celebrate the 100th anniversary of the birth of encabulation - dated from Dr. Wolfgang Albrecht Klossner&amp;rsquo;s first successful run in that historic barn on the outskirts of Eisenhüttenstadt - this article* collects in one place a number of resources that provide, if not a comprehensive history, at least a catalogue of the major milestones and concepts.
The Original Turbo Encabulator For a number of years now, work has been proceeding in order to bring perfection to the crudely conceived idea of a transmission that would not only supply inverse reactive current for use in unilateral phase detractors, but would also be capable of automatically synchronizing cardinal grammeters.</description>
    </item>
    
    <item>
      <title>Eight Billion People</title>
      <link>https://www.oranlooney.com/post/eight-billion/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/eight-billion/</guid>
      <description>Today is the last day when the number of people alive will start with a seven. Sometime late Tuesday afternoon, or perhaps early Wednesday morning, the population will cross the eight billion mark. When I was a kid, the number they taught us in school was five billion. By the time I was in college, it was up to six, and a decade ago it hit seven.
Now it&amp;rsquo;s at eight.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 6: Principal Component Analysis</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-6-pca/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-6-pca/</guid>
      <description>In the previous article in this series we distinguished between two kinds of unsupervised learning (cluster analysis and dimensionality reduction) and discussed the former in some detail. In this installment we turn our attention to the later.
In dimensionality reduction we seek a function \(f : \mathbb{R}^n \mapsto \mathbb{R}^m\) where \(n\) is the dimension of the original data \(\mathbf{X}\) and \(m\) is less than or equal to \(n\). That is, we want to map some high dimensional space into some lower dimensional space.</description>
    </item>
    
    <item>
      <title>A Seriously Slow Fibonacci Function</title>
      <link>https://www.oranlooney.com/post/slow-fibonacci/</link>
      <pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/slow-fibonacci/</guid>
      <description>I recently wrote an article which was ostensibly about the Fibonacci series but was really about optimization techniques. I wanted to follow up on its (extremely moderate) success by going in the exact opposite direction: by writing a Fibonacci function which is as slow as possible.
This is not as easy as it sounds: any program can trivially be made slower, but this is boring. How can we make it slow in a fair and interesting way?</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 5: Gaussian Mixture Models</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-5-gmm/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-5-gmm/</guid>
      <description>Consider the following motivating dataset:
Unlabled Data
 It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. The question is: is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?</description>
    </item>
    
    <item>
      <title>Adaptive Basis Functions</title>
      <link>https://www.oranlooney.com/post/adaptive-basis-functions/</link>
      <pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/adaptive-basis-functions/</guid>
      <description>Today, let me be vague. No statistics, no algorithms, no proofs. Instead, we’re going to go through a series of examples and eyeball a suggestive series of charts, which will imply a certain conclusion, without actually proving anything; but which will, I hope, provide useful intuition.
The premise is this:
 For any given problem, there exists learned featured representations which are better than any fixed/human-engineered set of features, even once the cost of the added parameters necessary to also learn the new features into account.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 4: Decision Trees</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-4-decision-tree/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-4-decision-tree/</guid>
      <description>So far in this series we’ve followed one particular thread: linear regression -&amp;gt; logistic regression -&amp;gt; neural network. This is a very natural progression of ideas, but it really represents only one possible approach. Today we’ll switch gears and look at a model with completely different pedigree: the decision tree, sometimes also referred to as Classification and Regression Trees, or simply CART models. In contrast to the earlier progression, decision trees are designed from the start to represent non-linear features and interactions.</description>
    </item>
    
    <item>
      <title>A Fairly Fast Fibonacci Function</title>
      <link>https://www.oranlooney.com/post/fibonacci/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/fibonacci/</guid>
      <description>A common example of recursion is the function to calculate the \(n\)-th Fibonacci number:
def naive_fib(n): if n &amp;lt; 2: return n else: return naive_fib(n-1) + naive_fib(n-2) This follows the mathematical definition very closely but it’s performance is terrible: roughly \(\mathcal{O}(2^n)\). This is commonly patched up with dynamic programming. Specifically, either the memoization:
from functools import lru_cache @lru_cache(100) def memoized_fib(n): if n &amp;lt; 2: return n else: return memoized_fib(n-1) + memoized_fib(n-2) or tabulation:</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 3: Backpropagation</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-3-backpropagation/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-3-backpropagation/</guid>
      <description>In today’s installment of Machine Learning From Scratch we’ll build on the logistic regression from last time to create a classifier which is able to automatically represent non-linear relationships and interactions between features: the neural network. In particular I want to focus on one central algorithm which allows us to apply gradient descent to deep neural networks: the backpropagation algorithm. The history of this algorithm appears to be somewhat complex (as you can hear from Yann LeCun himself in this 2018 interview) but luckily for us the algorithm in its modern form is not difficult - although it does require a solid handle on linear algebra and calculus.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 2: Logistic Regression</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-2-logistic-regression/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-2-logistic-regression/</guid>
      <description>In this second installment of the machine learning from scratch we switch the point of view from regression to classification: instead of estimating a number, we will be trying to guess which of 2 possible classes a given input belongs to. A modern example is looking at a photo and deciding if its a cat or a dog.
In practice, its extremely common to need to decide between \(k\) classes where \(k &amp;gt; 2\) but in this article we’ll limit ourselves to just two classes - the so-called binary classification problem - because generalizations to many classes are usually both tedious and straight-forward.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 1: Linear Regression</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/</guid>
      <description>To kick off this series, will start with something simple yet foundational: linear regression via ordinary least squares.
While not exciting, linear regression finds widespread use both as a standalone learning algorithm and as a building block in more advanced learning algorithms. The output layer of a deep neural network trained for regression with MSE loss, simple AR time series models, and the “local regression” part of LOWESS smoothing are all examples of linear regression being used as an ingredient in a more sophisticated model.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 0: Introduction</title>
      <link>https://www.oranlooney.com/post/ml-from-scratch-part-0-introduction/</link>
      <pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/ml-from-scratch-part-0-introduction/</guid>
      <description>Motivation  “As an apprentice, every new magician must prove to his own satisfaction, at least once, that there is truly great power in magic.” - The Flying Sorcerers, by David Gerrold and Larry Niven
 How do you know if you really understand something? You could just rely on the subjective experience of feeling like you understand. This sounds plausible - surely you of all people should know, right? But this runs head-first into in the Dunning-Kruger effect.</description>
    </item>
    
    <item>
      <title>Visualizing Multiclass Classification Results</title>
      <link>https://www.oranlooney.com/post/viz-tsne/</link>
      <pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/viz-tsne/</guid>
      <description>Introduction Visualizing the results of a binary classifier is already a challenge, but having more than two classes aggravates the matter considerably.
Let&amp;rsquo;s say we have $k$ classes. Then for each observation, there is one correct prediction and $k-1$ possible incorrect prediction. Instead of a $2 \times 2$ confusion matrix, we have a $k^2$ possibilities. Instead of having two kinds of error, false positives and false negatives, we have $k(k-1)$ kinds of errors.</description>
    </item>
    
    <item>
      <title>Craps Variants</title>
      <link>https://www.oranlooney.com/post/craps-game-variants/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/craps-game-variants/</guid>
      <description>Craps is a suprisingly fair game. I remember calculating the probability of winning craps for the first time in an undergraduate discrete math class: I went back through my calculations several times, certain there was a mistake somewhere. How could it be closer than $\frac{1}{36}$?
(Spoiler Warning If you haven&amp;rsquo;t calculated these odds for yourself then you may want to do so before reading further. I&amp;rsquo;m about to spoil it for you rather thoroughly in the name of exploring a more general case.</description>
    </item>
    
    <item>
      <title>Complex Numbers in R, Part II</title>
      <link>https://www.oranlooney.com/post/complex-r-part-2/</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/complex-r-part-2/</guid>
      <description>This post is part of a series on complex number functionality in the R programming language. You may want to read Part I before continuing if you are not already comfortable with the basics.
In Part I of this series, we dipped our toes in the water by explicitly creating some complex numbers and showing how they worked with the most basic mathematical operators, functions, and plots.
In this second part, we’ll take a more in-depth look at some scenarios where complex numbers arise naturally – where they are less of a choice an more of a necessity.</description>
    </item>
    
    <item>
      <title>Complex Numbers in R, Part I</title>
      <link>https://www.oranlooney.com/post/complex-r/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/complex-r/</guid>
      <description>R, like many scientific programming languages, has first-class support for complex numbers. And, just as in most other programming languages, this functionality is ignored by the vast majority of users.
Yet complex numbers can often offer surprisingly elegant formulations and solutions to problems. I want to convince you that familiarizing yourself with R’s excellent complex number functionality is well worth the effort and will pay off in two different ways: first by showing you how they are so amazingly useful you’ll want to go out of your way to use them, and then by showing you how they are so common and fundamental to modern analysis that you couldn’t avoid them if you wanted to.</description>
    </item>
    
    <item>
      <title>So, Apparently I&#39;m an iPad Developer Now</title>
      <link>https://www.oranlooney.com/post/apparently-ipad-developer/</link>
      <pubDate>Wed, 28 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/apparently-ipad-developer/</guid>
      <description>Last week my boss stopped by and dropped a brand spanking new iPad on my desk. &amp;quot;Make our application work on this,&amp;quot; he commanded. &amp;quot;You have two days before we demo it at the trade show.&amp;quot; Madness? No, these are web apps! You see, for the last couple years we&#39;ve been working exclusively on AJAX applications: web pages stuffed with so much JavaScript they look and feel like desktop apps. It&#39;s harder than writing desktop software, but if you pull it off you get an application that can be run anywhere, instantly.</description>
    </item>
    
    <item>
      <title>Deep Copy in JavaScript</title>
      <link>https://www.oranlooney.com/post/deep-copy-javascript/</link>
      <pubDate>Wed, 25 Nov 2009 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/deep-copy-javascript/</guid>
      <description>Update 2017-10-23: This article and code library have not kept up with the rapidly changing JavaScript landscape and are now hopelessly out of date. First came non-enumerable properties, and with ES2015 came the introduction of classes, proxies, symbols, and anonymous functions, all of which break the below logic. I&#39;m afraid I no longer know how to fully copy the full menagerie of JavaScript objects while preserving relative references, and it&#39;s quite possible that no one else knows either.</description>
    </item>
    
    <item>
      <title>Semantic Code</title>
      <link>https://www.oranlooney.com/post/semantic-code/</link>
      <pubDate>Wed, 30 Apr 2008 00:00:00 +0000</pubDate>
      
      <guid>https://www.oranlooney.com/post/semantic-code/</guid>
      <description>se-man-tic (si-man&amp;rsquo;tik) adj. &amp;nbsp; &amp;nbsp; 1. Of or relating to meaning, especially meaning in language.
 Programming destroys meaning. When we program, we first replace concepts with symbols and then replace those symbols with arbitrary codes &amp;mdash; that&amp;rsquo;s why it&amp;rsquo;s called coding.
At its worst programming is write-only: the program accomplishes a task, but is incomprehensible to humans. See, for example, the story of Mel. Such a program is correct, yet at the same time meaningless.</description>
    </item>
    
  </channel>
</rss>