<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML From Scratch, Part 6: Principal Component Analysis - OranLooney.com</title>
  <meta property="og:title" content="ML From Scratch, Part 6: Principal Component Analysis - OranLooney.com" />
  <meta name="twitter:title" content="ML From Scratch, Part 6: Principal Component Analysis - OranLooney.com" />
  <meta name="description" content="In the previous article in this series we distinguishedbetween two kinds of unsupervised learning (cluster analysis and dimensionalityreduction) and discussed the former in some detail. In this installment we turnour attention to the later.
In dimensionality reduction we seek a function \(f : \mathbb{R}^n \mapsto \mathbb{R}^m\) where \(n\) is the dimension of the original data \(\mathbf{X}\) and\(m\) is less than or equal to \(n\). That is, we want to map some high dimensionalspace into some lower dimensional space.">
  <meta property="og:description" content="In the previous article in this series we distinguishedbetween two kinds of unsupervised learning (cluster analysis and dimensionalityreduction) and discussed the former in some detail. In this installment we turnour attention to the later.
In dimensionality reduction we seek a function \(f : \mathbb{R}^n \mapsto \mathbb{R}^m\) where \(n\) is the dimension of the original data \(\mathbf{X}\) and\(m\) is less than or equal to \(n\). That is, we want to map some high dimensionalspace into some lower dimensional space.">
  <meta name="twitter:description" content="In the previous article in this series we distinguishedbetween two kinds of unsupervised learning (cluster analysis and dimensionalityreduction) and discussed the former in some detail. In this …">
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>
 
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/search/" title="Site Search"><i class="fa fa-search"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//honeycode.tumblr.com/" target="_blank" title="Honeycode Microblog">
      <svg class="svg-inline--fa fa-w-12" aria-hidden="true" data-prefix="fab" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="20 50 350 450" data-fa-i2svg="">
<path fill="currentColor" d="M193 97l86.6 50v100l-86.6 50l-86.6 -50v-100zM285 255l86.6 50v100l-86.6 50l-86.6 -50v-100zM100 255l86.6 50v100l-86.6 50l-86.6 -50v-100z"></path>
</svg>
    </a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>

    <li>
      <a href="https://www.librarything.com/catalog.php?view=olooney&amp;offset=0&amp;shelf_rows=10&amp;previousOffset=0&amp;shelf=shelf" target="_blank" title="LibraryThing">
	    <i class="fas fa-book-reader"></i>
	  </a>
	</li>
    
  </ul>

  </nav>
</header>


  <div class="main" role="main">
    <article class="article">
      <img src="/post/ml-from-scratch-part-6-pca_files/lead.jpg" class="article-image" />
      
      <h1 class="article-title">ML From Scratch, Part 6: Principal Component Analysis</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-author">
            by <a href="/about/">Oran Looney</a>
        </li>
        <li class="article-meta-date"><time>September 16, 2019</time></li>
        <li class="article-meta-tags">
          <a href="/tags/python/">
            <i class="fas fa-tag"></i>
            Python
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/statistics/">
            <i class="fas fa-tag"></i>
            Statistics
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/from-scratch/">
            <i class="fas fa-tag"></i>
            From Scratch
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/machine-learning/">
            <i class="fas fa-tag"></i>
            Machine Learning
          </a>&nbsp;
        </li>
      </ul>
      
<aside class="toc">
  
</aside>
      <p>In the <a href="/post/ml-from-scratch-part-5-gmm/">previous article</a> in <a href="/tags/from-scratch/">this series</a> we distinguished
between two kinds of unsupervised learning (cluster analysis and dimensionality
reduction) and discussed the former in some detail. In this installment we turn
our attention to the later.</p>
<p>In dimensionality reduction we seek a function <span class="math inline">\(f : \mathbb{R}^n \mapsto \mathbb{R}^m\)</span> where <span class="math inline">\(n\)</span> is the dimension of the original data <span class="math inline">\(\mathbf{X}\)</span> and
<span class="math inline">\(m\)</span> is less than or equal to <span class="math inline">\(n\)</span>. That is, we want to map some high dimensional
space into some lower dimensional space. (Contrast this with the map into a
finite set sought by cluster analysis.)</p>
<p>We will focus on one technique in particular: <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Primary Component
Analysis</a>, usually abbreviated PCA. We’ll derive PCA from first
principles, implement a working version (writing all the linear algebra code
from scratch), show an example of how PCA helps us visualize and gain insight
into a high dimensional data set, and end with a discussion a few more-or-less
principled ways to choose how many dimensions to keep.</p>
<div id="what-is-pca" class="section level2">
<h2>What is PCA?</h2>
<p>PCA is a <em>linear</em> dimensionality reduction technique. <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">Many</a> non-linear
<a href="https://en.wikipedia.org/wiki/Self-organizing_map">dimensionality</a> <a href="https://en.wikipedia.org/wiki/Autoencoder">reduction</a> <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">techniques</a> exist, but linear
methods are more mature, if more limited.</p>
<p>Linearity does not suffice to fully specify the problem, however. <a href="https://en.wikipedia.org/wiki/Factor_analysis">Factor
Analysis</a> also seeks a linear map, but takes a more statistical approach
and reaches a slightly different solution in practice. <a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization">Non-negative matrix
factorization</a> seeks a linear map represented by a matrix with no negative
elements - a restriction which PCA does not have.</p>
<p>I mention these other techniques to make the point that merely specifying that
<span class="math inline">\(f\)</span> should be a linear map underspecifies the problem, and we need to be
careful about what additional requirement we add if we’re going to end up with
PCA instead of some other method.</p>
<p>Surprisingly, there are actually at least three different ways of fully
specifying PCA, all of which seem very different at first but can be shown to
be mathematically equivalent:</p>
<ol style="list-style-type: decimal">
<li>Require the covariance matrix of the transformed data to be diagonal. This
is equivalent to saying that the transformed data has no <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a>,
or that all <span class="math inline">\(m\)</span> features of the transformed data are uncorrelated.</li>
<li>Seek a new basis for the data such that the first basis vector points in the
direction of maximum variation, or in other words is the “principle
component” of our data. Then require that the second basis vector points
also points in the direction of maximum variation in the plane orthogonal to
the first, and so on until a new orthonormal basis is constructed.</li>
<li>Seek a new basis for the data such that when we reconstruct the original
matrix from only the <span class="math inline">\(m\)</span> most significant components the <a href="http://users.ics.aalto.fi/harri/dityo/node6.html">reconstruction
error</a> is minimized. Reconstruction error is usually defined as the
<a href="https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm">Frobenius norm</a> of the difference between the original and
reconstructed matrix, but <a href="https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation">other definitions are possible.</a></li>
</ol>
<p>That these very different motivations all lead to the same formal solution is
reminiscent of the fact that the <a href="https://en.wikipedia.org/wiki/Model_of_computation">models of computation</a> proposed independently
by Turing, Church, and Gödel turned out to all be equivalent. Just as this
triple convergence led some to believe that the definition of computation was
discovered rather than merely invented, the fact that PCA keeps popping up
suggests that it is in some fundamental way the “right” way to think about
dimensionality reduction. Or maybe it just means mathematicians like to
use linear algebra whenever they can, because non-linear equations are so
difficult to solve. But I digress.</p>
<p>Under any of these three definitions, the linear map <span class="math inline">\(f\)</span> that we seek will turn
out to be represented by a unique<sup>*</sup> (*some terms and conditions apply)
orthogonal matrix <span class="math inline">\(Q\)</span>.</p>
<p>Orthogonal matrices are the generalization of the 3-dimensional concept of a
rotation or reflection: in particular they always preserve both <em>distance</em> and
<em>angles</em>. These are ideal properties for a transform to have because merely
rotating an object or holding it up to a mirror never <em>distorts</em> it, but
simply gives us a different perspective on it.</p>
<p>Imagine, for illustration’s sake that you held in your hand an unfamiliar
object made of some easily deformable material like clay or soft plastic. You
might turn it gently this way and that, peer at it from every angle, or hold it
up to the mirror. But these delicate operations do no harm to the object -
indeed, you don’t really need to touch the object at all! You’re merely
changing your own point of view. Not so if you had flattened or stretched or
twisted it; after these more violent operations any shape or pattern you
perceive may well be the product of your own manipulations and not a true
insight into the nature of the original form.</p>
<p><img src="/post/ml-from-scratch-part-6-pca_files/shape.png"></p>
<p>Orthogonal matrices are the safest and least distorting transformation that we
could apply, and by the same token the most conservative and cautious. These
are powerful virtues in a technique intended to help understand and explore
data without misrepresenting it.</p>
<p>Now, obviously we could rotate our data any which way to get a different
picture, but PCA does more: it rotates it so that in some sense in becomes
aligned to the axes - rather like straightening a picture hanging askew on the
wall. This is the property of PCA that is makes it so desirable for exploratory
analysis.</p>
<p>Orthogonal matrices are always square, so <span class="math inline">\(Q\)</span> is an <span class="math inline">\(m \times m\)</span> matrix.
But multiplying by a square matrix doesn’t reduce the dimensionality at all!
Why is this considered a dimensionality <em>reduction</em> technique? Well, it turns
out that once we’ve rotated our data so that it’s as <em>wide</em> as possible along
the first basis vector, that also means that it ends up as <em>thin</em> as possible
along the last few basis vectors. This only works if the original data really
were all quite close to some line or hyperplane, but with this assumption met
we can safely drop the least significant dimensions and retain only our
principle components, thus reducing dimensionality while keeping most of
the information (variance) of the data. Of course, deciding <em>exactly</em> how many
dimensions to drop/retain is a bit tricky, but we’ll come to that later.</p>
<p>For now, let’s explore the mathematics and show how PCA gives rise to a
unique solution subject to the above constraints.</p>
</div>
<div id="approach-1-the-direction-of-maximal-variation" class="section level2">
<h2>Approach #1: The Direction of Maximal Variation</h2>
<p>Before we can define the direction of maximal variance, we must be
clear about what we mean by variance in a given direction. First, let’s say
that <span class="math inline">\(\mathbf{x}\)</span> is an <span class="math inline">\(n\)</span>-dimensional random vector. This represents the
population our data will be sampled from. Next, suppose you have some
non-random vector <span class="math inline">\(\mathbf{q} \in \mathbb{R}^n\)</span>. Assuming this vector is
non-zero, it defines a line. What do mean by the phrase, “the variance of
<span class="math inline">\(\mathbf{x}\)</span> in the direction of <span class="math inline">\(\mathbf{q}\)</span>?”</p>
<p><img src="/post/ml-from-scratch-part-6-pca_files/variance_along_q.png"></p>
<p>The natural thing to do is to <em>project</em> the <span class="math inline">\(n\)</span>-dimensional random variable
<em>onto</em> the line defined by <span class="math inline">\(\mathbf{q}\)</span>. We can do this with a dot product
which we will write as the matrix product <span class="math inline">\(\mathbf{q}^T \mathbf{x}\)</span>. This new
quantity is clearly a scalar random variable, so we can apply the variance
operator to get a scalar measure of variance.</p>
<p><span class="math display">\[ \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \tag{1} \]</span></p>
<p>Does this suffice to allow us to define a direction of maximal variation? Not
quite. If we try to pose the naive optimization problem:</p>
<p><span class="math display">\[ \underset{\mathbf{q}}{\operatorname{argmax}} \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \tag{2} \]</span></p>
<p>We can easily prove no solution exists. Proof: Assume that <span class="math inline">\(\mathbf{a}\)</span> is the maximum. There
always exists another vector <span class="math inline">\(\mathbf{b} = 2 \mathbf{a}\)</span> which implies that:</p>
<p><span class="math display">\[\operatorname{Var}[ \mathbf{b}^T \mathbf{x}]  = 4 \operatorname{Var}[ \mathbf{a}^T \mathbf{x} ] &gt; \operatorname{Var}[ \mathbf{a}^T \mathbf{x} ] \tag{3}\]</span>.</p>
<p>Which implies that <span class="math inline">\(\mathbf{a}\)</span> was not the maximum after all, which is absurd.
Q.E.D. The upshot is that we must impose some additional constraint.</p>
<p>Recall that a dot product is only a projection in a geometric sense if <span class="math inline">\(\mathbf{q}\)</span> is a
<em>unit</em> vector. Why don’t we impose the condition</p>
<p><span class="math display">\[ \mathbf{q}^T \mathbf{q} = 1 \tag{4} \]</span></p>
<p>To obtain the <em>constrained</em> optimization problem</p>
<p><span class="math display">\[ \underset{\mathbf{q}}{\operatorname{argmax}} \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \quad\quad \text{such that} \, \mathbf{q}^T \mathbf{q} = 1 \tag{5} \]</span></p>
<p>Well at least this <em>has</em> a solution, even if it isn’t immediately obvious how
to solve it. We can’t simply set partial derivative with respect to
<span class="math inline">\(\mathbf{q}\)</span> equal to zero; that <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT condition</a> only applies in the <em>absence</em> of
active constraints. Earlier in this series, we’ve used techniques such as
stochastic gradient descent to solve <em>unconstrained</em> optimization problems, but
how do we deal the constraint?</p>
<div id="an-ode-to-lagrange-multipliers" class="section level3">
<h3>An Ode to Lagrange Multipliers</h3>
<p>Happily, a very general and powerful trick exists for translating constrained
optimization problems into unconstrained ones: <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">the method of Lagrange
multipliers</a>. In 1780, <a href="https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange">Lagrange</a> was studying the motion of
constrained physical systems. At the time, such problems were usually solved
by finding a suitable set of <a href="https://en.wikipedia.org/wiki/Generalized_coordinates">generalized coordinates</a> but this required a
great deal of human ingenuity.</p>
<p>As a concrete illustration, consider a bead on a stiff, bent wire.</p>
<p><img src="/post/ml-from-scratch-part-6-pca_files/bead_on_wire.png"></p>
<p>The bead moves in all three dimensions and therefore requires three coordinates
<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> to describe its position. However, it is also constrained to
the one dimensional path imposed by the shape of the wire so in theory its
position could be described by a single parameter <span class="math inline">\(t\)</span> describing its position
along the bent path. However, for a very complex path this might be hard to do,
and even harder to work with when calculating the momentum and energy necessary
to describe the dynamics of the system.</p>
<p>Lagrange developed a method by which this could always be done in an
essentially mechanical way, requiring no human insight. As just one example,
the technique is used in modern <a href="https://gamedev.stackexchange.com/questions/117284/when-are-lagrangian-mechanics-useful-in-a-game">physics engines</a> - as new contact points
are added or removed as objects touch in different ways, the physics engine can
dynamically add or remove constraints to model them without ever having to
worry about finding an appropriate set of generalized coordinates.</p>
<p>Lagrange’s genius was to imagine the system could “slip” just ever so
slightly out of its constraints and that the true solution would be the one
that <em>minimized</em> this virtual slippage. This could be elegantly handled by
associating an energy cost called ’virtual work" that penalized the system
proportional to the degree to which the constraints were violated. This trick
reconceptualizes a hard constraint as just another parameter to optimize in an
unconstrained system! And surprisingly enough, it does not result in an
<em>approximate</em> solution that only sort of obeys the constraint but instead
(assuming the constraint is physically possible and the resulting equations
have a closed form solution) gives an <em>exact</em> solution where the constraint is
<em>perfectly</em> obeyed.</p>
<p>It’s easy to use too, at least in our simple case. We introduce the Lagrange
multiplier <span class="math inline">\(\lambda\)</span> and rewrite our optimization as follows:</p>
<p><span class="math display">\[ \underset{\mathbf{q} ,\, \lambda}{\operatorname{argmax}} \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] + \lambda (1 - \mathbf{q}^T \mathbf{q}) \tag{6} \]</span></p>
<p>Why is this the same as the above? Let’s call the above function <span class="math inline">\(f(\mathbf{q}, \lambda)\)</span> write down the <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT conditions</a>:</p>
<p><span class="math display">\[ 
  \begin{align}
    \nabla_\mathbf{q} f &amp; = 0 \tag{7} \\
    \frac{\partial f}{\partial \lambda} &amp; = 0 \tag{8}
  \end{align}
\]</span></p>
<p>But equation (8) is simply <span class="math inline">\(\mathbf{q}^T \mathbf{q} - 1 = 0\)</span> which is simply
our unit vector constraint… this guarantees that when we solve (7) and (8),
the constraint will be exactly satisfied and we’ll will also have found a
solution to (6). Such is the magic of Lagrange multipliers.</p>
<p>But if (8) is just our constraint in a fancy new dress, how have we progressed
at all? Because (7) is now unconstrained (and therefore more tractable!)</p>
</div>
<div id="finding-the-direction-of-maximal-variation" class="section level3">
<h3>Finding the Direction of Maximal Variation</h3>
<p>Suppose the matrix <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(N \times n\)</span> matrix with <span class="math inline">\(N\)</span> rows where
each row vector is an independent realization of <span class="math inline">\(\mathbf{x}\)</span>. Also assume
(without loss of generality) that the mean of each column of <span class="math inline">\(\mathbf{X}\)</span> is
zero. (This can always be accomplished by simply subtracting off a mean vector
<span class="math inline">\(\mathbf{\mu}\)</span> before applying PCA.)</p>
<p>We can estimate the covariance matrix of <span class="math inline">\(\mathbf{X}\)</span> as</p>
<p><span class="math display">\[ \mathbf{C} = \frac{\mathbf{X}^T \mathbf{X}}{n} \tag{9} \]</span></p>
<p><span class="math display">\[ f = \mathbf{q}^T \mathbf{C} \mathbf{q} + \lambda (1 - \mathbf{q}^T \mathbf{q}) \tag{10} \]</span></p>
<p><span class="math display">\[ \nabla f = 2 \mathbf{C} \mathbf{q} - 2 \lambda \mathbf{q} \tag{11} \]</span></p>
<p>Dividing by two and moving each term to opposite sides of the equation, we get the
familiar equation for the eigenproblem:</p>
<p><span class="math display">\[ \mathbf{C} \mathbf{q} = \lambda \mathbf{q} \tag{12} \]</span></p>
<p>This shows that every “direction of maximal variation” is in fact an eigenvector
of the covariance matrix, and the variance in that direction is the corresponding
eigenvalue.</p>
<p>Because the covariance matrix <span class="math inline">\(\mathbf{C}\)</span> is real-valued, symmetric, and positive
definite, we know that the eigenvalues will all be real-valued (as expected.)</p>
<p>Thus, to find the axes of maximal variation, it suffices to find the eigendecomposition
of <span class="math inline">\(\mathbf{C}\)</span>.</p>
<p>Define <span class="math inline">\(\mathbf{Q}\)</span> to be the <span class="math inline">\(n \times n\)</span> <em>right</em> eigenvalue matrix (meaning
each <em>column</em> is an eigenvector) and <span class="math inline">\(\mathbf{\Lambda}\)</span> is the diagonal <span class="math inline">\(n \times n\)</span>
matrix containing eigenvalues along the diagonal.</p>
<p><span class="math display">\[ \mathbf{C} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T \tag{13} \]</span></p>
<p>We will discuss the algorithm necessary to compute <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\mathbf{\Lambda}\)</span>
below, but first let’s discuss some alternative ways to motivate PCA.</p>
</div>
</div>
<div id="approach-2-diagonalizing-the-covariance-matrix" class="section level2">
<h2>Approach #2: Diagonalizing the Covariance Matrix</h2>
<p>We could have skipped the entire “direction of maximal variation” and Lagrange
multiplier argument if we had simply argued as follows: I want my features to
be uncorrelated. Which is to say, I want the covariance matrix of my data to be
diagonal. However, when I estimate the covariance matrix for my data in their
original form, I see that the covariance matrix is <strong>not</strong> diagonal. That
means my original features exhibit some multiple collinearity, which is bad. To
fix this problem, I will transform my data in such a way as make the covariance
matrix diagonal. It is well-known that a matrix can be diagonalized by finding
its eigendecomposition. Therefore, I need to find the eigendecomposition
<span class="math inline">\(\mathbf{C} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T\)</span>.</p>
<p>The resulting right eigenvector matrix <span class="math inline">\(\mathbf{Q}\)</span> can be applied to <span class="math inline">\(X\)</span>,
yielding a new, transformed data set <span class="math inline">\(\mathbf{X}&#39;\)</span>.</p>
<p><span class="math display">\[ \mathbf{X}&#39; = \mathbf{X} \mathbf{Q} \tag{14} \]</span></p>
<p>When we then estimate the empirical covariance of <span class="math inline">\(\mathbf{X}&#39;\)</span>, we find</p>
<p><span class="math display">\[ \begin{align}
\mathbf{C}&#39; &amp; = \frac{\mathbf{X}&#39;^T \mathbf{X}&#39;}{n} \tag{15} \\
&amp; = \frac{(\mathbf{X}\mathbf{Q})^T (\mathbf{X}\mathbf{Q})}{n} \\
&amp; = \frac{\mathbf{Q}^T \mathbf{X}^T \mathbf{X}\mathbf{Q}}{n} \\
&amp; = \mathbf{Q}^T \Bigg( \frac{\mathbf{X}^T \mathbf{X}}{n} \Bigg) \mathbf{Q} \\
&amp; = \mathbf{Q}^T \mathbf{C} \mathbf{Q} \\
&amp; = \mathbf{\Lambda}
\end{align}
\]</span></p>
<p>Because <span class="math inline">\(\mathbf{\Lambda}\)</span> is a diagonal matrix, we’ve shown that the empirical
covariance of our transformed data set is diagonal; which is to say, all of the
features of <span class="math inline">\(X&#39;\)</span> are independent.</p>
<p>This argument is much more brutish and perhaps too on the nose: we wanted a
diagonal covariance matrix, so we diagonalized it, anyone got a problem with
that? However, it has the advantage of requiring nothing more than linear
algebra: no statistics, multivariate calculus, or optimization theory here! All
we need is a common-sense attitude toward working with data, or what my boss
sometimes calls “street statistics.”</p>
<div id="a-brief-aside-about-loadings" class="section level3">
<h3>A Brief Aside About Loadings</h3>
<p>At this point it may also be worth mentioning that multiplying by the left
eigenvector matrix <span class="math inline">\(\mathbf{Q}\)</span> is only one of two common ways to define the
transformed data <span class="math inline">\(\mathbf{X}&#39;\)</span>. Alternatively, we could have used the so-called
“<a href="https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another">loadings</a>” matrix, defined like so:</p>
<p><span class="math display">\[ \mathbf{L} = \mathbf{Q} \sqrt{\mathbf{\Lambda}} \tag{16} \]</span></p>
<p>The square root of a matrix may seem strange to you, but recall that
<span class="math inline">\(\mathbf{\Lambda}\)</span> is diagonal, so this just means the element-wise square root
of each eigenvalue. Using <span class="math inline">\(\mathbf{L}^{-1}\)</span> instead of <span class="math inline">\(\mathbf{Q}\)</span> to
transform from <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(\mathbf{X}&#39;\)</span> means the empirical covariance
matrix of the transformed data will be the identity matrix. This can be more
intuitive in some cases. Also, many software packages report the loading matrix
instead of the eigenvectors on the basis that they are easier to interpret.</p>
</div>
</div>
<div id="approach-3-minimizing-reconstruction-error" class="section level2">
<h2>Approach #3: Minimizing Reconstruction Error</h2>
<p>The third and final way to motivate the mathematical formalism of PCA is to
view it as a form of <em>compression</em>. Specifically, of all possible linear
projections from <span class="math inline">\(n\)</span> to <span class="math inline">\(m\)</span> dimensions, taking the first <span class="math inline">\(k\)</span> components of the
PCA transformation of <span class="math inline">\(X\)</span> minimizes the <em>reconstruction error</em>. Reconstruction
error is usually defined as the Frobenius distance between the original and
reconstructed matrix but interestingly enough the theorem holds for a few
other metrics as well, suggesting it’s a deep property of PCA and not some
quirk of the Frobenius norm.</p>
<p>I won’t go through this derivation here - you can find <a href="http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture25.pdf">a presentation
elsewhere</a> if you’re interested in the details - but it’s an extremely
powerful point of view which goes straight to the heart of the dimensional
reduction strategy. If we can reconstruct our original data almost exactly from
only a handful of components that lends strong support to the notion that any
interesting information about the image must have been contained in those few
components.</p>
<p>Consider this series of images, taken from <a href="https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples">this article</a>:</p>
<p><img src="/post/ml-from-scratch-part-6-pca_files/pca_image_example.png"></p>
<p>Here, 512 dimensions was reduced to just 29, but the reconstructed image is still perfectly recognizable.</p>
<p>With three separate theoretical justifications under our belt - which is two too many to be honest -
let’s turn our attention to the concrete problem of implementing eigendecomposition from scratch.</p>
</div>
<div id="algorithm-for-solving-the-eigenproblem" class="section level2">
<h2>Algorithm for Solving the Eigenproblem</h2>
<p>The modern approach to implementing PCA is to find the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition</a> of a matrix <span class="math inline">\(A\)</span> which
almost immediately gives us the eigenvalues of and eigenvectors of <span class="math inline">\(A^T A\)</span>. The
best known SVD algorithm is the <a href="http://people.duke.edu/~hpgavin/SystemID/References/Golub+Reinsch-NM-1970.pdf">Golub-Reinsh Algorithm</a>. This is an
iterative algorithm. Starting with <span class="math inline">\(A_1 = A\)</span> we calculate <span class="math inline">\(A_{k+1}\)</span> from <span class="math inline">\(A_k\)</span>
until we achieve convergence.</p>
<p>For each step we first use <a href="https://en.wikipedia.org/wiki/Householder_transformation">Householder reflections</a> to reduce the matrix
to bidiagonal form <span class="math inline">\(A_k\)</span>, then use a <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> of <span class="math inline">\(X_k^T X_k\)</span> to
set many of the off-diagonal elements to zero. The resulting matrix <span class="math inline">\(A_{k+1}\)</span>
is <a href="https://en.wikipedia.org/wiki/Tridiagonal_matrix">tridiagonal</a>, but at each step the off-diagonal elements get smaller
and smaller. This is very much like trying to get rid of all the air bubbles in
wallpaper by flattening them with a stick, only to have new bubbles pop up.
However, the off-diagonal elements introduced by the process are smaller on
average than the original and it can be proved to converge to zero even if they
will never be exactly zero. In practice this converge is extremely rapid for
well-conditioned matrices.</p>
<p>There is also a <a href="https://arxiv.org/abs/0909.4061">randomized algorithm due to Halko, Martinsson, and Tropp</a>
which can be much faster, especially when we only want to retain a small number
of components. This is commonly used with very large sparse matrices.</p>
<p>Normally I would tackle one of these “best practice” algorithms, but after
studying them I found them to be larger in scope than what I would want to
tackle for one of these articles. Instead, I decided to implement an older but
still quite adequate eigenvalue algorithm: known as the <a href="https://en.wikipedia.org/wiki/QR_algorithm">QR algorithm</a>.
In addition to being easy to understand and implement, it has the advantage
that we can use the <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> function that we implemented in the
earlier <a href="/post/ml-from-scratch-part-1-linear-regression/">article on linear regression</a>. It’s just as fast or faster than
Golub-Reinsh; the disadvantage is that it is not as numerically stable
particularly for the smallest eigenvalues. Because in PCA we normally intend to
discard these anyway, this is not such a bad deal!</p>
<p>Recall from that <a href="/post/ml-from-scratch-part-1-linear-regression/">previous article</a> our implementations for Householder
reflections and QR decompositions:</p>
<pre><code>def householder_reflection(a, e):
    &#39;&#39;&#39;
    Given a vector a and a unit vector e,
    (where a is non-zero and not collinear with e)
    returns an orthogonal matrix which maps a
    into the line of e.
    &#39;&#39;&#39;
    
    assert a.ndim == 1
    assert np.allclose(1, np.sum(e**2))
    
    u = a - np.sign(a[0]) * np.linalg.norm(a) * e  
    v = u / np.linalg.norm(u)
    H = np.eye(len(a)) - 2 * np.outer(v, v)
    
    return H


def qr_decomposition(A):
    &#39;&#39;&#39;
    Given an n x m invertable matrix A, returns the pair:
        Q an orthogonal n x m matrix
        R an upper triangular m x m matrix
    such that QR = A.
    &#39;&#39;&#39;
    
    n, m = A.shape
    assert n &gt;= m
    
    Q = np.eye(n)
    R = A.copy()
    
    for i in range(m - int(n==m)):
        r = R[i:, i]
        
        if np.allclose(r[1:], 0):
            continue
            
        # e is the i-th basis vector of the minor matrix.
        e = np.zeros(n-i)
        e[0] = 1  
        
        H = np.eye(n)
        H[i:, i:] = householder_reflection(r, e)

        Q = Q @ H.T
        R = H @ R
    
    return Q, R</code></pre>
<p>Using these we can implement the QR algorithm in just a few lines of code.</p>
<p>The QR algorithm is iterative: at each step, we calculate <span class="math inline">\(A_{k+1}\)</span> by taking
the QR decomposition of <span class="math inline">\(A_{k}\)</span>, reversing the order of Q and R, and
multiplying the matrices together. Each time we do this, the off-diagonals get
smaller.</p>
<pre><code>def eigen_decomposition(A, max_iter=100):
    A_k = A
    Q_k = np.eye( A.shape[1] )
    
    for k in range(max_iter):
        Q, R = qr_decomposition(A_k)
        Q_k = Q_k @ Q
        A_k = R @ Q

    eigenvalues = np.diag(A_k)
    eigenvectors = Q_k
    return eigenvalues, eigenvectors</code></pre>
</div>
<div id="implementing-pca" class="section level2">
<h2>Implementing PCA</h2>
<p>We made a number of simplifying assumptions in the above theory and now we have
to pay with a corresponding amount of busywork to get our data into an
idealized form. There are really two pieces of book-keeping to implement:</p>
<ol style="list-style-type: decimal">
<li>We need to ensure than the data are centered</li>
<li>Optionally “whiten” the data so that each feature has unit variance</li>
<li>put eigenvalues in descending order</li>
</ol>
<p>Aside from these considerations, the entire <code>fit()</code> function is little more
than the handful of lines necessary to diagonalize the empirical covariance
matrix. All of the hard work is done inside <code>eigen_decomposition()</code>.</p>
<pre><code>class PCA:
    def __init__(self, n_components=None, whiten=False):
        self.n_components = n_components
        self.whiten = bool(whiten)
    
    def fit(self, X):
        n, m = X.shape
        
        # subtract off the mean to center the data.
        self.mu = X.mean(axis=0)
        X = X - self.mu
        
        # whiten if necessary
        if self.whiten:
            self.std = X.std(axis=0)
            X = X / self.std
        
        # Eigen Decomposition of the covariance matrix
        C = X.T @ X / (n-1)
        self.eigenvalues, self.eigenvectors = eigen_decomposition(C)
        
        # truncate the number of components if doing dimensionality reduction
        if self.n_components is not None:
            self.eigenvalues = self.eigenvalues[0:self.n_components]
            self.eigenvectors = self.eigenvectors[:, 0:self.n_components]
        
        # the QR algorithm tends to puts eigenvalues in descending order 
        # but is not guarenteed to. To make sure, we use argsort.
        descending_order = np.flip(np.argsort(self.eigenvalues))
        self.eigenvalues = self.eigenvalues[descending_order]
        self.eigenvectors = self.eigenvectors[:, descending_order]

        return self

    def transform(self, X):
        X = X - self.mu
        
        if self.whiten:
            X = X / self.std
        
        return X @ self.eigenvectors
    
    @property
    def proportion_variance_explained(self):
        return self.eigenvalues / np.sum(self.eigenvalues)
    </code></pre>
</div>
<div id="wine-quality-example" class="section level2">
<h2>Wine Quality Example</h2>
<p>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html">wine quality data set</a> consists of 178 wines, each described in
terms of 13 different objectively quantifiable chemical or optical properties such as the
concentration of alcohol or the hue and intensity of the color. Each has been
assigned to one of three possible classes depending on a subjective judgement of
quality.</p>
<p>Thirteen dimensions isn’t nearly as bad as the hundreds or thousands commonly encountered
in machine learning, but still rather more than the handful we poor 3-dimensional creatures are
comfortable thinking about. We’d like to get that down to something manageable, certainly no more than three.
So some kind of dimensionality reduction is indicated.</p>
<pre><code>import pandas as pd
import seaborn
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn.datasets import load_wine

wine = load_wine()
X = wine.data

df = pd.DataFrame(data=X, columns=wine.feature_names)
display(df.head().T)</code></pre>
<p>A sample of the first five wines in the dataset:</p>
<table style="width:80%">
<thead>
<tr class="header">
<th>
</th>
<th align="right">
#1
</th>
<th align="right">
#2
</th>
<th align="right">
#3
</th>
<th align="right">
#4
</th>
<th align="right">
#5
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>
alcohol
</td>
<td align="right">
14.23
</td>
<td align="right">
13.2
</td>
<td align="right">
13.16
</td>
<td align="right">
14.37
</td>
<td align="right">
13.24
</td>
</tr>
<tr class="even">
<td>
malic_acid
</td>
<td align="right">
1.71
</td>
<td align="right">
1.78
</td>
<td align="right">
2.36
</td>
<td align="right">
1.95
</td>
<td align="right">
2.59
</td>
</tr>
<tr class="odd">
<td>
ash
</td>
<td align="right">
2.43
</td>
<td align="right">
2.14
</td>
<td align="right">
2.67
</td>
<td align="right">
2.5
</td>
<td align="right">
2.87
</td>
</tr>
<tr class="even">
<td>
alcalinity_of_ash
</td>
<td align="right">
15.6
</td>
<td align="right">
11.2
</td>
<td align="right">
18.6
</td>
<td align="right">
16.8
</td>
<td align="right">
21
</td>
</tr>
<tr class="odd">
<td>
magnesium
</td>
<td align="right">
127
</td>
<td align="right">
100
</td>
<td align="right">
101
</td>
<td align="right">
113
</td>
<td align="right">
118
</td>
</tr>
<tr class="even">
<td>
total_phenols
</td>
<td align="right">
2.8
</td>
<td align="right">
2.65
</td>
<td align="right">
2.8
</td>
<td align="right">
3.85
</td>
<td align="right">
2.8
</td>
</tr>
<tr class="odd">
<td>
flavanoids
</td>
<td align="right">
3.06
</td>
<td align="right">
2.76
</td>
<td align="right">
3.24
</td>
<td align="right">
3.49
</td>
<td align="right">
2.69
</td>
</tr>
<tr class="even">
<td>
nonflavanoid_phenols
</td>
<td align="right">
0.28
</td>
<td align="right">
0.26
</td>
<td align="right">
0.3
</td>
<td align="right">
0.24
</td>
<td align="right">
0.39
</td>
</tr>
<tr class="odd">
<td>
proanthocyanins
</td>
<td align="right">
2.29
</td>
<td align="right">
1.28
</td>
<td align="right">
2.81
</td>
<td align="right">
2.18
</td>
<td align="right">
1.82
</td>
</tr>
<tr class="even">
<td>
color_intensity
</td>
<td align="right">
5.64
</td>
<td align="right">
4.38
</td>
<td align="right">
5.68
</td>
<td align="right">
7.8
</td>
<td align="right">
4.32
</td>
</tr>
<tr class="odd">
<td>
hue
</td>
<td align="right">
1.04
</td>
<td align="right">
1.05
</td>
<td align="right">
1.03
</td>
<td align="right">
0.86
</td>
<td align="right">
1.04
</td>
</tr>
<tr class="even">
<td>
od280/od315_of_diluted_wines
</td>
<td align="right">
3.92
</td>
<td align="right">
3.4
</td>
<td align="right">
3.17
</td>
<td align="right">
3.45
</td>
<td align="right">
2.93
</td>
</tr>
<tr class="odd">
<td>
proline
</td>
<td align="right">
1065
</td>
<td align="right">
1050
</td>
<td align="right">
1185
</td>
<td align="right">
1480
</td>
<td align="right">
735
</td>
</tr>
</tbody>
</table>
<p>These data have two characteristics that we should consider carefully.</p>
<p>First, we can see that the features of this dataset are not on the same scale;
proline in particular is a thousand times greater than the others. That
strongly suggests that we should “whiten” the data (scale everything so that
each feature has unit variance before applying PCA.) Here, just for the
purposes of this visualization, we will manually whiten the data.</p>
<pre><code>X_white = (X - X.mean(axis=0))/X.std(axis=0)
C = X_white.T @ X_white / (X_white.shape[0] - 1)
plt.figure(figsize=(6,6))
plt.imshow(C, cmap=&#39;binary&#39;)
plt.title(&quot;Covariance Matrix of Wine Data&quot;)
plt.xticks(np.arange(0, 13, 1))
plt.yticks(np.arange(0, 13, 1))
plt.colorbar()</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_covariance.png"></p>
<p>Second, it is clear from this plot that this dataset exhibits significant
<a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a>. Every feature exhibiting high correlations with
several others; no feature is truly independent. While that would be a bad
thing for say, linear regression, it means these data are an ideal candidate
for PCA.</p>
<pre><code>pca = PCA(whiten=True)
pca.fit(X)
X_prime = pca.transform(X)</code></pre>
<p>From the eigenvalues, we can see that the first few components explain most of
the variance:</p>
<pre><code>pca.eigenvalues
array([4.732, 2.511, 1.454, 0.924, 0.858, 0.645, 
       0.554, 0.350, 0.291, 0.252, 0.227, 0.170,  0.104])</code></pre>
<p>The raw eigenvectors are hard to interpret directly, but if you like you can
read the columns (starting with the leftmost) and see which features are being
rolled up into each component; for example, it seems that “flavanoids” and
“phenols” (whatever those are) are major contributors to the first principle
component, among others, while “ash” contributes almost nothing to it.</p>
<pre><code>pca.eigenvectors

array([[ 0.144, -0.484, -0.207,  0.018,  0.266,  0.214, -0.056,  0.396, -0.509, -0.212,  0.226,  0.266, -0.015],
       [-0.245, -0.225,  0.089, -0.537, -0.035,  0.537,  0.421,  0.066,  0.075,  0.309, -0.076, -0.122, -0.026],
       [-0.002, -0.316,  0.626,  0.214,  0.143,  0.154, -0.149, -0.17 ,  0.308,  0.027,  0.499,  0.05 ,  0.141],
       [-0.239,  0.011,  0.612, -0.061, -0.066, -0.101, -0.287,  0.428, -0.2  , -0.053, -0.479,  0.056, -0.092],
       [ 0.142, -0.3  ,  0.131,  0.352, -0.727,  0.038,  0.323, -0.156, -0.271, -0.068, -0.071, -0.062, -0.057],
       [ 0.395, -0.065,  0.146, -0.198,  0.149, -0.084, -0.028, -0.406, -0.286,  0.32 , -0.304,  0.304,  0.464],
       [ 0.423,  0.003,  0.151, -0.152,  0.109, -0.019, -0.061, -0.187, -0.05 ,  0.163,  0.026,  0.043, -0.832],
       [-0.299, -0.029,  0.17 ,  0.203,  0.501, -0.259,  0.595, -0.233, -0.196, -0.216, -0.117, -0.042, -0.114],
       [ 0.313, -0.039,  0.149, -0.399, -0.137, -0.534,  0.372,  0.368,  0.209, -0.134,  0.237,  0.096,  0.117],
       [-0.089, -0.53 , -0.137, -0.066,  0.076, -0.419, -0.228, -0.034, -0.056,  0.291, -0.032, -0.604,  0.012],
       [ 0.297,  0.279,  0.085,  0.428,  0.173,  0.106,  0.232,  0.437, -0.086,  0.522,  0.048, -0.259,  0.09 ],
       [ 0.376,  0.164,  0.166, -0.184,  0.101,  0.266, -0.045, -0.078, -0.137, -0.524, -0.046, -0.601,  0.157],
       [ 0.287, -0.365, -0.127,  0.232,  0.158,  0.12 ,  0.077,  0.12 ,  0.576, -0.162, -0.539,  0.079, -0.014]])</code></pre>
<p>Just as a cross check, we can plot the covariance matrix of the transformed
data the same way we did the raw data above:</p>
<pre><code>plt.figure(figsize=(6,6))
plt.imshow(C_prime, cmap=&#39;binary&#39;)
plt.title(&quot;Covariance Matrix of Transformed Data&quot;)
plt.xticks(np.arange(0, 13, 1))
plt.yticks(np.arange(0, 13, 1))
plt.colorbar()</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_transformed_covariance.png"></p>
<p>And we can see the expected structure: eigenvalues descending from 4.7 to 0.1
along the diagonal, and exactly 0 away from the diagonal.</p>
</div>
<div id="visualizing-the-components" class="section level2">
<h2>Visualizing the Components</h2>
<p>PCA was applied only to the 13 features; the partition into three quality
classes was not included. Still, we would like to know if the primary
components have something to say about these classes, so we will color code
them with red, green, and blue.</p>
<p>Let’s start by visualizing only the first component as points along a line:</p>
<pre><code>plt.figure(figsize=(10, 4))
for c in np.unique(wine.target):
    color = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;][c]
    X_class = X_prime[wine.target == c]
    plt.scatter(X_class[:, 0], X_class[:, 1]*0, color=color, alpha=0.3)
plt.title(&quot;Primary Components of Wine Quality&quot;)
plt.xlabel(&quot;PC1&quot;)</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_pc1.png"></p>
<p>This may seem a little silly, but in fact boiling it down to only a single
component is often the best option: if you can boil a phenomenon down to a
single number in a way that captures the essence, that could be very useful and
in some cases represents an important discovery. For example, the reason we can
talk about “mass” without clarifying “inertial mass” vs. “gravitational mass”
is because we strongly believe those quantities are identical in all cases. Is
it possible that wine is neither complex nor multidimensional, but simply
exists along a spectrum from poor to good quality?</p>
<p>However, in this case, it seems like the first principle component does <em>not</em>
fully capture the concept of quality. Let’s try plotting the first two
components.</p>
<pre><code>plt.figure(figsize=(10, 8))
for c in np.unique(wine.target):
    color = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;][c]
    X_class = X_prime[wine.target == c]
    plt.scatter(X_class[:, 0], X_class[:, 1], color=color, alpha=0.6)
plt.title(&quot;Primary Components of Wine Quality&quot;)
plt.xlabel(&quot;PC1&quot;)
plt.ylabel(&quot;PC2&quot;)</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_pc2.png"></p>
<p>Now we can see that each class corresponds to a well-localized cluster with
little overlap. In many cases, where PC1 is right on the boundary between two
classes, it is PC2 that supplies the tiebreaker. Note that we could draw a
single <em>curved</em> path that would connect these three regions in order or
ascending quality; this suggests that a non-linear dimensionality reduction
technique, say some kind of manifold learning like <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>, <em>might</em> be
able to reduce quality to a single dimension. But is that the representation it
would discover on its own, when trained in an unsupervised manner?</p>
<p>It may also be worth trying three dimensions, just in case PC3 has some
non-ignorable contribution to quality. Three dimensions is always a little hard
to visualize with standard plotting tools, but we can use pairwise 2D plots:</p>
<pre><code>plt.figure(figsize=(16, 8))

plt.subplot(1, 2, 1)
for c in np.unique(wine.target):
    color = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;][c]
    X_class = X_prime[wine.target == c]
    plt.scatter(X_class[:, 0], X_class[:, 2], color=color, alpha=0.6)
plt.title(&quot;Primary Components of Wine Quality&quot;)
plt.xlabel(&quot;PC1&quot;)
plt.ylabel(&quot;PC3&quot;)

plt.subplot(1, 2, 2)
for c in np.unique(wine.target):
    color = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;][c]
    X_class = X_prime[wine.target == c]
    plt.scatter(X_class[:, 1], X_class[:, 2], color=color, alpha=0.6)
plt.title(&quot;Primary Components of Wine Quality&quot;)
plt.xlabel(&quot;PC2&quot;)
plt.ylabel(&quot;PC3&quot;)</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_pc3.png"></p>
<p>Or the kind of plot which is called 3D, although it’s really just
a slightly more sophisticated projection on 2D:</p>
<pre><code>fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
ax.view_init(15, -60)

for c in np.unique(wine.target):
    color = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;][c]
    X_class = X_prime[wine.target == c]
    ax.scatter(X_class[:, 0], X_class[:, 1], X_class[:, 2], color=color, alpha=0.6)
    
# chart junk
plt.title(&quot;First 3 Primary Components of Wine Quality&quot;)
ax.set_xlabel(&#39;PC1&#39;)
ax.set_ylabel(&#39;PC2&#39;)
ax.set_zlabel(&#39;PC3&#39;)</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_pc3_3d.png"></p>
<p>While these charts look attractive enough, they don’t seem to make a compelling
case for including PC3. At least for the purposes of understanding wine quality
it seems we can retain just two principle components and still get a complete
picture.</p>
<p>At this point in a real analysis, we would spend some time understanding what
PC1 and PC2 really represent, looking at which of the 13 original features
contribute to each, and whether positive or negative, and how this relates to
quality. But there is an elephant in the room - the informal or seemingly <em>ad
hoc</em> method I used for deciding to use two components instead of one or three.
While “just look at the data and use the one that makes sense” has a certain
pragmatic and commonsensical appeal, it’s also easy to see that it’s subjective
enough to allow bias to creep in. As such, many people have asked themselves if
there were not some rigorous decision rule that could be applied.</p>
</div>
<div id="strategies-for-choosing-the-number-of-dimensions" class="section level2">
<h2>Strategies for Choosing The Number of Dimensions</h2>
<p>The oldest and most venerable method involves plotting the eigenvalues in
descending order as a function of dimension number: whimsically called a <a href="https://en.wikipedia.org/wiki/Scree_plot">scree
plot</a> after a resemblance to the “elbow” that appears near the base of some
mountain where loose stones are piled upon a slope:</p>
<p><img src="/post/ml-from-scratch-part-6-pca_files/scree.png"></p>
<p>To determine the number of components to retain, it is suggested to look for a
visual “elbow” point at which the chart noticeably flattens out and use that
for the cut-off.</p>
<p>Alternatively, if a deterministic rule is required, one might use so-called the
<a href="https://en.wikipedia.org/wiki/Factor_analysis#Older_methods">Kaiser criterion</a> : drop all components with an eigenvalue less than 1 and
retain all those with an eigenvalue greater than 1.</p>
<p>Before discussing the merits or demerits of these approaches, let’s just create
the scree plot for the wine quality data set:</p>
<pre><code>fig = plt.figure(figsize=(10, 7))
plt.title(&quot;Scree Plot (Eigenvalues in Decreasing Order)&quot;)
plt.plot([1, 13], [1, 1], color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&quot;Kaiser Rule&quot;)
plt.xticks(np.arange(1, 14, 1))
plt.xlim(1, 13)
plt.ylim(0, 5)
plt.ylabel(&quot;Eigenvalue&quot;)
plt.xlabel(&quot;Principle Component Index&quot;)
plt.grid(linestyle=&#39;--&#39;)
plt.plot(range(1, 14), pca.eigenvalues, label=&quot;Eigenvalues&quot;)
plt.legend()</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_scree_plot.png"></p>
<p>In this case, no prominent elbow is apparent, at least to my eyes. This is
disappointing but not surprising: whenever I’ve tried to use this rule in my
work I’ve found it to be at least somewhat ambiguous - and liable to influenced
by irrelevant things such as the aspect ratio of the chart!</p>
<p>The Kaiser criterion, on the other hand, is a <a href="https://stats.stackexchange.com/questions/253535/the-advantages-and-disadvantages-of-using-kaiser-rule-to-select-the-number-of-pr">complete train wreck</a>
and ultimately is no more likely to protect you from criticism than simply
asserting that you used your best judgement.</p>
<p>Another approach is to decide before hand that you want to retain some fraction
of the total variance, say 80% or 99%, and choose a number of components which
give you the desired fidelity. (This approach is particularly attractive if you
have the “compression” point-of-view in mind.) Although the proportion of
variance explained is calculated from the same eigenvalues used for the scree
plot, the difference here is that we are now looking at the cumulative sum of
eigenvalues.</p>
<pre><code>fig = plt.figure(figsize=(10, 7))
plt.title(&quot;Variance Explained By Component&quot;)
plt.xticks(np.arange(1, 14, 1))
plt.yticks(np.arange(0, 1.0001, 0.1))
plt.xlim(1, 13)
plt.ylim(0, 1)
plt.ylabel(&quot;Proportion of Variance Explained&quot;)
plt.xlabel(&quot;Principle Component Index&quot;)
plt.grid(linestyle=&#39;--&#39;)
plt.fill_between(
    range(1, 14), 
    np.cumsum(pca.proportion_variance_explained), 
    0, 
    color=&quot;lightblue&quot;, 
    label=&quot;Cumulative&quot;)
plt.plot(
    range(1, 14), 
    np.cumsum(pca.proportion_variance_explained), 
    0, 
    color=&quot;darkblue&quot;)
plt.plot(
    range(1, 14), 
    pca.proportion_variance_explained, 
    label=&quot;Incremental&quot;, 
    color=&quot;orange&quot;, 
    linestyle=&quot;--&quot;)
plt.legend(loc=&#39;upper left&#39;)</code></pre>
<p><img src="/post/ml-from-scratch-part-6-pca_files/wine_variance_explained.png"></p>
<p>One benefit of this approach is that it is much easier to explain; one does not
need to use term “eigen” at all! People familiar with ANOVA will be comfortable
with the concept of “proportion of variance explained” and this can even be
glossed as “information” for audiences where even the word “variance” might be
a little scary. “We used PCA to compress the data from 512 to 29 dimensions
while retaining 95% of the information” may be criticized for not using the
information theoretic definition of “information” but is clear enough to a
broad audience.</p>
<p>Still, we haven’t really solved the issue of having to choose an arbitrary
threshold, have we? All we’ve done is couch the choice in terms of a more
intuitive metric. I’m not sure any definitive and universally accepted answer
exists - but the wonderfully named paper <em><a href="http://www.quantpsy.org/pubs/preacher_maccallum_2003.pdf">Repairing Tom Swift’s
Electric Factor Analysis Machine</a></em> suggests one method, and I’ve seen
several references to this <a href="https://vismod.media.mit.edu/tech-reports/TR-514.pdf">paper by Minka</a> which may represent the
current state-of-the-art.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>PCA is the archetypical dimensionality reduction method; just as
<a href="https://en.wikipedia.org/wiki/K-means_clustering"><span class="math inline">\(k\)</span>-means</a> is the archetypical clustering method. Now that we’ve
implemented both a dimensional reduction and a clustering method (in the <a href="/post/ml-from-scratch-part-5-gmm/">last
article</a>) <a href="/tags/from-scratch/">from scratch</a>, we should have a pretty good handle on
the basics of unsupervised learning. In particular, we’ve seen many of the
frustration and limitations inherent in unsupervised methods, which boil down
to the impossibility of objectively deciding in a given model is doing a good
job or a bad job. This in turn makes it next to impossible to decide between
similar models, which tends to come down to a question of subjective judgement.
Unfortunately, these models <em>do</em> have hyperparameters, so there are choices
that need to be made… but can any choice be defended to the satisfaction of a
hostile (or who simply have extremely high standards) third-party?</p>
<p>More rewarding then wrestling with the ill-defined problems of unsupervised
learning was the implementation of a eigendecomposition algorithm in a way that
met the fairly stringent rules of the “from scratch” challenge.</p>
</div>

    </article>

    <hr>


    <ul class="pager article-pager">
      <li class="pager-newer">
          <a href="/post/eight-billion/" data-toggle="tooltip" data-placement="top" title="Eight Billion People">&lt; Newer</a>
      </li>
      <li class="pager-older">
        <a href="/post/slow-fibonacci/" data-toggle="tooltip" data-placement="top" title="A Seriously Slow Fibonacci Function">Older &gt;</a>
      </li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2024 Oran Looney
  </div>
  <ul class="site-footer-items">
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
