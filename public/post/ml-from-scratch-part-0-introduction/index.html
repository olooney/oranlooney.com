<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML From Scratch, Part 0: Introduction - OranLooney.com</title>
  <meta property="og:title" content="ML From Scratch, Part 0: Introduction - OranLooney.com" />
  <meta name="twitter:title" content="ML From Scratch, Part 0: Introduction - OranLooney.com" />
  <meta name="description" content="Motivation  “As an apprentice, every new magician must prove to his own satisfaction, at least once, that there is truly great power in magic.” - The Flying Sorcerers, by David Gerrold and Larry Niven
 How do you know if you really understand something? You could just rely on the subjective experience of feeling like you understand. This sounds plausible - surely you of all people should know, right? But this runs head-first into in the Dunning-Kruger effect.">
  <meta property="og:description" content="Motivation  “As an apprentice, every new magician must prove to his own satisfaction, at least once, that there is truly great power in magic.” - The Flying Sorcerers, by David Gerrold and Larry Niven
 How do you know if you really understand something? You could just rely on the subjective experience of feeling like you understand. This sounds plausible - surely you of all people should know, right? But this runs head-first into in the Dunning-Kruger effect.">
  <meta name="twitter:description" content="Motivation  “As an apprentice, every new magician must prove to his own satisfaction, at least once, that there is truly great power in magic.” - The Flying Sorcerers, by David Gerrold and Larry Niven …">
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>
 
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/search/" title="Site Search"><i class="fa fa-search"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//honeycode.tumblr.com/" target="_blank" title="Honeycode Microblog">
      <svg class="svg-inline--fa fa-w-12" aria-hidden="true" data-prefix="fab" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="20 50 350 450" data-fa-i2svg="">
<path fill="currentColor" d="M193 97l86.6 50v100l-86.6 50l-86.6 -50v-100zM285 255l86.6 50v100l-86.6 50l-86.6 -50v-100zM100 255l86.6 50v100l-86.6 50l-86.6 -50v-100z"></path>
</svg>
    </a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>
  </ul>

  </nav>
</header>


  <div class="main" role="main">
    <article class="article">
      <img src="/post/ml-from-scratch-part-0-introduction_files/lead.jpg" class="article-image" />
      
      <h1 class="article-title">ML From Scratch, Part 0: Introduction</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-author">
            by <a href="/about/">Oran Looney</a>
        </li>
        <li class="article-meta-date"><time>November 11, 2018</time></li>
        <li class="article-meta-tags">
          <a href="/tags/python/">
            <i class="fas fa-tag"></i>
            Python
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/statistics/">
            <i class="fas fa-tag"></i>
            Statistics
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/from-scratch/">
            <i class="fas fa-tag"></i>
            From Scratch
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/machine-learning/">
            <i class="fas fa-tag"></i>
            Machine Learning
          </a>&nbsp;
        </li>
      </ul>
      
<aside class="toc">
  
</aside>
      <div id="motivation" class="section level2">
<h2>Motivation</h2>
<blockquote>
<p>“As an apprentice, every new magician must prove to his own satisfaction, at
least once, that there is truly great power in magic.” - The Flying Sorcerers,
by David Gerrold and Larry Niven</p>
</blockquote>
<p>How do you know if you really understand something? You <em>could</em> just rely on
the subjective experience of <em>feeling</em> like you understand. This sounds
plausible - surely you of all people should know, right? But this runs
head-first into in the <a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect">Dunning-Kruger effect</a>. Introspection is not a
reliable guide to self-knowledge.</p>
<p>A more objective criterion is suggested by this pithy quote:</p>
<blockquote>
<p>“What I cannot create, I do not understand.” - Richard Feynman</p>
</blockquote>
<p>This is a very famous quote, but it’s not entirely unambiguous. If we’re going
to use it as a guide, we’ll first have to break it down a little.</p>
<p>The most common interpretation might be, “what I cannot explain to a layperson
or to a curious child, I do not understand.” Feynman unambiguously valued the
ability to explain complex physics in plain English, as exemplified in this
anecdote:</p>
<blockquote>
<p>Before the commercial announcement of the Connection Machine CM-1 and all of
our future products, Richard would give a sentence-by-sentence critique of
the planned presentation. “Don’t say ‘reflected acoustic wave.’ Say [echo].”
Or, “Forget all that ‘local minima’ stuff. Just say there’s a bubble caught
in the crystal and you have to shake it out.” Nothing made him angrier than
making something simple sound complicated. - <a href="http://longnow.org/essays/richard-feynman-and-connection-machine/">Danny Hillis</a></p>
</blockquote>
<p>As has often been remarked, explaining things well is often just as beneficial
to the teacher as to the student; it helps reinforce ideas and builds intuition.</p>
<p>If that is all that Feynman had meant, though, why use the term “create” at
all? Surely “explain” or “teach” is closer to the meaning discussed above. So
while “explain in simple terms” is certainly <em>part</em> of it, “create” includes
more than just that. Feynman gives us a clue in this story from his
autobiography:</p>
<blockquote>
<p>“During the conference I was staying with my sister in Syracuse. I brought the
paper home and said to her,”I can’t understand these things that Lee and Yang
are saying. It’s all so complicated."</p>
<p>“No,” she said, “what you mean is not that you can’t understand it, but that
you didn’t invent it. You didn’t figure it out your own way, from hearing the
clue. What you should do is imagine you’re a student again, and take this paper
upstairs, read every line of it, and check the equations. Then you’ll
understand it very easily.”</p>
<p>I took her advice, and checked through the whole thing, and found it to be very
obvious and simple. I had been afraid to read it, thinking it was too
difficult." - Richard Feynman, <i>Surely You’re Joking, Mr. Feynman!</i></p>
</blockquote>
<p>So in the context of math or physics, “create” means something closer to
“derive from first principles by hand.” This is a very strong criteria! If a
person could go into an empty office with a stack of scratch paper and a supply
of sharp pencils, write down all first principles and proceed to derive every
important theorem in their chosen field by hand then it must be conceded
that such a person has some real knowledge.</p>
<p>In the context of computer science and programming, “create” might mean
something like, “write a program from scratch that implements the given
algorithm.” Since machine learning straddles the two, “create” means both: pose
a machine learning problem mathematically, reduce the problem to some tractable
form on paper, then write and implement an algorithm to produce a numerical
approximation of the answer.</p>
<p>Now, if someone attempts this exercise, one of two things will happen. First,
they may succeed completely on their first try. If so, great! They’ve proved
what they set out to prove. But more likely, they’ll
only succeed partially and get stuck at some point. Well, now they have the
opportunity to correct a deficiency in their own understanding that they
weren’t previously aware of, which is also a great outcome. After all, Feynman
didn’t go in empty-handed - he took the challenging paper with him, and surely
referenced it often. But at the end, his own notes would record his own
complete derivation from start to finish and therefore serve as a testimonial
to his own understanding.</p>
</div>
<div id="ground-rules" class="section level2">
<h2>Ground Rules</h2>
<p>It was in the spirit of the above considerations that in the fall of 2018 I set
myself a goal: I would, over the course of the next year, derive and implement
a representative sample of fundamental models and algorithms from machine
learning, entirely from scratch and (insofar as was possible) entirely from my
own understanding. Where I found my understanding sufficient, this would be an
exercise in recreational programming; where my understanding failed me, it would
be a chance to shore up the foundations.</p>
<p>This is possibly less insane than it may appear. Although there are aspects of
machine learning that are <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory">very</a> technical, for the most part the
implementation of practical algorithms requires little more than some
moderately advanced statistics, a few semesters of linear algebra, some general
familiarity with numerical optimization and of course basic programming skills.</p>
<p>Because an open-ended project like this has a tendency to get out of control,
I also decided to set some ground rules to help keep things sane.</p>
<p>First, mathematical derivations are in scope. This usually means posing and
solving an optimization problem of some form, such as <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">MLE.</a> This is
straight-forward for most of the algorithms on my list but could get a little
hairy for things like backpropagation (which requires some fairly non-trivial
matrix calculus) or SVMs (which basically requires the entire theory of
<a href="https://en.wikipedia.org/wiki/Quadratic_programming">Quadradic Programming</a>). In practice, the presentation of these
derivations is bottlenecked by the necessity of typesetting the equations
in <span class="math inline">\(\LaTeX\)</span>, so these will typically be little more than sketches of the proofs.</p>
<p>Second, the algorithms used will be state-of-the-art, or at least reasonably
so. For example, while we <em>could</em> solve linear regression with gradient
descent, it would be a bit of a cop-out. Instead, we’ll implement what modern
statistical software actually does under the hood. One particular consequence
of this rule is that I will be implementing vectorized versions of the
algorithms whenever possible: while iterating over every example in the
training set is often easier to understand, it’s also pretty far removed from
the realities of modern implementations which rely heavily on vectorization or
even GPU acceleration for performance.</p>
<p>Third, I will implement and test all algorithms on some data set. As the Agile
crowd would say, working software is the primary measure of progress. For
convenience, I will use Python 3 and allow myself <code>numpy</code> arrays… but <em>not</em>
<code>numpy.linalg</code> or other high-level libraries like <code>scipy.optimize</code>; matrix
multiplication is about the most complex operation we’ll let the libraries do
for us. I considered not using <code>numpy</code> at all, but it allows us to express
algorithms in vectorized notation and frankly the algorithms are both
clearer and more realistic with it. This restriction only applies to the
implementation of the algorithm itself and excludes tests - I will routinely
use higher-level libraries (like <code>numpy.linalg</code>, <code>pandas</code>, <code>scipy</code>, or
<code>sklearn.datasets</code>) when <em>testing</em> the algorithm.</p>
<p>Fourth and finally, I’ll be publishing write-ups as I go. I’ve found in
practice this can be more time consuming than the original exercise. However
attempting to explain each algorithm in simple terms to a broad audience should
help me to understand them a little better as well.</p>
</div>
<div id="project-scope" class="section level2">
<h2>Project Scope</h2>
<p>While I want to touch on every aspect of machine learning, there’s little point
in implementing minor variations of basically the same algorithms over and
over. Instead, let’s pick one or two representative algorithms from each
category and leave it at that. We want to make sure that we get reasonable
coverage over the types of ML <em>problems</em> (supervised/unsupervised,
regression/classification, etc.), as well as good coverage over the most
important <em>algorithms</em> that crop up repeatedly in ML.</p>
<p>Here’s a tentative list of algorithms I would like to tackle:</p>
<table>
<colgroup>
<col width="29%" />
<col width="27%" />
<col width="36%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Problem</th>
<th align="center">Model</th>
<th align="center">Algorithm</th>
<th align="center">Article</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Regression</td>
<td align="center">Linear Regression</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/QR_decomposition">QR Decomposition</a></td>
<td align="center"><a href="/post/ml-from-scratch-part-1-linear-regression/">Part 1</a></td>
</tr>
<tr class="even">
<td align="center">Classification</td>
<td align="center">Logistic Regression</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a></td>
<td align="center"><a href="/post/ml-from-scratch-part-2-logistic-regression/">Part 2</a></td>
</tr>
<tr class="odd">
<td align="center">Classification</td>
<td align="center">Neural Network</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a></td>
<td align="center"><a href="/post/ml-from-scratch-part-3-backpropagation/">Part 3</a></td>
</tr>
<tr class="even">
<td align="center">Classification</td>
<td align="center">Decision Tree</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Recursive_partitioning">Recursive Partition</a></td>
<td align="center"><a href="/post/ml-from-scratch-part-4-decision-tree/">Part 4</a></td>
</tr>
<tr class="odd">
<td align="center">Clustering</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model">Gaussian Mixture Model</a></td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM Algorithm</a></td>
<td align="center"><a href="/post/ml-from-scratch-part-5-gmm/">Part 5</a></td>
</tr>
<tr class="even">
<td align="center">Dimension Reduction</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a></td>
<td align="center"><a href="https://en.wikipedia.org/wiki/QR_algorithm">QR Eigenvalue Algorithm</a></td>
<td align="center"><a href="/post/ml-from-scratch-part-6-pca/">Part 6</a></td>
</tr>
<tr class="odd">
<td align="center">Recommendation</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Low-rank_approximation">Low-Rank Matrix Approximation</a></td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Low-rank_approximation#Alternating_projections_algorithm">Alternating Projections</a></td>
<td align="center">TBD</td>
</tr>
<tr class="even">
<td align="center">Regression</td>
<td align="center">General Additive Models</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Backfitting_algorithm">Backfitting</a></td>
<td align="center">TBD</td>
</tr>
<tr class="odd">
<td align="center">Classification</td>
<td align="center">Support Vector Machines</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization">SMO Algorithm</a></td>
<td align="center">TBD</td>
</tr>
</tbody>
</table>
<p>Other candidates I considered but ultimately decided were out-of-scope:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Factor_analysis">Factor Analysis</a> - We already have PCA for dimensional reduction and GMM
as an example of using the EM algorithm to solve for latent random variables.</li>
<li>K-Means - We’ll do GMM instead, since k-means is just GMM with hard assignment.</li>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">K-Nearest Neighbors</a> - A naive algorithm is trivial while a serious
algorithm would mostly involve implementing a spatial index (such as
<a href="https://en.wikipedia.org/wiki/R-tree">R-Trees</a>) which takes us pretty far afield from learning algorithms.</li>
<li>Ensemble models - e.g. Random Forest or Boosted Trees. Not a good fit for the
“from scratch” approach and can best be understood as “composing” two or more
other mature models.</li>
<li><a href="https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464">CNN, RNN, etc.</a> - We’ll do the vanilla deep neural network from scratch
but more advanced topologies are best explored with a framework with
automated differentiation.</li>
<li><a href="https://en.wikipedia.org/wiki/Learning_to_rank">Learning-to-Rank</a> - e.g. <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry-Luce</a>, etc. These can
generally be reduced to logistic regression or viewed as latent variable
models and solved with the EM algorithm.</li>
<li><a href="https://courses.cs.washington.edu/courses/cse590q/04au/papers/WinklerEM.pdf">Felligi-Sunter Record Linkage</a> - another take on the EM algorithm, and requiring to many
prerequisites like <a href="https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance">Jaro-Winkler distance</a>.</li>
</ul>
</div>
<div id="bottom-up-approach-to-machine-learning" class="section level2">
<h2>Bottom-Up Approach to Machine Learning</h2>
<p>In the spirit of the Feynman technique, let’s spend a few minutes talking
through the problem in plain English and see if we can understand why machine
learning seems to focus so heavily on a few mathematical techniques and
approaches; this, in turn, should make it clear why it’s worth understanding
these techniques in depth.</p>
<p>The problem, in the broadest possible terms, is to get a computer to learn how
to do something. This is in contrast to traditional programming, where the
computer does not usually “learn” anything, but follows a program written by a
human programmer. Computers also aren’t very good at “doing” most things,
although they are very good (and very fast!) at the few things they <em>can</em> do.</p>
<p>So, what <em>are</em> computers good at? In decreasing order (increasing by the amount
of time it takes) computers can do the following:</p>
<ol style="list-style-type: decimal">
<li>Addition and subtraction<br />
</li>
<li>Multiplication</li>
<li>Division</li>
<li>Comparing two numbers to decide what to do next</li>
<li>Other math functions like <code>exp()</code>, <code>log()</code>, <code>sin()</code>, <code>cos()</code>, etc.</li>
<li>Remembering a billion numbers</li>
<li>Looking something up in a file or database</li>
<li>Talking to another computer over a network</li>
</ol>
<p>This fairly standard set of costs actually leads directly to some
important insights that guide research into practical machine learning.</p>
<p>First, we want to restrict ourselves as much as possible to simple arithmetic.
While we may occasionally allow ourselves a division or even, <em>gasp</em>, an
exponentiation, we really want to stick to fast operations like addition,
multiplication, taking the greater of two numbers with <code>max(a,b)</code>, or taking the sign of a number with <code>sign(a)</code>.</p>
<p>Second, any “learning” we do should be in the form of updating a
structured set of numbers. We call these the “parameters” to distinguished them
from the “data.” The parameters may be shaped like a vector, a matrix, or a
tree, but if we want to combine parameters and data with simple arithmetic, then
both must ultimately be represented as data structures with numeric values.</p>
<p>On the other hand, we want to avoid representing learning as a formatted string
or program. For example, the internal state of our learning algorithm could
literally be a a string describing a C program:</p>
<pre><code>float f(float* x) { 
    float z = 42; 
    if ( x[0] &lt; 5 &amp;&amp; x[1] &gt; 2 ) z -= 10; 
    if ( x[2] &gt; 7 || x[5] == 2 ) z += 3; 
    for ( int i=6; i&lt;11; i++ ) { z += x[i]; }
    if ( x[1] == 1 ) {
        for ( int i=3; i&lt;5; i++ ) { z -= 2 * x[i]; }
    }
    return z;
}</code></pre>
<p>To apply this to data, we would compile this C program and pass our data
into the function <code>f()</code>. To “learn”, the algorithm would add, remove, and
modify individual lines, characters, or perhaps syntactic statements or
expressions. This is sometimes called <a href="https://en.wikipedia.org/wiki/Genetic_programming">genetic programming</a>.
To be 100% clear, this is only bad if we allow <em>arbitrary</em> programs
involving <code>AND</code>, <code>OR</code>, <code>NOT</code>, <code>if/else</code>, <code>while</code>, <code>for</code>, intermediate
variables, and the like. Genetic programming can work well if the “genes” of
the program are very carefully designed. Indeed, it is sometimes used as
the “top level” learning algorithm in so-called automated machine learning
frameworks such as <a href="https://automl.info/tpot/">TPOT</a>. However, for the kind of fitting and optimization
we’re mainly interested in, genetic algorithms are hopelessly inefficient.</p>
<p>Why is learning an arbitrary program problematic? Does it simply not work?
Surely any equation we write down could also be represented in a more general
form as a program, and surely we could find that program by exhaustive
breadth-first search if necessary. And isn’t it also true that every program
has a <a href="https://en.wikipedia.org/wiki/G%C3%B6del_numbering">Gödel number</a>? So how is this fundamentally any different than
learning a set of numbers?</p>
<p>The problem isn’t that it doesn’t work, or that there’s anything wrong with
that approach in theory. The problem is simply that the space of programs we
would need to search is extremely large (the number of legal programs grows
exponentially with the length of the program with very high fan-out), and it is
exceedingly difficult to know if we’re getting “closer” to the right answer or
not. That’s a bad combination and means that “sufficient time” is often a lot
longer than we’re willing to wait.</p>
<p>To illustrate that second point, consider this (correct) program which finds
the greatest common divisor of a pair of numbers:</p>
<pre><code>def gcd(x, y):
    while y != 0:
        (x, y) = (y, x % y)
    return x</code></pre>
<p>Let’s say that the <code>"def gcd(x, y)"</code> is fixed as part of the problem
specification. Then there’s literally not a single character, word, or symbol
we could change in the body of that function which would not make it incorrect.
If I change <code>%</code> to <code>*</code>, it doesn’t terminate, if I change <code>y != 0</code> to <code>y != 1</code>
it’s so completely wrong it can never return a correct solution even by
accident, and so on. Therefore, in the space of all possible programs, this
correct program is surrounded on all sides by wildly incorrect programs. That
means that a greedy or even an evolutionary algorithm is unlikely to find this
elegant program. It <em>is</em> possible to find it via exhaustive breadth first search
(where depth is the length of the program) but this is brute force and hard to
scale.</p>
<p>So, in practical machine learning, we do not try to learn arbitrary <em>programs</em>,
we learn <em>parameters</em> for functions from some <em>family</em> of functions. For
example, let’s say a data point is represented as the vector <span class="math inline">\(\vec{x} \in \mathbb{R}^n\)</span> and our parameters are the vector <span class="math inline">\(\vec{p} \in \mathbb{R}^n\)</span>.
Then, keeping in mind that we mostly want to stick to arithmetic, the simplest
thing we could do is a dot product between these two vectors:</p>
<p><span class="math display">\[ f(\vec{x} ; \vec{p} ) = \vec{x} \cdot \vec{p} = \sum_{i=1}^n x_i p_i \]</span></p>
<p>That looks too simple to work, but in fact we’ll see in the <a href="/post/ml-from-scratch-part-1-linear-regression/">next article in
this series</a>, that it works surprisingly well for a very large class of
problems. Not for all problems of course; and throughout the series we will
gradually add complexity to the representation. This will, in turn, create
problems for us in terms of fitting/training these more complex models. In
parallel, we will develop ever more powerful techniques to deal with these
problems as they arise. In particular we will see again and again how a well
chosen representation will allow us to find very fast algorithms for learning
optimal parameters.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Next time, we’ll start with <a href="/post/ml-from-scratch-part-1-linear-regression/">linear regression</a>, followed by <a href="/post/ml-from-scratch-part-2-logistic-regression/">logistic
regression</a> and some simple <a href="/post/ml-from-scratch-part-3-backpropagation/">neural networks</a>. As new articles
are added, you can find them collected under the <a href="/tags/from-scratch/">“from scratch”</a> tag.</p>
</div>

    </article>

    <hr>


    <ul class="pager article-pager">
      <li class="pager-newer">
          <a href="/post/ml-from-scratch-part-1-linear-regression/" data-toggle="tooltip" data-placement="top" title="ML From Scratch, Part 1: Linear Regression">&lt; Newer</a>
      </li>
      <li class="pager-older">
        <a href="/post/viz-tsne/" data-toggle="tooltip" data-placement="top" title="Visualizing Multiclass Classification Results">Older &gt;</a>
      </li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2024 Oran Looney
  </div>
  <ul class="site-footer-items">
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
