[{"content":" Today is the last day when the number of people alive will start with a seven. Sometime late Tuesday afternoon, or perhaps early Wednesday morning, the population will cross the eight billion mark. When I was a kid, the number they taught us in school was five billion. By the time I was in college, it was up to six, and a decade ago it hit seven.\nNow it\u0026rsquo;s eight. Is this just a factoid, a little piece of trivia we have to keep updating so we can win pub quizzes?\nI don\u0026rsquo;t think so. Oh, the specific number is arbitrary enough. But the trend, the larger pattern \u0026mdash; that\u0026rsquo;s important. Take a look at this graph:\n\nFor most of human history, humans could by counted in the millions, not billions. We hit one billion around 1800 have been growing exponentially ever since. I haven\u0026rsquo;t been around quite long enough to see it double, but my parents have. But here\u0026rsquo;s one interesting thing: it\u0026rsquo;s not likely to double again, not unless something drastically changes. The growth rate is slowing, the population curve is flattening out, and current projections have the population stable at around 11 Billion by the year 2050. Which means this past century may be completely unique - the only time in all of history when six billion new humans were added in a single century.\nEcology Analogy In ecology, they sometimes draw this sigmoid growth curve and divide it into phases:\nIn a typical ecological model the carrying capacity is determined by food supply, but in the case of humans it seems to be driven by the demographic transition but some ideas still carry over. There was a period of exponential growth where there always more young people around than old people. Resources seemed unlimited and growth was unchecked. The population pyramid is very wide at the bottom.\nHowever, as we approach the plataeu, everything changes. Growth slows and eventually stablizes. Competition over scarce resources increases. The population pyramid narrows and there relatively fewer young people around.\nJapan provides a sneak peak into what we\u0026rsquo;re likely to expect globally. In Japan, their growth rate has already become negative, resulting in a large elderly population that is straining their social support systems and weakening the economy.\nRegime Shift Here\u0026rsquo;s a little piece of folk wisdom from my days as physics student: \u0026ldquo;If any quantity changes by more than an order of magnitude, double check all your approximations. They may no longer be valid.\u0026rdquo;\nFor example, you\u0026rsquo;ve probably heard of turbulent and laminar flow.\n  Both are fluid flows ultimately based on the same equations. There\u0026rsquo;s no hard cut-off between the two. But they behave extremely differently; so much so that that its easier to try to understand them as two separate phenomena.\nChanges in scale often result in this kind of so-called regime shift.\n \u0026ldquo;Building a four-foot tower requires a steady hand, a level surface, and 10 undamaged beer cans. Building a 400 foot tower doesn\u0026rsquo;t merely require 100 times as many beer cans.\u0026rdquo; - Steve McConnell\n We can tell a similar story across a wide variety of problems. A human can run about 8 miles per hour. To go from 8 mph to 80, you don\u0026rsquo;t just \u0026ldquo;run harder.\u0026rdquo; You need to build an engine or jump off a cliff. To go from 80 to 800, you need a jet and an airframe specifically designed to break the sound barrier. At 8,000 mph the physics of airflow go through another qualitative shift as we enter the hypersonic regime. Not only do you now need a scramjet, you also need to start seriously worrying about how your going to get rid of all that heat. Each order of magnitude isn\u0026rsquo;t just harder, it\u0026rsquo;s completely and qualitatively different.\nRate of Innovation Population is not the only thing that\u0026rsquo;s been growing exponentially. In his 1975 book Science Since Babylon, Derek de Solla observed that the number of PhDs being issued was doubling roughly every 20 years. This fact is often communicated with the vivid expression, \u0026ldquo;90% of all scientists who have ever lived are alive today.\u0026rdquo;\nWhile some of this is driven by population growth, increased availablity of education also plays a role. As a result, this number is actually growing faster than population.\nAll those scientists and engineers building and figuring out stuff cause a lot of churn.\n In the whispering quiet of Heaven\u0026rsquo;s night you imagine you can hear the paradigms shatter, shards of theory tinkling into brilliant dust as the lifework of some corporate think tank is reduced to the tersest historical footnote\u0026hellip; - William Gibson, Hinterlands\n The first time I really felt this first-hand was working with JavaScript frameworks circa 2010. It was a wild time. Entire frameworks would come into existence, become de facto standards or best practices overnight, and be considered outdated and untouchable a year or two later.\nOh, you\u0026rsquo;re still generating HTML on the server? Haven\u0026rsquo;t you heard about AJAX and XHR? The X stands for XML which is going to be the Next Big Thing. Of course, you would never actually pass XML; that\u0026rsquo;s so last year. (Yeah, that includes XHTML; we\u0026rsquo;re moving forward with HTML5 because it turns out getting developers to consistently close their tags is really hard.) No, everyone is passing JSON to REST APIs now. Actually, use jQuery to do it for you. What, you\u0026rsquo;re still using jQuery? You gotta get up to speed with MVC frames like AngularJS. They\u0026rsquo;ll bind your data to HTML for you. Thank you for being an earlier adopter of AngularJS 1.0; please transition your project to AngularJS 2.0 where we\u0026rsquo;ve broken backwards compatibility every way we can think of, plus a couple new ways we invented just for this project! Actually, let\u0026rsquo;s all just use React. (How about Vue? Oh, I\u0026rsquo;m fine; how are you?) Of course, you\u0026rsquo;d never actually write your own HTML/CSS; you\u0026rsquo;d use components and a CSS framework like Bootstrap. But obviously component libraries are rigid and inflexible and we should write our own \u0026ldquo;lightweight\u0026rdquo; HTML. Don\u0026rsquo;t forget to add responsive design! And support Retina! And touch events. Gotta bend over backwards to support IE6 - just kidding, it\u0026rsquo;s all Chrome now. Don\u0026rsquo;t use Python on the server \u0026mdash; use Node.js. Oh god NPM is so bad but never mind: gotta move fast, break things. Don\u0026rsquo;t use JavaScript \u0026mdash; use CoffeeScript or TypeScript. Actually, JavasScript is fine now. (Thanks ECMA!) Here\u0026rsquo;s one way to package JavaScript into modules. Here\u0026rsquo;s another, incompatible way. Maybe a third-party library can help unify the two? Oh, actually now there are three incompatible ways to package modules.\nMy suspicion is that kind of maelstrom is going to become increasingly common as the overall rate of innovation continues to increase. Workers in a variety of fields will have to find their own strategies for coping with this kind of constant, overwhelming flood of change. Programmers were able to adapt by moving to CI/CD workflows, but programmers have always been very good at writing their own tools to meet their own needs. Other fields and industries won\u0026rsquo;t necessarily have that capability and it\u0026rsquo;s hard to see how that will play out.\nBefore and After If it\u0026rsquo;s true that the population does stabilize at around 11 billion, then I think future historians will draw a sigmoid population curve and divide history into three phases - the low population era ancient and medieval history, the transitional centuries, and steady state of what they will think of as \u0026ldquo;modernity.\u0026rdquo; We, of course, would be in the pre-modern transitional period, albeit at the tail end of that.\nWhat will that new world look like? Well, there are something we can say:\n   Category Transitional Era Future Steady State     Population Growth Exponential Flat   Population Pyramid Wide Base of Young People Narrow Pillar   Population Distribution Sparse Dense   Innovation Time Scale Decades Years   Natural Resources Plentiful Constrained    These are the first-order, easily predictable effects. But they already paint a picture of a very different world which will lead to second-order effects which are much harder to predict.\nTo take one trivial example, currently workers in many fields are expected to move up to management after they have a decade or two of experience. But that model only works if there\u0026rsquo;s a cheap and plentiful supply of young, inexperienced people. If the population pyramid drastically changes shape, that basic assumption could change and executing projects where all the real work is done by the least experienced people may no longer be the dominant strategy. Of course, cultural lag could mean it could take a long time for people to realize that even if it does come true, because concepts like hierarchy and expected career progression are baked into our culture at an almost subconscious level.\nThere\u0026rsquo;s never going to be a specific date we can point to and say, \u0026ldquo;there! that was the day the modern world began!\u0026rdquo; But eight billion people on November 15th, 2022 is as good as any a point as any other to start thinking seriously about the end of exponential population growth, the increasingly constrained resources of our planet, and the dizzying rate of change that will characterize the rest of our lives.\n","date":"November 14, 2022","href":"https://www.oranlooney.com/post/eight-billion/","thumbnail":"/post/eight-billion_files/lead.192x128.jpg","title":"Eight Billion People"},{"content":"Hi, I\u0026rsquo;m Oran Looney. I do math. I write programs. I science\u0026hellip; data? That doesn\u0026rsquo;t sound right. I -tican stats? No, that\u0026rsquo;s even worse. My ideal job title would be Senior Nematode Wrangler because many of the neural networks I train are roughly the same complexity as the C. elegans connectome. Sadly, this is not yet a recognized specialty within the broader field of machine learning.\nThe best way to get in touch with me is through email or to message me on LinkedIn.\nI hold master\u0026rsquo;s degrees in physics and math and have worked as an Interface Analyst, a Software Engineer, a Director of Software Development, a Software Architect, and a Data Scientist. These days I\u0026rsquo;m professionally interested in R, Python, data visualization, applied statistics, machine learning, and healthcare data management.\nOutside of work, I like puzzles: programming challenges such as Advent of Code or leetcode; traditional puzzles like the Moscow puzzles; puzzle games like euclidea or the Professor Layton games. I\u0026rsquo;ve also been thrilled by the recent wave of recreational mathematics and other educational content on Youtube and elsewhere: 3Blue1Brown, Ben Eater, Mathologer, Numberphile, Veritasium, back-pen/red-pen, or Dr.Peyam. I think this new wave of math and science popularizers are doing great work and I encourage you to check them out and support them if you can. Or if you\u0026rsquo;re just feeling charitable in general, consider supporting Wikipedia or helping hungry children in Wisconsin.\nThis site was built with blogdown, an R package that combines Pandoc with the static site generator Hugo. It makes extensive use of MathJax for formatting LaTeX equations. Articles that are mainly in R are authored in a Rmarkdown Notebook while articles that are mainly in Python are first authored in a JupyterLab Notebook and then ported to vanilla Markdown. Many of the photos come from Unsplash due to its good selection and permissive license. While the site is almost entirely static, it uses an Nginx server running on a tiny AWS EC2 instance, mainly for historical reasons. For HTTPS, it uses a free SSL cert from Let\u0026rsquo;s Encrypt.\n","date":"November 11, 2022","href":"https://www.oranlooney.com/about/","thumbnail":"","title":"About Me"},{"content":" Programming Writing Code \u0026ldquo;A procedure should fit on a page.\u0026rdquo; \u0026mdash;David Tribble\n\u0026ldquo;When in doubt, use brute force.\u0026rdquo; \u0026mdash;Ken Thompson\n\u0026ldquo;The most important single aspect of software development is to be clear about what you are trying to build.\u0026rdquo; \u0026mdash;Bjarne Stroustrup\n\u0026ldquo;The cardinal sin is to make a choice without knowing you are making one.\u0026rdquo; \u0026mdash;Jonathan Shewchuk\n\u0026ldquo;The cost of adding a feature isn\u0026rsquo;t just the time it takes to code it. The cost also includes the addition of an obstacle to future expansion\u0026hellip; the trick is to pick features that don\u0026rsquo;t fight each other.\u0026rdquo; \u0026mdash;John Carmack\n\u0026ldquo;The road to programming hell is paved with global variables.\u0026rdquo; \u0026mdash;Steve McConnell\n\u0026ldquo;The psychological profiling [of a programmer] is mostly the ability to shift levels of abstraction, from low level to high level. To see something in the small and to see something in the large.\u0026rdquo; \u0026mdash;Donald Knuth\n\u0026ldquo;The whole point of getting things done is knowing what to leave undone.\u0026rdquo; \u0026mdash;Oswalf Chambers\n\u0026ldquo;Be careful that victories do not carry the seeds of future defeats.\u0026rdquo; \u0026mdash;Ralph Stockman\nProcess \u0026ldquo;If it\u0026rsquo;s your decision, it\u0026rsquo;s design; if not, it\u0026rsquo;s a requirement.\u0026rdquo; \u0026mdash;Alistair Cockburn\n\u0026ldquo;Building a four-foot tower requires a steady hand, a level surface, and 10 undamaged beer cans. Building a 400 foot tower doesn\u0026rsquo;t merely require 100 times as many beer cans.\u0026rdquo; \u0026mdash;Steve McConnell\n\u0026ldquo;If we\u0026rsquo;d asked the customers what they wanted, they would have said, \u0026lsquo;faster horses.\u0026lsquo;\u0026rdquo; \u0026mdash;Henry Ford\n\u0026ldquo;No one has ever found a bug in a piece of vaporware.\u0026rdquo; \u0026mdash;Unknown\n\u0026ldquo;A program is a poem: you cannot write a poem without writing it. Yet people talk about programming as if it were a production process and measure \u0026lsquo;programmer productivity\u0026rsquo; in terms of \u0026lsquo;number of lines of code produced.\u0026rsquo; In doing so they book that number on the wrong side of the ledger: We should always refer to \u0026lsquo;the number of lines of code spent.\u0026rsquo;\u0026rdquo; \u0026mdash;Edsger Dijkstra\n\u0026ldquo;As a programmer, it\u0026rsquo;s your job to put yourself out-of-business. What you can do today can be automated tomorrow.\u0026rdquo; \u0026mdash;Douglas Mcilroy\n\u0026ldquo;Measuring programming progress by lines of code is like measuring aircraft building progress by weight.\u0026rdquo; \u0026mdash;Bill Gates\n\u0026ldquo;If you can\u0026rsquo;t write it down in English, you can\u0026rsquo;t code it.\u0026rdquo; \u0026mdash;Peter Halpern\n\u0026ldquo;Without requirements or design, programming is the art of adding bugs to an empty text file.\u0026rdquo; \u0026mdash;Louis Srygley\n\u0026ldquo;A specification, design, procedure, or test plan that will not fit on one page of 8.5-by-11 inch paper cannot be understood.\u0026rdquo; \u0026mdash;Mark Ardis\n\u0026ldquo;The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.\u0026rdquo; \u0026mdash;Tom Cargill\n\u0026ldquo;Any program is a model of a model within a theory of a model of an abstraction of some portion of the world or of some universe of discourse.\u0026rdquo; \u0026mdash;Meir M. Lehman\n\u0026ldquo;Less than 10 percent of the code has to do with the ostensible purpose of the system; the rest deals with input-output, data validation, data structure maintenance, and other housekeeping.\u0026rdquo; \u0026mdash;May Shaw\n\u0026ldquo;[Thompson\u0026rsquo;s rule for first-time telescope makers] It is faster to make a four-inch mirror then a six-inch mirror than to make a six-inch mirror.\u0026rdquo; \u0026mdash;Bill McKeeman\n\u0026ldquo;Build one to throw away - you will anyway.\u0026rdquo; \u0026mdash;George Stocker\n\u0026ldquo;People don\u0026rsquo;t want to buy a quarter-inch drill, they want a quarter-inch hole.\u0026rdquo; \u0026mdash;Theodore Levitt\n\u0026ldquo;We build our computer [systems] the way we build our cities: over time, without a plan, on top of ruins.\u0026rdquo; \u0026mdash;Ellen Ullman\n\u0026ldquo;Every great developer you know got there by solving problems they were unqualified to solve until they actually did it.\u0026rdquo; \u0026mdash;Patrick McKenzie\n\u0026ldquo;With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody.\u0026rdquo; \u0026mdash;Hyrum\u0026rsquo;s Law\n\u0026ldquo;[Chesterton\u0026rsquo;s Fence] If you don\u0026rsquo;t see the use of it, I certainly won\u0026rsquo;t let you clear it away. Go away and think. Then, when you can come back and tell me that you do see the use of it, I may allow you to destroy it.\u0026rdquo; \u0026mdash;G. K. Chesterton\nBugs \u0026ldquo;The first step in fixing a broken program is getting it to fail repeatably.\u0026rdquo; \u0026mdash;Tom Duff\n\u0026ldquo;Finding your bug is a process of confirming the many things that you believe are true - until you find one which is not true.\u0026rdquo; \u0026mdash;Norm Matloff\n\u0026ldquo;Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.\u0026rdquo; \u0026mdash;Brian Kernigham\n\u0026ldquo;The most effective debugging tool is still careful thought, coupled with judiciously placed print statements.\u0026rdquo; \u0026mdash;Brian Kernigham\n\u0026ldquo;Never test [at runtime] for an error condition you don\u0026rsquo;t know how to handle.\u0026rdquo; \u0026mdash;Steinbach\n\u0026ldquo;Each new user of a new system uncovers a new class of bugs.\u0026rdquo; \u0026mdash;Brian Kernighan\n\u0026ldquo;In our business, one in a million is next Tuesday.\u0026rdquo; \u0026mdash;Gordon Letwin\nSimplicity \u0026ldquo;Controlling complexity is the essence of computer programming.\u0026rdquo; \u0026mdash;Brian Kernigham\n\u0026ldquo;If people do not believe that mathematics is simple, it is only because they do not realize how complicated life is.\u0026rdquo; \u0026mdash;John von Neumann\n\u0026ldquo;A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.\u0026rdquo; \u0026mdash;John Gall\n\u0026ldquo;Simplicity is prerequisite for reliability.\u0026rdquo; \u0026mdash;Edsger Dijkstra\n\u0026ldquo;The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise.\u0026rdquo; \u0026mdash;Edsger Dijkstra\n\u0026ldquo;[\u0026hellip;] but there is one quality that cannot be purchased that way - and that is reliability. The price of reliability is the pursuit of the utmost simplicity. It is a price which the very rich find most hard to pay.\u0026rdquo; \u0026mdash;C.A.R. Hoare\n\u0026ldquo;Inside every large program is a small program trying to get out.\u0026rdquo; \u0026mdash;Tony Hoare\n\u0026ldquo;UNIX is simple. It just takes a genius to understand its simplicity.\u0026rdquo; \u0026mdash;Dennis Ritchie\n\u0026ldquo;Complexity kills. It sucks the life out of developers, it makes products difficult to plan, build, and test, it introduces security challenges and it causes end-users and administrators frustration.\u0026rdquo; \u0026mdash;Ray Ozzie\n\u0026ldquo;There are two ways of constructing software. One way is to make it so simple that there are obviously no deficiencies. The other is to make it so complex that there are no obvious deficiencies.\u0026rdquo; \u0026mdash;C.A.R. Hoare\n\u0026ldquo;The purpose of software engineering is to control complexity, not to create it.\u0026rdquo; \u0026mdash;Pamela Zave\n\u0026ldquo;The key to understanding complicated things is to know what not to look at.\u0026rdquo; \u0026mdash;Harold Abelson\n\u0026ldquo;The ability to simplify means to eliminate the unnecessary so that the necessary may speak.\u0026rdquo; \u0026mdash;Hans Hoffman\n\u0026ldquo;Any intelligent fool can make things bigger, more complex, more violent. It takes a touch of genius - and a lot of courage - to move in the opposite direction.\u0026rdquo; \u0026mdash;Albert Einstein\n\u0026ldquo;Such is modern computing: everything simple is made too complicated because it\u0026rsquo;s easy to fiddle with: everything complicated stays complicated because it is hard to fix.\u0026rdquo; \u0026mdash;Rob Pike\n\u0026ldquo;Simplicity is hard to build, easy to use, and hard to charge for. Complexity is easy to build, hard to use, and easy to charge for.\u0026rdquo; \u0026mdash;Chris Sacca\n\u0026ldquo;Knowledge is a process of piling up facts. Wisdom lies in simplification.\u0026rdquo; \u0026mdash;Martin Luther King, Jr.\nOptimization \u0026ldquo;[The First Rule of Program Optimization] Don\u0026rsquo;t do it.\u0026rdquo; \u0026ldquo;[The Second Rule of Program Optimization-For experts only] Don\u0026rsquo;t do it yet.\u0026rdquo; \u0026mdash;Michael A. Jackson\n\u0026ldquo;In non-I/O-bound programs, a few percent of the source code typically accounts for over half the run time.\u0026rdquo; \u0026mdash;Donald Knuth\n\u0026ldquo;Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered.\u0026rdquo; \u0026mdash;Donald Knuth\n\u0026ldquo;The fastest I/O is no I/O.\u0026rdquo; \u0026mdash;Nil\u0026rsquo;s-Peter Nelson\n\u0026ldquo;The cheapest, fastest, and most reliable components of a computer system are those that aren\u0026rsquo;t there.\u0026rdquo; \u0026mdash;Gordon Bell\n\u0026ldquo;You know that algorithm that all the papers make fun of in their intro? Implement that and forget the rest of the paper.\u0026rdquo; \u0026mdash;Ian Wong\nScience Methodology \u0026ldquo;Knowledge itself is power.\u0026rdquo; \u0026mdash;Francis Bacon\n\u0026ldquo;Models should be as simple as possible, but not more so.\u0026rdquo; \u0026mdash;Attributed to Einstein\n\u0026ldquo;Measure what is measurable, and make measurable what is not so.\u0026rdquo; \u0026mdash;Galileo Galilei\n\u0026ldquo;Science is simply common sense at its best, that is, rigidly accurate in observation, and merciless to fallacy in logic.\u0026rdquo; \u0026mdash;Thomas Henry Huxley\n\u0026ldquo;The only relevant test of the validity of a hypothesis is comparison of its predictions with experience.\u0026rdquo; \u0026mdash;Milton Friedman\n\u0026ldquo;We try things. Occasionally they even work.\u0026rdquo; \u0026mdash;Rob Balder\n\u0026ldquo;Some people will never learn anything, for this reason, because they understand everything too soon.\u0026rdquo; \u0026mdash;Alexander Pope.\n\u0026ldquo;It is a capital mistake to theorize before one has data.\u0026rdquo; \u0026mdash;Sir Arthur Conan Doyle\n\u0026ldquo;Those who have taken upon them to lay down the law of nature as a thing already searched out and understood, whether they have spoken in simple assurance or professional affectation, have therein done philosophy and the sciences great injury.\u0026rdquo; \u0026mdash;Francis Bacon\n\u0026ldquo;Although this may seem a paradox, all exact science is dominated by the idea of approximation. When a man tells you that he knows the exact truth about anything, you are safe in inferring that he is an inexact man. Every careful measurement in science is always given with the probable error\u0026hellip; every observer admits that he is likely wrong, and knows about how much wrong he is likely to be.\u0026rdquo; \u0026mdash;Bertrand Russell\nStatistics \u0026ldquo;Statistics is the grammar of science.\u0026rdquo; \u0026mdash;Karl Pearson\n\u0026ldquo;All knowledge degenerates into probability.\u0026rdquo; \u0026mdash;David Hume\n\u0026ldquo;If a man will begin with certainties, he shall end in doubts; but if he will be content to begin with doubts he shall end in certainties.\u0026rdquo; \u0026mdash;Francis Bacon\n\u0026ldquo;There are three types of lies \u0026ndash; lies, damn lies, and statistics.\u0026rdquo; \u0026mdash;Benjamin Disraeli\n\u0026ldquo;If your experiment needs statistics, you ought to have done a better experiment.\u0026rdquo; \u0026mdash;Lord Ernest Rutherford\n\u0026ldquo;To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.\u0026rdquo; \u0026mdash;Ronald Fisher\n\u0026ldquo;The actual and physical conduct of an experiment must govern the statistical procedure of its interpretation.\u0026rdquo; \u0026mdash;Ronald Fisher\n\u0026ldquo;You can\u0026rsquo;t fix by analysis what you bungled by design.\u0026rdquo; \u0026mdash;Light, Singer and Willett\n\u0026ldquo;He uses statistics as a drunken man uses lamp-posts\u0026ndash;for support rather than illumination.\u0026rdquo; \u0026mdash;Andrew Lang\n\u0026ldquo;It is largely because of lack of knowledge of what statistics is that the person untrained in it trusts himself with a tool quite as dangerous as any he may pick out from the whole armamentarium of scientific methodology.\u0026rdquo; \u0026mdash;Edwin B. Wilson\nResearch \u0026ldquo;What I cannot create, I do not understand.\u0026rdquo; \u0026mdash;Richard Feynman\n\u0026ldquo;Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.\u0026rdquo; \u0026mdash;Richard Feynman\n\u0026ldquo;If you find that you\u0026rsquo;re spending almost all your time on theory, start turning some attention to practical things; it will improve your theories. If you find that you\u0026rsquo;re spending almost all your time on practice, start turning some attention to theoretical things; it will improve your practice.\u0026rdquo; \u0026mdash;Donald Knuth\n\u0026ldquo;It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used.\u0026rdquo; \u0026mdash;Gottfried Wilhelm Leibniz, 1685\n\u0026ldquo;Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question which can always be made precise.\u0026rdquo; \u0026mdash;John Tukey\n\u0026ldquo;Email is a wonderful thing for people whose role in life is to be on top of things. But not for me; my role is to be on the bottom of things. What I do takes long hours of studying and uninterruptible concentration.\u0026rdquo; \u0026mdash;Donald Knuth\nMachine Learning \u0026ldquo;People worry that computers will get too smart and take over the world, but the real problem is that they\u0026rsquo;re too stupid and they\u0026rsquo;ve already taken over the world.\u0026rdquo; \u0026mdash;Pedro Domingos\n\u0026ldquo;Programming, like all engineering, is a lot of work: we have to build everything from scratch. [Machine] Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops; [Machine] Learners combine knowledge with data to grow programs.\u0026rdquo; \u0026mdash;Pedro Domingos\n\u0026ldquo;As one Google Translate engineer put it, \u0026lsquo;when you go from 10,000 training examples to 10 billion training examples, it all starts to work. Data trumps everything.\u0026lsquo;\u0026rdquo; \u0026mdash;Garry Kasparov\n\u0026ldquo;A single neuron in the brain is an incredibly complex machine that even today we don\u0026rsquo;t understand. A single \u0026lsquo;neuron\u0026rsquo; in a neural network is an incredibly simple mathematical function that captures a minuscule fraction of the complexity of a biological neuron.\u0026rdquo; \u0026mdash;Andrew Ng\n\u0026ldquo;Coming up with features is difficult, time-consuming, and requires expert knowledge. \u0026lsquo;Applied machine learning\u0026rsquo; is basically feature engineering.\u0026rdquo; \u0026mdash;Andrew Ng\n\u0026ldquo;I once ran a small neural net 100 times on simple three-dimensional data re-selecting the initial weights to be small and random on each run. I found 32 distinct minima, each of which gave a different picture, and having about equal test set error.\u0026rdquo; \u0026mdash;Leo Breiman\n\u0026ldquo;The one nice thing about Random Forest is that is doesn\u0026rsquo;t overfit [as more trees are added]. You can’t have too many trees: it just stabilizes.\u0026rdquo; \u0026mdash;Trevor Hastie\nPhilosophy Truth \u0026ldquo;One should respect public opinion insofar as is necessary to avoid starvation and keep out of prison, but anything that goes beyond this is voluntary submission to an unnecessary tyranny.\u0026rdquo; \u0026mdash;Bertrand Russell\n\u0026ldquo;If it can be destroyed by the truth, it deserves to be destroyed by the truth.\u0026rdquo; \u0026mdash;Carl Sagan?\n\u0026ldquo;One of the saddest lessons of history is this: If we\u0026rsquo;ve been bamboozled long enough, we tend to reject any evidence of the bamboozle. We\u0026rsquo;re no longer interested in finding out the truth. The bamboozle has captured us. It\u0026rsquo;s simply too painful to acknowledge, even to ourselves, that we\u0026rsquo;ve been taken. Once you give a charlatan power over you, you almost never get it back.\u0026rdquo; \u0026mdash;Carl Sagan\n\u0026ldquo;Humankind cannot bear very much reality.\u0026rdquo; \u0026mdash;T. S. Eliot\n\u0026ldquo;Certain mystes aver that the real world has been constructed by the human mind, since our ways are governed by the artificial categories into which we place essentially undifferentiated things, things weaker than our words for them.\u0026rdquo; \u0026mdash;Gene Wolfe, Book of the New Sun\n\u0026ldquo;Men must be taught as if you taught them not / And things unknown proposed as things forgot.\u0026rdquo; \u0026mdash;Alexander Pope\n\u0026ldquo;Above all, don\u0026rsquo;t lie to yourself. The man who lies to himself and listens to his own lies comes to a point that he cannot distinguish the truth within him, or around him, and so loses all respect for himself and for others.\u0026rdquo; \u0026mdash;Fyodor Dostoyevsky\n\u0026ldquo;Some of the greatest discoveries consist mainly in the clearing away of psychological roadblocks which obstruct the approach to reality; which is why, post factum, they appear so obvious.\u0026rdquo; \u0026mdash;Arthur Koestler\nEthics \u0026ldquo;The only good is knowledge and the only evil is ignorance.\u0026rdquo; \u0026mdash;Socrates\n\u0026ldquo;Violence is the last refuge of the incompetent.\u0026rdquo; \u0026mdash;Isaac Asimov\n\u0026ldquo;Trust, but verify.\u0026rdquo; \u0026mdash;Russian Proverb\n\u0026ldquo;When you believe in things that you don\u0026rsquo;t understand, then you suffer: superstition ain\u0026rsquo;t the way.\u0026rdquo; \u0026mdash;Stevie Wonder\n\u0026ldquo;There is a cult of ignorance in the United States, and there always has been. The strain of anti-intellectualism has been a constant thread winding its way through our political and cultural life, nurtured by the false notion that democracy means that \u0026lsquo;my ignorance is just as good as your knowledge.\u0026rsquo;\u0026rdquo; \u0026mdash;Isaac Asimov\n\u0026ldquo;The world would be a much simpler place if one could bring about social change merely by making a logically consistent moral argument.\u0026rdquo; \u0026mdash;Peter Singer\n\u0026ldquo;Any sufficiently crappy research is indistinguishable from fraud.\u0026rdquo; \u0026mdash;Andrew Gelman\nHumor Jokes \u0026ldquo;Should array indices start at 0 or 1? My compromise of 0.5 was rejected without, I thought, proper consideration.\u0026rdquo; \u0026mdash;Stan Kelly-Bootle\n\u0026ldquo;It ain\u0026rsquo;t what you don\u0026rsquo;t know that gets you in trouble. It\u0026rsquo;s what you know for sure that just ain\u0026rsquo;t so.\u0026rdquo; \u0026mdash;Josh Billings?\n\u0026ldquo;A statistician is a person who draws a mathematically precise line from an unwarranted assumption to a foregone conclusion.\u0026rdquo; \u0026mdash;Anonymous\n\u0026ldquo;If it works, it\u0026rsquo;s obsolete.\u0026rdquo; \u0026mdash;Marshall Mcluhan\n\u0026ldquo;Beware of bugs in the above code; I have only proved it correct, not tried it.\u0026rdquo; \u0026mdash;Donald Knuth\n\u0026ldquo;The essence of XML is this: the problem it solves is not hard, and it does not solve the problem well.\u0026rdquo; \u0026mdash;Phil Wadler\n\u0026ldquo;Nothing is more permanent than a temporary solution.\u0026rdquo; \u0026mdash;Russian Proverb\n\u0026ldquo;Eschew clever rules.\u0026rdquo; \u0026mdash;Joe Condon\n\u0026ldquo;The term \u0026lsquo;exponential\u0026rsquo; is used quadratically too often.\u0026rdquo; \u0026mdash;Geoffrey Hinton\n\u0026ldquo;Statistics means never having to say you\u0026rsquo;re certain.\u0026rdquo; \u0026mdash;Rob Hyndman\n\u0026ldquo;A mathematician is a device for turning coffee into theorems.\u0026rdquo; \u0026mdash;Paul Erdos\n\u0026ldquo;Corollary: A co-mathematician is a device for turning ffee into co-theorems.\u0026rdquo; \u0026mdash;Unknown\n\u0026ldquo;A statistician is someone who is good with numbers but lacks the personality to be an accountant.\u0026rdquo; \u0026mdash;Unknown\n\u0026ldquo;Anyone who considers arithmetical methods of producing random numbers is, of course, in a state of sin.\u0026rdquo; \u0026mdash;John Von Neumann\n\u0026ldquo;Learning to program teaches you how to think. Computer science is a liberal art.\u0026rdquo; \u0026mdash;Steve Jobs\n\u0026ldquo;You either believe in the law of the excluded middle, or you don\u0026rsquo;t.\u0026rdquo; \u0026mdash;Lew Lefton\n\u0026ldquo;Pointers are real. They’re what the hardware understands. Somebody has to deal with them. You can’t just place a LISP book on top of an x86 chip and hope that the hardware learns about lambda calculus by osmosis.\u0026rdquo; \u0026mdash;James Mickens\n\u0026ldquo;Math is all about nuance. For example, there\u0026rsquo;s a fine line between a numerator and a denominator.\u0026rdquo; \u0026mdash;Unknown\nPoems A Dozen, A Gross, A Score\n\\[ \\frac{12 + 144 + 20 + 3\\sqrt{4}}{7 + 5 \\times 11} = 9^2 \\] A dozen, a gross, plus a score\nPlus three times the square root of four\nDivided by seven\nPlus five times eleven\nIs nine squared (and not a bit more.) \u0026mdash;John Saxon\nWord Crunching\nI\nwrote\na poem\non a page\nbut then each line grew\nto the word sum of the previous two\nuntil I started to worry at all these words coming with such frequency\nbecause, as you can see, it can be easy to run out of space when a poem gets all Fibonacci sequency. \u0026mdash;Brian Bilston\n\nRSA Algorithm \\[ p, q \\in \\mathbb{P} \\] \\[ n = pq \\] \\[ \\phi = (p-1)(q-1) \\] \\[ \\gcd(e, \\phi) = 1 \\land d \\equiv e^{-1} (\\mathrm{mod} \\phi) \\Rightarrow c = m^e (\\mathrm{mod}\\ n) \\land m = c^d (\\mathrm{mod}\\ n) \\] Take two large prime numbers, $q$ and $p$. Find the product $n$, and the totient $\\phi$. If $e$ and $\\phi$ have GCD one and $d$ is $e$\u0026rsquo;s inverse, then you\u0026rsquo;re done! For sending $m$ raised to the $e$ reduced $\\mathrm{mod}\\ n$ gives secre-$c$! \u0026mdash;Daniel G.\n\nA Certain Definite Integral\n\\[ \\int_1^{\\sqrt[3]{3}} t^2dt \\cos(3\\pi/9) = \\log(\\sqrt[3]{e}) \\] The Integral $t$-squared $dt$ From one to the cube root of three Times the cosine Of three $\\pi$ over nine\nEquals log of the cube root of \u0026ldquo;e\u0026rdquo;. \u0026mdash;Unknown\n\nMnemonic For Calculating Sixteen\n\\[ ln(e^4)\\sqrt{1024} + 6 \\times 12 - 8 \\times 23 = 16 \\] The log of e to the four\nTimes the square root of one thousand twenty four\nAdding six dozens please\nMinus eight twenty-threes\nIs sixteen, case closed, shut the door. \u0026mdash;Unknown\n\nA Complete Circle\n\\[ e^{2 \\pi i} = 1 \\] We start with the constant called \u0026ldquo;pi\u0026rdquo;\nAnd then multiply by two i\nApply exponential\n(This step is essential)\nAnd one\u0026rsquo;s the result - who knows why! \u0026mdash;Dan Shved\n\nBertrand\u0026rsquo;s Postulate\nChebyshev said it, but I\u0026rsquo;ll say it again:\nthere\u0026rsquo;s always a prime between $n$ and $2n$. \u0026mdash;N. J. Fine\n","date":"November 11, 2022","href":"https://www.oranlooney.com/quotes/","thumbnail":"","title":"Quotes"},{"content":"In the previous article in this series we distinguished\rbetween two kinds of unsupervised learning (cluster analysis and dimensionality\rreduction) and discussed the former in some detail. In this installment we turn\rour attention to the later.\nIn dimensionality reduction we seek a function \\(f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m\\) where \\(n\\) is the dimension of the original data \\(\\mathbf{X}\\) and\r\\(m\\) is less than or equal to \\(n\\). That is, we want to map some high dimensional\rspace into some lower dimensional space. (Contrast this with the map into a\rfinite set sought by cluster analysis.)\nWe will focus on one technique in particular: Primary Component\rAnalysis, usually abbreviated PCA. We’ll derive PCA from first\rprinciples, implement a working version (writing all the linear algebra code\rfrom scratch), show an example of how PCA helps us visualize and gain insight\rinto a high dimensional data set, and end with a discussion a few more-or-less\rprincipled ways to choose how many dimensions to keep.\nWhat is PCA?\rPCA is a linear dimensionality reduction technique. Many non-linear\rdimensionality reduction techniques exist, but linear\rmethods are more mature, if more limited.\nLinearity does not suffice to fully specify the problem, however. Factor\rAnalysis also seeks a linear map, but takes a more statistical approach\rand reaches a slightly different solution in practice. Non-negative matrix\rfactorization seeks a linear map represented by a matrix with no negative\relements - a restriction which PCA does not have.\nI mention these other techniques to make the point that merely specifying that\r\\(f\\) should be a linear map underspecifies the problem, and we need to be\rcareful about what additional requirement we add if we’re going to end up with\rPCA instead of some other method.\nSurprisingly, there are actually at least three different ways of fully\rspecifying PCA, all of which seem very different at first but can be shown to\rbe mathematically equivalent:\nRequire the covariance matrix of the transformed data to be diagonal. This\ris equivalent to saying that the transformed data has no multicollinearity,\ror that all \\(m\\) features of the transformed data are uncorrelated.\rSeek a new basis for the data such that the first basis vector points in the\rdirection of maximum variation, or in other words is the “principle\rcomponent” of our data. Then require that the second basis vector points\ralso points in the direction of maximum variation in the plane orthogonal to\rthe first, and so on until a new orthonormal basis is constructed.\rSeek a new basis for the data such that when we reconstruct the original\rmatrix from only the \\(m\\) most significant components the reconstruction\rerror is minimized. Reconstruction error is usually defined as the\rFrobenius norm of the difference between the original and\rreconstructed matrix, but other definitions are possible.\r\rThat these very different motivations all lead to the same formal solution is\rreminiscent of the fact that the models of computation proposed independently\rby Turing, Church, and Gödel turned out to all be equivalent. Just as this\rtriple convergence led some to believe that the definition of computation was\rdiscovered rather than merely invented, the fact that PCA keeps popping up\rsuggests that it is in some fundamental way the “right” way to think about\rdimensionality reduction. Or maybe it just means mathematicians like to\ruse linear algebra whenever they can, because non-linear equations are so\rdifficult to solve. But I digress.\nUnder any of these three definitions, the linear map \\(f\\) that we seek will turn\rout to be represented by a unique* (*some terms and conditions apply)\rorthogonal matrix \\(Q\\).\nOrthogonal matrices are the generalization of the 3-dimensional concept of a\rrotation or reflection: in particular they always preserve both distance and\rangles. These are ideal properties for a transform to have because merely\rrotating an object or holding it up to a mirror never distorts it, but\rsimply gives us a different perspective on it.\nImagine, for illustration’s sake that you held in your hand an unfamiliar\robject made of some easily deformable material like clay or soft plastic. You\rmight turn it gently this way and that, peer at it from every angle, or hold it\rup to the mirror. But these delicate operations do no harm to the object -\rindeed, you don’t really need to touch the object at all! You’re merely\rchanging your own point of view. Not so if you had flattened or stretched or\rtwisted it; after these more violent operations any shape or pattern you\rperceive may well be the product of your own manipulations and not a true\rinsight into the nature of the original form.\nOrthogonal matrices are the safest and least distorting transformation that we\rcould apply, and by the same token the most conservative and cautious. These\rare powerful virtues in a technique intended to help understand and explore\rdata without misrepresenting it.\nNow, obviously we could rotate our data any which way to get a different\rpicture, but PCA does more: it rotates it so that in some sense in becomes\raligned to the axes - rather like straightening a picture hanging askew on the\rwall. This is the property of PCA that is makes it so desirable for exploratory\ranalysis.\nOrthogonal matrices are always square, so \\(Q\\) is an \\(m \\times m\\) matrix.\rBut multiplying by a square matrix doesn’t reduce the dimensionality at all!\rWhy is this considered a dimensionality reduction technique? Well, it turns\rout that once we’ve rotated our data so that it’s as wide as possible along\rthe first basis vector, that also means that it ends up as thin as possible\ralong the last few basis vectors. This only works if the original data really\rwere all quite close to some line or hyperplane, but with this assumption met\rwe can safely drop the least significant dimensions and retain only our\rprinciple components, thus reducing dimensionality while keeping most of\rthe information (variance) of the data. Of course, deciding exactly how many\rdimensions to drop/retain is a bit tricky, but we’ll come to that later.\nFor now, let’s explore the mathematics and show how PCA gives rise to a\runique solution subject to the above constraints.\n\rApproach #1: The Direction of Maximal Variation\rBefore we can define the direction of maximal variance, we must be\rclear about what we mean by variance in a given direction. First, let’s say\rthat \\(\\mathbf{x}\\) is an \\(n\\)-dimensional random vector. This represents the\rpopulation our data will be sampled from. Next, suppose you have some\rnon-random vector \\(\\mathbf{q} \\in \\mathbb{R}^n\\). Assuming this vector is\rnon-zero, it defines a line. What do mean by the phrase, “the variance of\r\\(\\mathbf{x}\\) in the direction of \\(\\mathbf{q}\\)?”\nThe natural thing to do is to project the \\(n\\)-dimensional random variable\ronto the line defined by \\(\\mathbf{q}\\). We can do this with a dot product\rwhich we will write as the matrix product \\(\\mathbf{q}^T \\mathbf{x}\\). This new\rquantity is clearly a scalar random variable, so we can apply the variance\roperator to get a scalar measure of variance.\n\\[ \\operatorname{Var}[ \\mathbf{q}^T \\mathbf{x} ] \\tag{1} \\]\nDoes this suffice to allow us to define a direction of maximal variation? Not\rquite. If we try to pose the naive optimization problem:\n\\[ \\underset{\\mathbf{q}}{\\operatorname{argmax}} \\operatorname{Var}[ \\mathbf{q}^T \\mathbf{x} ] \\tag{2} \\]\nWe can easily prove no solution exists. Proof: Assume that \\(\\mathbf{a}\\) is the maximum. There\ralways exists another vector \\(\\mathbf{b} = 2 \\mathbf{a}\\) which implies that:\n\\[\\operatorname{Var}[ \\mathbf{b}^T \\mathbf{x}] = 4 \\operatorname{Var}[ \\mathbf{a}^T \\mathbf{x} ] \u0026gt; \\operatorname{Var}[ \\mathbf{a}^T \\mathbf{x} ] \\tag{3}\\].\nWhich implies that \\(\\mathbf{a}\\) was not the maximum after all, which is absurd.\rQ.E.D. The upshot is that we must impose some additional constraint.\nRecall that a dot product is only a projection in a geometric sense if \\(\\mathbf{q}\\) is a\runit vector. Why don’t we impose the condition\n\\[ \\mathbf{q}^T \\mathbf{q} = 1 \\tag{4} \\]\nTo obtain the constrained optimization problem\n\\[ \\underset{\\mathbf{q}}{\\operatorname{argmax}} \\operatorname{Var}[ \\mathbf{q}^T \\mathbf{x} ] \\quad\\quad \\text{such that} \\, \\mathbf{q}^T \\mathbf{q} = 1 \\tag{5} \\]\nWell at least this has a solution, even if it isn’t immediately obvious how\rto solve it. We can’t simply set partial derivative with respect to\r\\(\\mathbf{q}\\) equal to zero; that KKT condition only applies in the absence of\ractive constraints. Earlier in this series, we’ve used techniques such as\rstochastic gradient descent to solve unconstrained optimization problems, but\rhow do we deal the constraint?\nAn Ode to Lagrange Multipliers\rHappily, a very general and powerful trick exists for translating constrained\roptimization problems into unconstrained ones: the method of Lagrange\rmultipliers. In 1780, Lagrange was studying the motion of\rconstrained physical systems. At the time, such problems were usually solved\rby finding a suitable set of generalized coordinates but this required a\rgreat deal of human ingenuity.\nAs a concrete illustration, consider a bead on a stiff, bent wire.\nThe bead moves in all three dimensions and therefore requires three coordinates\r\\(x\\), \\(y\\), and \\(z\\) to describe its position. However, it is also constrained to\rthe one dimensional path imposed by the shape of the wire so in theory its\rposition could be described by a single parameter \\(t\\) describing its position\ralong the bent path. However, for a very complex path this might be hard to do,\rand even harder to work with when calculating the momentum and energy necessary\rto describe the dynamics of the system.\nLagrange developed a method by which this could always be done in an\ressentially mechanical way, requiring no human insight. As just one example,\rthe technique is used in modern physics engines - as new contact points\rare added or removed as objects touch in different ways, the physics engine can\rdynamically add or remove constraints to model them without ever having to\rworry about finding an appropriate set of generalized coordinates.\nLagrange’s genius was to imagine the system could “slip” just ever so\rslightly out of its constraints and that the true solution would be the one\rthat minimized this virtual slippage. This could be elegantly handled by\rassociating an energy cost called ’virtual work\" that penalized the system\rproportional to the degree to which the constraints were violated. This trick\rreconceptualizes a hard constraint as just another parameter to optimize in an\runconstrained system! And surprisingly enough, it does not result in an\rapproximate solution that only sort of obeys the constraint but instead\r(assuming the constraint is physically possible and the resulting equations\rhave a closed form solution) gives an exact solution where the constraint is\rperfectly obeyed.\nIt’s easy to use too, at least in our simple case. We introduce the Lagrange\rmultiplier \\(\\lambda\\) and rewrite our optimization as follows:\n\\[ \\underset{\\mathbf{q} ,\\, \\lambda}{\\operatorname{argmax}} \\operatorname{Var}[ \\mathbf{q}^T \\mathbf{x} ] + \\lambda (1 - \\mathbf{q}^T \\mathbf{q}) \\tag{6} \\]\nWhy is this the same as the above? Let’s call the above function \\(f(\\mathbf{q}, \\lambda)\\) write down the KKT conditions:\n\\[ \\begin{align}\r\\nabla_\\mathbf{q} f \u0026amp; = 0 \\tag{7} \\\\\r\\frac{\\partial f}{\\partial \\lambda} \u0026amp; = 0 \\tag{8}\r\\end{align}\r\\]\nBut equation (8) is simply \\(\\mathbf{q}^T \\mathbf{q} - 1 = 0\\) which is simply\rour unit vector constraint… this guarantees that when we solve (7) and (8),\rthe constraint will be exactly satisfied and we’ll will also have found a\rsolution to (6). Such is the magic of Lagrange multipliers.\nBut if (8) is just our constraint in a fancy new dress, how have we progressed\rat all? Because (7) is now unconstrained (and therefore more tractable!)\n\rFinding the Direction of Maximal Variation\rSuppose the matrix \\(\\mathbf{X}\\) is an \\(N \\times n\\) matrix with \\(N\\) rows where\reach row vector is an independent realization of \\(\\mathbf{x}\\). Also assume\r(without loss of generality) that the mean of each column of \\(\\mathbf{X}\\) is\rzero. (This can always be accomplished by simply subtracting off a mean vector\r\\(\\mathbf{\\mu}\\) before applying PCA.)\nWe can estimate the covariance matrix of \\(\\mathbf{X}\\) as\n\\[ \\mathbf{C} = \\frac{\\mathbf{X}^T \\mathbf{X}}{n} \\tag{9} \\]\n\\[ f = \\mathbf{q}^T \\mathbf{C} \\mathbf{q} + \\lambda (1 - \\mathbf{q}^T \\mathbf{q}) \\tag{10} \\]\n\\[ \\nabla f = 2 \\mathbf{C} \\mathbf{q} - 2 \\lambda \\mathbf{q} \\tag{11} \\]\nDividing by two and moving each term to opposite sides of the equation, we get the\rfamiliar equation for the eigenproblem:\n\\[ \\mathbf{C} \\mathbf{q} = \\lambda \\mathbf{q} \\tag{12} \\]\nThis shows that every “direction of maximal variation” is in fact an eigenvector\rof the covariance matrix, and the variance in that direction is the corresponding\reigenvalue.\nBecause the covariance matrix \\(\\mathbf{C}\\) is real-valued, symmetric, and positive\rdefinite, we know that the eigenvalues will all be real-valued (as expected.)\nThus, to find the axes of maximal variation, it suffices to find the eigendecomposition\rof \\(\\mathbf{C}\\).\nDefine \\(\\mathbf{Q}\\) to be the \\(n \\times n\\) right eigenvalue matrix (meaning\reach column is an eigenvector) and \\(\\mathbf{\\Lambda}\\) is the diagonal \\(n \\times n\\)\rmatrix containing eigenvalues along the diagonal.\n\\[ \\mathbf{C} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T \\tag{13} \\]\nWe will discuss the algorithm necessary to compute \\(\\mathbf{Q}\\) and \\(\\mathbf{\\Lambda}\\)\rbelow, but first let’s discuss some alternative ways to motivate PCA.\n\r\rApproach #2: Diagonalizing the Covariance Matrix\rWe could have skipped the entire “direction of maximal variation” and Lagrange\rmultiplier argument if we had simply argued as follows: I want my features to\rbe uncorrelated. Which is to say, I want the covariance matrix of my data to be\rdiagonal. However, when I estimate the covariance matrix for my data in their\roriginal form, I see that the covariance matrix is not diagonal. That\rmeans my original features exhibit some multiple collinearity, which is bad. To\rfix this problem, I will transform my data in such a way as make the covariance\rmatrix diagonal. It is well-known that a matrix can be diagonalized by finding\rits eigendecomposition. Therefore, I need to find the eigendecomposition\r\\(\\mathbf{C} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T\\).\nThe resulting right eigenvector matrix \\(\\mathbf{Q}\\) can be applied to \\(X\\),\ryielding a new, transformed data set \\(\\mathbf{X}\u0026#39;\\).\n\\[ \\mathbf{X}\u0026#39; = \\mathbf{X} \\mathbf{Q} \\tag{14} \\]\nWhen we then estimate the empirical covariance of \\(\\mathbf{X}\u0026#39;\\), we find\n\\[ \\begin{align}\r\\mathbf{C}\u0026#39; \u0026amp; = \\frac{\\mathbf{X}\u0026#39;^T \\mathbf{X}\u0026#39;}{n} \\tag{15} \\\\\r\u0026amp; = \\frac{(\\mathbf{X}\\mathbf{Q})^T (\\mathbf{X}\\mathbf{Q})}{n} \\\\\r\u0026amp; = \\frac{\\mathbf{Q}^T \\mathbf{X}^T \\mathbf{X}\\mathbf{Q}}{n} \\\\\r\u0026amp; = \\mathbf{Q}^T \\Bigg( \\frac{\\mathbf{X}^T \\mathbf{X}}{n} \\Bigg) \\mathbf{Q} \\\\\r\u0026amp; = \\mathbf{Q}^T \\mathbf{C} \\mathbf{Q} \\\\\r\u0026amp; = \\mathbf{\\Lambda}\r\\end{align}\r\\]\nBecause \\(\\mathbf{\\Lambda}\\) is a diagonal matrix, we’ve shown that the empirical\rcovariance of our transformed data set is diagonal; which is to say, all of the\rfeatures of \\(X\u0026#39;\\) are independent.\nThis argument is much more brutish and perhaps too on the nose: we wanted a\rdiagonal covariance matrix, so we diagonalized it, anyone got a problem with\rthat? However, it has the advantage of requiring nothing more than linear\ralgebra: no statistics, multivariate calculus, or optimization theory here! All\rwe need is a common-sense attitude toward working with data, or what my boss\rsometimes calls “street statistics.”\nA Brief Aside About Loadings\rAt this point it may also be worth mentioning that multiplying by the left\reigenvector matrix \\(\\mathbf{Q}\\) is only one of two common ways to define the\rtransformed data \\(\\mathbf{X}\u0026#39;\\). Alternatively, we could have used the so-called\r“loadings” matrix, defined like so:\n\\[ \\mathbf{L} = \\mathbf{Q} \\sqrt{\\mathbf{\\Lambda}} \\tag{16} \\]\nThe square root of a matrix may seem strange to you, but recall that\r\\(\\mathbf{\\Lambda}\\) is diagonal, so this just means the element-wise square root\rof each eigenvalue. Using \\(\\mathbf{L}^{-1}\\) instead of \\(\\mathbf{Q}\\) to\rtransform from \\(\\mathbf{X}\\) to \\(\\mathbf{X}\u0026#39;\\) means the empirical covariance\rmatrix of the transformed data will be the identity matrix. This can be more\rintuitive in some cases. Also, many software packages report the loading matrix\rinstead of the eigenvectors on the basis that they are easier to interpret.\n\r\rApproach #3: Minimizing Reconstruction Error\rThe third and final way to motivate the mathematical formalism of PCA is to\rview it as a form of compression. Specifically, of all possible linear\rprojections from \\(n\\) to \\(m\\) dimensions, taking the first \\(k\\) components of the\rPCA transformation of \\(X\\) minimizes the reconstruction error. Reconstruction\rerror is usually defined as the Frobenius distance between the original and\rreconstructed matrix but interestingly enough the theorem holds for a few\rother metrics as well, suggesting it’s a deep property of PCA and not some\rquirk of the Frobenius norm.\nI won’t go through this derivation here - you can find a presentation\relsewhere if you’re interested in the details - but it’s an extremely\rpowerful point of view which goes straight to the heart of the dimensional\rreduction strategy. If we can reconstruct our original data almost exactly from\ronly a handful of components that lends strong support to the notion that any\rinteresting information about the image must have been contained in those few\rcomponents.\nConsider this series of images, taken from this article:\nHere, 512 dimensions was reduced to just 29, but the reconstructed image is still perfectly recognizable.\nWith three separate theoretical justifications under our belt - which is two too many to be honest -\rlet’s turn our attention to the concrete problem of implementing eigendecomposition from scratch.\n\rAlgorithm for Solving the Eigenproblem\rThe modern approach to implementing PCA is to find the Singular Value Decomposition of a matrix \\(A\\) which\ralmost immediately gives us the eigenvalues of and eigenvectors of \\(A^T A\\). The\rbest known SVD algorithm is the Golub-Reinsh Algorithm. This is an\riterative algorithm. Starting with \\(A_1 = A\\) we calculate \\(A_{k+1}\\) from \\(A_k\\)\runtil we achieve convergence.\nFor each step we first use Householder reflections to reduce the matrix\rto bidiagonal form \\(A_k\\), then use a QR decomposition of \\(X_k^T X_k\\) to\rset many of the off-diagonal elements to zero. The resulting matrix \\(A_{k+1}\\)\ris tridiagonal, but at each step the off-diagonal elements get smaller\rand smaller. This is very much like trying to get rid of all the air bubbles in\rwallpaper by flattening them with a stick, only to have new bubbles pop up.\rHowever, the off-diagonal elements introduced by the process are smaller on\raverage than the original and it can be proved to converge to zero even if they\rwill never be exactly zero. In practice this converge is extremely rapid for\rwell-conditioned matrices.\nThere is also a randomized algorithm due to Halko, Martinsson, and Tropp\rwhich can be much faster, especially when we only want to retain a small number\rof components. This is commonly used with very large sparse matrices.\nNormally I would tackle one of these “best practice” algorithms, but after\rstudying them I found them to be larger in scope than what I would want to\rtackle for one of these articles. Instead, I decided to implement an older but\rstill quite adequate eigenvalue algorithm: known as the QR algorithm.\rIn addition to being easy to understand and implement, it has the advantage\rthat we can use the QR decomposition function that we implemented in the\rearlier article on linear regression. It’s just as fast or faster than\rGolub-Reinsh; the disadvantage is that it is not as numerically stable\rparticularly for the smallest eigenvalues. Because in PCA we normally intend to\rdiscard these anyway, this is not such a bad deal!\nRecall from that previous article our implementations for Householder\rreflections and QR decompositions:\ndef householder_reflection(a, e):\r\u0026#39;\u0026#39;\u0026#39;\rGiven a vector a and a unit vector e,\r(where a is non-zero and not collinear with e)\rreturns an orthogonal matrix which maps a\rinto the line of e.\r\u0026#39;\u0026#39;\u0026#39;\rassert a.ndim == 1\rassert np.allclose(1, np.sum(e**2))\ru = a - np.sign(a[0]) * np.linalg.norm(a) * e v = u / np.linalg.norm(u)\rH = np.eye(len(a)) - 2 * np.outer(v, v)\rreturn H\rdef qr_decomposition(A):\r\u0026#39;\u0026#39;\u0026#39;\rGiven an n x m invertable matrix A, returns the pair:\rQ an orthogonal n x m matrix\rR an upper triangular m x m matrix\rsuch that QR = A.\r\u0026#39;\u0026#39;\u0026#39;\rn, m = A.shape\rassert n \u0026gt;= m\rQ = np.eye(n)\rR = A.copy()\rfor i in range(m - int(n==m)):\rr = R[i:, i]\rif np.allclose(r[1:], 0):\rcontinue\r# e is the i-th basis vector of the minor matrix.\re = np.zeros(n-i)\re[0] = 1 H = np.eye(n)\rH[i:, i:] = householder_reflection(r, e)\rQ = Q @ H.T\rR = H @ R\rreturn Q, R\rUsing these we can implement the QR algorithm in just a few lines of code.\nThe QR algorithm is iterative: at each step, we calculate \\(A_{k+1}\\) by taking\rthe QR decomposition of \\(A_{k}\\), reversing the order of Q and R, and\rmultiplying the matrices together. Each time we do this, the off-diagonals get\rsmaller.\ndef eigen_decomposition(A, max_iter=100):\rA_k = A\rQ_k = np.eye( A.shape[1] )\rfor k in range(max_iter):\rQ, R = qr_decomposition(A_k)\rQ_k = Q_k @ Q\rA_k = R @ Q\reigenvalues = np.diag(A_k)\reigenvectors = Q_k\rreturn eigenvalues, eigenvectors\r\rImplementing PCA\rWe made a number of simplifying assumptions in the above theory and now we have\rto pay with a corresponding amount of busywork to get our data into an\ridealized form. There are really two pieces of book-keeping to implement:\nWe need to ensure than the data are centered\rOptionally “whiten” the data so that each feature has unit variance\rput eigenvalues in descending order\r\rAside from these considerations, the entire fit() function is little more\rthan the handful of lines necessary to diagonalize the empirical covariance\rmatrix. All of the hard work is done inside eigen_decomposition().\nclass PCA:\rdef __init__(self, n_components=None, whiten=False):\rself.n_components = n_components\rself.whiten = bool(whiten)\rdef fit(self, X):\rn, m = X.shape\r# subtract off the mean to center the data.\rself.mu = X.mean(axis=0)\rX = X - self.mu\r# whiten if necessary\rif self.whiten:\rself.std = X.std(axis=0)\rX = X / self.std\r# Eigen Decomposition of the covariance matrix\rC = X.T @ X / (n-1)\rself.eigenvalues, self.eigenvectors = eigen_decomposition(C)\r# truncate the number of components if doing dimensionality reduction\rif self.n_components is not None:\rself.eigenvalues = self.eigenvalues[0:self.n_components]\rself.eigenvectors = self.eigenvectors[:, 0:self.n_components]\r# the QR algorithm tends to puts eigenvalues in descending order # but is not guarenteed to. To make sure, we use argsort.\rdescending_order = np.flip(np.argsort(self.eigenvalues))\rself.eigenvalues = self.eigenvalues[descending_order]\rself.eigenvectors = self.eigenvectors[:, descending_order]\rreturn self\rdef transform(self, X):\rX = X - self.mu\rif self.whiten:\rX = X / self.std\rreturn X @ self.eigenvectors\r@property\rdef proportion_variance_explained(self):\rreturn self.eigenvalues / np.sum(self.eigenvalues)\r\r\rWine Quality Example\rThe wine quality data set consists of 178 wines, each described in\rterms of 13 different objectively quantifiable chemical or optical properties such as the\rconcentration of alcohol or the hue and intensity of the color. Each has been\rassigned to one of three possible classes depending on a subjective judgement of\rquality.\nThirteen dimensions isn’t nearly as bad as the hundreds or thousands commonly encountered\rin machine learning, but still rather more than the handful we poor 3-dimensional creatures are\rcomfortable thinking about. We’d like to get that down to something manageable, certainly no more than three.\rSo some kind of dimensionality reduction is indicated.\nimport pandas as pd\rimport seaborn\rimport matplotlib.pyplot as plt\rfrom mpl_toolkits.mplot3d import Axes3D\rfrom sklearn.datasets import load_wine\rwine = load_wine()\rX = wine.data\rdf = pd.DataFrame(data=X, columns=wine.feature_names)\rdisplay(df.head().T)\rA sample of the first five wines in the dataset:\n\r\r\r#1\r\r#2\r\r#3\r\r#4\r\r#5\r\r\r\r\r\ralcohol\r\r14.23\r\r13.2\r\r13.16\r\r14.37\r\r13.24\r\r\r\rmalic_acid\r\r1.71\r\r1.78\r\r2.36\r\r1.95\r\r2.59\r\r\r\rash\r\r2.43\r\r2.14\r\r2.67\r\r2.5\r\r2.87\r\r\r\ralcalinity_of_ash\r\r15.6\r\r11.2\r\r18.6\r\r16.8\r\r21\r\r\r\rmagnesium\r\r127\r\r100\r\r101\r\r113\r\r118\r\r\r\rtotal_phenols\r\r2.8\r\r2.65\r\r2.8\r\r3.85\r\r2.8\r\r\r\rflavanoids\r\r3.06\r\r2.76\r\r3.24\r\r3.49\r\r2.69\r\r\r\rnonflavanoid_phenols\r\r0.28\r\r0.26\r\r0.3\r\r0.24\r\r0.39\r\r\r\rproanthocyanins\r\r2.29\r\r1.28\r\r2.81\r\r2.18\r\r1.82\r\r\r\rcolor_intensity\r\r5.64\r\r4.38\r\r5.68\r\r7.8\r\r4.32\r\r\r\rhue\r\r1.04\r\r1.05\r\r1.03\r\r0.86\r\r1.04\r\r\r\rod280/od315_of_diluted_wines\r\r3.92\r\r3.4\r\r3.17\r\r3.45\r\r2.93\r\r\r\rproline\r\r1065\r\r1050\r\r1185\r\r1480\r\r735\r\r\r\r\rThese data have two characteristics that we should consider carefully.\nFirst, we can see that the features of this dataset are not on the same scale;\rproline in particular is a thousand times greater than the others. That\rstrongly suggests that we should “whiten” the data (scale everything so that\reach feature has unit variance before applying PCA.) Here, just for the\rpurposes of this visualization, we will manually whiten the data.\nX_white = (X - X.mean(axis=0))/X.std(axis=0)\rC = X_white.T @ X_white / (X_white.shape[0] - 1)\rplt.figure(figsize=(6,6))\rplt.imshow(C, cmap=\u0026#39;binary\u0026#39;)\rplt.title(\u0026quot;Covariance Matrix of Wine Data\u0026quot;)\rplt.xticks(np.arange(0, 13, 1))\rplt.yticks(np.arange(0, 13, 1))\rplt.colorbar()\rSecond, it is clear from this plot that this dataset exhibits significant\rmulticollinearity. Every feature exhibiting high correlations with\rseveral others; no feature is truly independent. While that would be a bad\rthing for say, linear regression, it means these data are an ideal candidate\rfor PCA.\npca = PCA(whiten=True)\rpca.fit(X)\rX_prime = pca.transform(X)\rFrom the eigenvalues, we can see that the first few components explain most of\rthe variance:\npca.eigenvalues\rarray([4.732, 2.511, 1.454, 0.924, 0.858, 0.645, 0.554, 0.350, 0.291, 0.252, 0.227, 0.170, 0.104])\rThe raw eigenvectors are hard to interpret directly, but if you like you can\rread the columns (starting with the leftmost) and see which features are being\rrolled up into each component; for example, it seems that “flavanoids” and\r“phenols” (whatever those are) are major contributors to the first principle\rcomponent, among others, while “ash” contributes almost nothing to it.\npca.eigenvectors\rarray([[ 0.144, -0.484, -0.207, 0.018, 0.266, 0.214, -0.056, 0.396, -0.509, -0.212, 0.226, 0.266, -0.015],\r[-0.245, -0.225, 0.089, -0.537, -0.035, 0.537, 0.421, 0.066, 0.075, 0.309, -0.076, -0.122, -0.026],\r[-0.002, -0.316, 0.626, 0.214, 0.143, 0.154, -0.149, -0.17 , 0.308, 0.027, 0.499, 0.05 , 0.141],\r[-0.239, 0.011, 0.612, -0.061, -0.066, -0.101, -0.287, 0.428, -0.2 , -0.053, -0.479, 0.056, -0.092],\r[ 0.142, -0.3 , 0.131, 0.352, -0.727, 0.038, 0.323, -0.156, -0.271, -0.068, -0.071, -0.062, -0.057],\r[ 0.395, -0.065, 0.146, -0.198, 0.149, -0.084, -0.028, -0.406, -0.286, 0.32 , -0.304, 0.304, 0.464],\r[ 0.423, 0.003, 0.151, -0.152, 0.109, -0.019, -0.061, -0.187, -0.05 , 0.163, 0.026, 0.043, -0.832],\r[-0.299, -0.029, 0.17 , 0.203, 0.501, -0.259, 0.595, -0.233, -0.196, -0.216, -0.117, -0.042, -0.114],\r[ 0.313, -0.039, 0.149, -0.399, -0.137, -0.534, 0.372, 0.368, 0.209, -0.134, 0.237, 0.096, 0.117],\r[-0.089, -0.53 , -0.137, -0.066, 0.076, -0.419, -0.228, -0.034, -0.056, 0.291, -0.032, -0.604, 0.012],\r[ 0.297, 0.279, 0.085, 0.428, 0.173, 0.106, 0.232, 0.437, -0.086, 0.522, 0.048, -0.259, 0.09 ],\r[ 0.376, 0.164, 0.166, -0.184, 0.101, 0.266, -0.045, -0.078, -0.137, -0.524, -0.046, -0.601, 0.157],\r[ 0.287, -0.365, -0.127, 0.232, 0.158, 0.12 , 0.077, 0.12 , 0.576, -0.162, -0.539, 0.079, -0.014]])\rJust as a cross check, we can plot the covariance matrix of the transformed\rdata the same way we did the raw data above:\nplt.figure(figsize=(6,6))\rplt.imshow(C_prime, cmap=\u0026#39;binary\u0026#39;)\rplt.title(\u0026quot;Covariance Matrix of Transformed Data\u0026quot;)\rplt.xticks(np.arange(0, 13, 1))\rplt.yticks(np.arange(0, 13, 1))\rplt.colorbar()\rAnd we can see the expected structure: eigenvalues descending from 4.7 to 0.1\ralong the diagonal, and exactly 0 away from the diagonal.\n\rVisualizing the Components\rPCA was applied only to the 13 features; the partition into three quality\rclasses was not included. Still, we would like to know if the primary\rcomponents have something to say about these classes, so we will color code\rthem with red, green, and blue.\nLet’s start by visualizing only the first component as points along a line:\nplt.figure(figsize=(10, 4))\rfor c in np.unique(wine.target):\rcolor = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;][c]\rX_class = X_prime[wine.target == c]\rplt.scatter(X_class[:, 0], X_class[:, 1]*0, color=color, alpha=0.3)\rplt.title(\u0026quot;Primary Components of Wine Quality\u0026quot;)\rplt.xlabel(\u0026quot;PC1\u0026quot;)\rThis may seem a little silly, but in fact boiling it down to only a single\rcomponent is often the best option: if you can boil a phenomenon down to a\rsingle number in a way that captures the essence, that could be very useful and\rin some cases represents an important discovery. For example, the reason we can\rtalk about “mass” without clarifying “inertial mass” vs. “gravitational mass”\ris because we strongly believe those quantities are identical in all cases. Is\rit possible that wine is neither complex nor multidimensional, but simply\rexists along a spectrum from poor to good quality?\nHowever, in this case, it seems like the first principle component does not\rfully capture the concept of quality. Let’s try plotting the first two\rcomponents.\nplt.figure(figsize=(10, 8))\rfor c in np.unique(wine.target):\rcolor = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;][c]\rX_class = X_prime[wine.target == c]\rplt.scatter(X_class[:, 0], X_class[:, 1], color=color, alpha=0.6)\rplt.title(\u0026quot;Primary Components of Wine Quality\u0026quot;)\rplt.xlabel(\u0026quot;PC1\u0026quot;)\rplt.ylabel(\u0026quot;PC2\u0026quot;)\rNow we can see that each class corresponds to a well-localized cluster with\rlittle overlap. In many cases, where PC1 is right on the boundary between two\rclasses, it is PC2 that supplies the tiebreaker. Note that we could draw a\rsingle curved path that would connect these three regions in order or\rascending quality; this suggests that a non-linear dimensionality reduction\rtechnique, say some kind of manifold learning like t-SNE, might be\rable to reduce quality to a single dimension. But is that the representation it\rwould discover on its own, when trained in an unsupervised manner?\nIt may also be worth trying three dimensions, just in case PC3 has some\rnon-ignorable contribution to quality. Three dimensions is always a little hard\rto visualize with standard plotting tools, but we can use pairwise 2D plots:\nplt.figure(figsize=(16, 8))\rplt.subplot(1, 2, 1)\rfor c in np.unique(wine.target):\rcolor = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;][c]\rX_class = X_prime[wine.target == c]\rplt.scatter(X_class[:, 0], X_class[:, 2], color=color, alpha=0.6)\rplt.title(\u0026quot;Primary Components of Wine Quality\u0026quot;)\rplt.xlabel(\u0026quot;PC1\u0026quot;)\rplt.ylabel(\u0026quot;PC3\u0026quot;)\rplt.subplot(1, 2, 2)\rfor c in np.unique(wine.target):\rcolor = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;][c]\rX_class = X_prime[wine.target == c]\rplt.scatter(X_class[:, 1], X_class[:, 2], color=color, alpha=0.6)\rplt.title(\u0026quot;Primary Components of Wine Quality\u0026quot;)\rplt.xlabel(\u0026quot;PC2\u0026quot;)\rplt.ylabel(\u0026quot;PC3\u0026quot;)\rOr the kind of plot which is called 3D, although it’s really just\ra slightly more sophisticated projection on 2D:\nfig = plt.figure(figsize=(10, 8))\rax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;)\rax.view_init(15, -60)\rfor c in np.unique(wine.target):\rcolor = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;][c]\rX_class = X_prime[wine.target == c]\rax.scatter(X_class[:, 0], X_class[:, 1], X_class[:, 2], color=color, alpha=0.6)\r# chart junk\rplt.title(\u0026quot;First 3 Primary Components of Wine Quality\u0026quot;)\rax.set_xlabel(\u0026#39;PC1\u0026#39;)\rax.set_ylabel(\u0026#39;PC2\u0026#39;)\rax.set_zlabel(\u0026#39;PC3\u0026#39;)\rWhile these charts look attractive enough, they don’t seem to make a compelling\rcase for including PC3. At least for the purposes of understanding wine quality\rit seems we can retain just two principle components and still get a complete\rpicture.\nAt this point in a real analysis, we would spend some time understanding what\rPC1 and PC2 really represent, looking at which of the 13 original features\rcontribute to each, and whether positive or negative, and how this relates to\rquality. But there is an elephant in the room - the informal or seemingly ad\rhoc method I used for deciding to use two components instead of one or three.\rWhile “just look at the data and use the one that makes sense” has a certain\rpragmatic and commonsensical appeal, it’s also easy to see that it’s subjective\renough to allow bias to creep in. As such, many people have asked themselves if\rthere were not some rigorous decision rule that could be applied.\n\rStrategies for Choosing The Number of Dimensions\rThe oldest and most venerable method involves plotting the eigenvalues in\rdescending order as a function of dimension number: whimsically called a scree\rplot after a resemblance to the “elbow” that appears near the base of some\rmountain where loose stones are piled upon a slope:\nTo determine the number of components to retain, it is suggested to look for a\rvisual “elbow” point at which the chart noticeably flattens out and use that\rfor the cut-off.\nAlternatively, if a deterministic rule is required, one might use so-called the\rKaiser criterion : drop all components with an eigenvalue less than 1 and\rretain all those with an eigenvalue greater than 1.\nBefore discussing the merits or demerits of these approaches, let’s just create\rthe scree plot for the wine quality data set:\nfig = plt.figure(figsize=(10, 7))\rplt.title(\u0026quot;Scree Plot (Eigenvalues in Decreasing Order)\u0026quot;)\rplt.plot([1, 13], [1, 1], color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=\u0026quot;Kaiser Rule\u0026quot;)\rplt.xticks(np.arange(1, 14, 1))\rplt.xlim(1, 13)\rplt.ylim(0, 5)\rplt.ylabel(\u0026quot;Eigenvalue\u0026quot;)\rplt.xlabel(\u0026quot;Principle Component Index\u0026quot;)\rplt.grid(linestyle=\u0026#39;--\u0026#39;)\rplt.plot(range(1, 14), pca.eigenvalues, label=\u0026quot;Eigenvalues\u0026quot;)\rplt.legend()\rIn this case, no prominent elbow is apparent, at least to my eyes. This is\rdisappointing but not surprising: whenever I’ve tried to use this rule in my\rwork I’ve found it to be at least somewhat ambiguous - and liable to influenced\rby irrelevant things such as the aspect ratio of the chart!\nThe Kaiser criterion, on the other hand, is a complete train wreck\rand ultimately is no more likely to protect you from criticism than simply\rasserting that you used your best judgement.\nAnother approach is to decide before hand that you want to retain some fraction\rof the total variance, say 80% or 99%, and choose a number of components which\rgive you the desired fidelity. (This approach is particularly attractive if you\rhave the “compression” point-of-view in mind.) Although the proportion of\rvariance explained is calculated from the same eigenvalues used for the scree\rplot, the difference here is that we are now looking at the cumulative sum of\reigenvalues.\nfig = plt.figure(figsize=(10, 7))\rplt.title(\u0026quot;Variance Explained By Component\u0026quot;)\rplt.xticks(np.arange(1, 14, 1))\rplt.yticks(np.arange(0, 1.0001, 0.1))\rplt.xlim(1, 13)\rplt.ylim(0, 1)\rplt.ylabel(\u0026quot;Proportion of Variance Explained\u0026quot;)\rplt.xlabel(\u0026quot;Principle Component Index\u0026quot;)\rplt.grid(linestyle=\u0026#39;--\u0026#39;)\rplt.fill_between(\rrange(1, 14), np.cumsum(pca.proportion_variance_explained), 0, color=\u0026quot;lightblue\u0026quot;, label=\u0026quot;Cumulative\u0026quot;)\rplt.plot(\rrange(1, 14), np.cumsum(pca.proportion_variance_explained), 0, color=\u0026quot;darkblue\u0026quot;)\rplt.plot(\rrange(1, 14), pca.proportion_variance_explained, label=\u0026quot;Incremental\u0026quot;, color=\u0026quot;orange\u0026quot;, linestyle=\u0026quot;--\u0026quot;)\rplt.legend(loc=\u0026#39;upper left\u0026#39;)\rOne benefit of this approach is that it is much easier to explain; one does not\rneed to use term “eigen” at all! People familiar with ANOVA will be comfortable\rwith the concept of “proportion of variance explained” and this can even be\rglossed as “information” for audiences where even the word “variance” might be\ra little scary. “We used PCA to compress the data from 512 to 29 dimensions\rwhile retaining 95% of the information” may be criticized for not using the\rinformation theoretic definition of “information” but is clear enough to a\rbroad audience.\nStill, we haven’t really solved the issue of having to choose an arbitrary\rthreshold, have we? All we’ve done is couch the choice in terms of a more\rintuitive metric. I’m not sure any definitive and universally accepted answer\rexists - but the wonderfully named paper Repairing Tom Swift’s\rElectric Factor Analysis Machine suggests one method, and I’ve seen\rseveral references to this paper by Minka which may represent the\rcurrent state-of-the-art.\n\rConclusion\rPCA is the archetypical dimensionality reduction method; just as\r\\(k\\)-means is the archetypical clustering method. Now that we’ve\rimplemented both a dimensional reduction and a clustering method (in the last\rarticle) from scratch, we should have a pretty good handle on\rthe basics of unsupervised learning. In particular, we’ve seen many of the\rfrustration and limitations inherent in unsupervised methods, which boil down\rto the impossibility of objectively deciding in a given model is doing a good\rjob or a bad job. This in turn makes it next to impossible to decide between\rsimilar models, which tends to come down to a question of subjective judgement.\rUnfortunately, these models do have hyperparameters, so there are choices\rthat need to be made… but can any choice be defended to the satisfaction of a\rhostile (or who simply have extremely high standards) third-party?\nMore rewarding then wrestling with the ill-defined problems of unsupervised\rlearning was the implementation of a eigendecomposition algorithm in a way that\rmet the fairly stringent rules of the “from scratch” challenge.\n\r","date":"September 16, 2019","href":"https://www.oranlooney.com/post/ml-from-scratch-part-6-pca/","thumbnail":"/post/ml-from-scratch-part-6-pca_files/lead.192x128.jpg","title":"ML From Scratch, Part 6: Principal Component Analysis"},{"content":"I recently wrote an article which was ostensibly about the Fibonacci\rseries but was really about optimization techniques. I wanted to follow up on\rits (extremely moderate) success by going in the exact opposite direction:\rby writing a Fibonacci function which is as slow as possible.\nThis is not as easy as it sounds: any program can trivially be made slower,\rbut this is boring. How can we make it slow in a fair and interesting way? The\ranswer is to use a model of computation which is not deliberately\rdesigned to be slow but which in practice is quite slow, usually because the\rdesigner had something quite different than performance in mind.\nWhile there are several to choose from, I selected the\r\\(\\lambda\\)-calculus (read “lambda calculus”) as being particularly easy to\rwrite an spectacularly inefficient implementation in.\nSome of you no doubt will be having flashbacks at the mention of the name,\rwhile others have already started slowly edging their mouse towards the close\rtab icon. But don’t worry - if you done any programming before you’ve already\rseen all the “hard” ideas associated with it and what’s left is a “toy”\rlanguage that can be learned in a few minutes. By the end of this article you\rwill see clearly how you could write your own non-trivial programs directly in\rthe \\(\\lambda\\)-calculus.\nIn fact, it’s main problem is that it’s too simple: it is difficult at first\rto see how anyone could do anything with it. Luckily for us, there exists a\rset of macros which turn the \\(\\lambda\\)-calculus into a much higher level\rlanguage. While Alonzo Church was inventing the \\(\\lambda\\)-calculus itself\rhe also developed this set of macros in parallel so that he could convince\rhimself and others that it really could compute. These macros provide a simple\rand concrete way of encoding numbers, mathematical operators, boolean logic,\rand even data structures like lists, maps, and trees. Today, this technique is\rcalled Church encoding. If \\(\\lambda\\)-calculus is machine code, then the\rChurch encoding is C.\nGoal\rDavid Allen says to begin with the goal in mind, so let’s take a look at what\rwe’re shooting for. As a blueprint, let’s first look at a performance-naive\rimplementation of a function which finds the \\(n\\)-th Fibonacci number,\rimplemented in vanilla Python:\ndef fibonacci(n):\rif n \u0026lt;= 1:\rreturn n\relse:\rreturn naive_fib(n-1) + naive_fib(n-2)\rLet’s put together a shopping list of features we need to implement this function:\nnatural numbers\raddition\rsubtraction\rless than comparison\rif/else branch\rrecursion\r\rWell, we certainly have our work cut out for us, don’t we? The first four\rrequire a model of the Peano axioms, the if/else branch requires a model\rof Boolean algebra, and recursion is usually regarded as a\rnon-trivial language feature.\nFor the purposes of this article, we’re going to take a non-standard\rapproach and not use the original notation. Instead, we’ll use a subset of\rPython which has a one-to-one correspondence with the \\(\\lambda\\)-calculus but which\ris hopefully both more familiar and more accessible to readers: all of the code\rin this article will run in any Python 3 interpreter so that readers may follow\ralong and try their own experiments if they like. I shall call this subset the\r“Pythonic” \\(\\lambda\\)-calculus when I want to specifically refer to the lambda\rcalculus implemented as a subset of the Python grammar.\nIn the next section I will describe this subset more formally. In this section,\rI’ll just do a quick high-level overview so you have some idea of where we are\rheading.\nBasically, we’ll be writing Python code, but restricting ourselves to only\rusing the anonymous function syntax (e.g., lambda x: ...) and function calls\r(e.g., f(x)).\nIn addition, we will expand our language with definitions, which are\rbasically macros with a human readable name that expand out to\r\\(\\lambda\\)-calculus expressions. Here is an example of a definition, the\rcontents of which we will return to later:\nplus = lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x))\rDefinitions get substituted into other expressions very much like running “Find\rand Replace All”:\ns/plus/(lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)))/g\rBy introducing definitions, we will gradually build up a surprisingly\rhigh-level and expressive language on top of the \\(\\lambda\\)-calculus. We can\rthen revert to proper \\(\\lambda\\)-calculus at any time by simply expanding out\rall the definitions like macros.\nWith those preliminaries out of the way, let me show you are goal.\rAlthough we will the rest of this article to understand in detail how it\rworks, we will end up with a Fibonacci function that looks something like this:\nfibonacci = Y(lambda f: PIR(\r# define function f(n) as:\rlambda n: (\r# if n \u0026lt; 2\rless_than(n)(two))\r# then n\r(n)\r# else f(n-1) + f(n-2)\r(plus\r(f(minus(n)(one)))\r(f(minus(n)(two))))))\rAs always in Python, # indicates an non-executable comment.\nIt shouldn’t be to hard to see the basic shape of the familiar, friendly\rFibonacci function in there. If you have any LISP, it will even look vaguely\rfamiliar, although the parentheses are in slightly different places. (In LISP,\rfunction application looks like (minus n 2) instead of minus(n)(two).)\n\rThe Pythonic Lambda Calculus\rIn the \\(\\lambda\\)-calculus there are only two operations: abstraction and\rapplication. These two can be composed to write any program (computable\rfunction) that has or ever will be written.\nWhat do we mean by the term “abstraction?” Let’s say we have a concrete\rcalculation, such as the sums of squares of a vector like (3, 4). We could\rtype the following into a Python interpreter:\n3*3 + 4*4\rIn an interactive programming session, this might suffice: you type something\rin and hit ENTER to see the answer. But in most cases values 3 and 4 are too\rhard-coded to be useful. So we can abstract this by replacing 3 with a\rplaceholder variable x and introducing a function of x:\ndef half_ss(x):\rreturn x*x + 4*4\rNow that we have an abstraction, a.k.a. a function, how do we use it? Before\rwe can do any concrete computation, we need to know what x is supposed to be.\rThis is called function application or simply application, and we use the\rfamiliar f(x) syntax common to math and almost all programming languages.\nhalf_ss(3)\rBut this “half” function isn’t very satisfactory. The 4 is still hard coded.\nIf we want to do the same thing again and abstract 4 into y, we have a\rcouple of choices. We could greatly complicate our language and add another\rsyntactic primitive ,:\ndef half_ss(x, y):\rreturn x*x + y*y\rss(3, 4)\rBut if we want to keep things simple, why don’t we simply perform abstraction\rtwice?\ndef ss(x):\rdef half_ss(y):\rreturn x*x + y*y\rreturn half_ss\rNote that when we call ss(3), what is returned is again a function, so\rwe can use a second application to pass the second argument:\nss(3)(4)\rThe two applications “cancel out” the two abstractions, and we are left with\rthe concrete value 3*3 + 4*4, more commonly known as 25.\nThis works because we call ss with the first argument and instead of resolving\rto a value, it instead returns a function that we can call with the second argument\rto get the result. We can repeat this operation as many times as necessary, but\rit get’s inconvenient to make up a silly name like half_ss each time; instead\rwe’ll use an anonymous lambda function:\nss = lambda x: lambda y: x*x + y*y\rss(3)(4)\rSo to recap, we implement the abstraction operation of \\(\\lambda\\)-calculus in\rPython by using a lambda expressions with a single argument each, and if we\rwant to define a function that takes more than one argument we stack up lambda\rexpressions like lambda x: lambda y: lambda z :... .\nNote that we can write our entire program without recourse to any names if\rwe treat ss as a definition and replace the string ss with its right-hand\rside where ever it appears. The final program is:\n(lambda x: lambda y: x*x + y*y)(3)(4)\rWhich you can run and verify that it gives the answer 25.\nIn the \\(\\lambda\\)-calculus, application is the only way we can do anything, so\rwe should re-write the binary expressions as more primitive functions:\n(lambda x: lambda y: plus(mult(x)(x))(mult(y)(y)))(3)(4)\rOne specific trick deserves attention. We could define a constant function\rlike so:\nlambda x: c\rWhen called, this function ignores the argument x and always returns the\rconstant c. This expression however, has c as a free variable: there is\rno lambda c: above it. We can remedy that with another abstraction:\nlambda c: lambda x: c\rThis gives us a way to construct constant functions out of any value. This kind\rof thing is called a closure. Closures have the property that they can\r“remember” values assigned to them at runtime. In fact, the function ss\rdefined above also returned a closure, which “remembered” that x was supposed\rto equal 3 until the time came when a concrete calculation could be carried\rout. Closures are extremely important in the \\(\\lambda\\)-calculus because they are\rour only way of defining data structures and passing state around.\nSetting aside closures, let’s next discuss how computation is actually carried\rout. You may well have noticed that the operation I’ve called “application”\rshows the syntax that says a function should be called with a certain\rargument, but I haven’t said anything about how the calling of a function\rshould actually be carried out!\nIt turns out this is exactly the reverse of abstraction - we replace abstract\rvariables with concrete values. This is called the \\(\\beta\\)-reduction rule (read\r“beta reduction”.) For example, we may call the function lambda x: x(x)(x)\rwith concrete value t by writing (lambda x: x(x)(x))(t). The\r\\(\\beta\\)-reduction rules allows us to remove the lambda x and go through and\rreplace every x in the body of the function with t, which leaves us with\rt(t)(t). Since t is a free variable, we cannot reduce this any further, so\rwe stop. Let’s do the same thing to our sums-of-squares example:\n(lambda x: lambda y: plus(mult(x)(x))(mult(y)(y)))(3)(4)\r(lambda y: plus(mult(3)(3))(mult(y)(y)))(4)\rplus(mult(3)(3))(mult(4)(4))\rSince plus, mult, and even 3 and 4 are just definitions, we could\rexpand those definitions and continue the beta-reduction process until only a\rsingle concrete value remains. Below, we will study the Church encoding for\rnatural numbers and learn exactly how to do this.\nSo abstraction introduces lambda expressions, application tells us when we\rshould call a function and which argument to pass in, and \\(\\beta\\)-reduction\rtells us how to carry out that function call.\nThere is one other rule, called \\(\\alpha\\)-replacement (read “alpha\rreplacement”), which tells us that we can change the variable names of\rfunctions whenever we want, as long as we are consistent and change it\reverywhere inside the body of the function too while also avoiding conflicts\rwith other variable names. For example, these two lambda expressions are the\rsame, and we can use the \\(\\alpha\\)-replacement rule to transform one into the\rother and vice versa:\nlambda x: x === lambda y: y\rlambda x: x !== lambda x: y\rThe lambda expression lambda x: y on the other hand, would not be the same\rbecause we cannot replace x with y without a conflict. We could change\rlambda x: y into lambda z: y, but that would be a different function. This\rrule should be intuitively obvious. It’s also not particularly important\rbecause we equally well could have used an infinite sequence of variable\rnames to avoid conflicts; this would eliminate the need for the\r\\(\\alpha\\)-replacement rule. The \\(\\beta\\)-reduction rule captures the very\ressence of what it means to carry out a computation; the \\(\\alpha\\)-replacement\rrule is book-keeping.\nThe above gives the flavor of the \\(\\lambda\\)-calculus: abstraction, application,\r\\(\\alpha\\)-replacement and \\(\\beta\\)-reduction. Since the \\(\\lambda\\)-calculus is\rTuring complete, this can be interpreted as implying that all programming\rcan be reduced to abstraction and application, and all computation can be\rreduced to the \\(\\beta\\)-reduction rule; all else is vanity and grasping at wind.\nBut the examples I’ve used have share the weakness that if you go far enough\rdown, you are relying on other operations. For example, mult = lambda x: lambda y: x * y is ultimately defined in terms of the built-in multiplication\roperation *, and x and y are are of type int. This won’t do; indeed,\rthis is a serious defect, because the whole point of the lambda calculus is to\rprove that these two operations suffice to define any computable function.\rCopping out halfway through and relying on native operations proves nothing.\nTo correct this defect, we need to start from scratch and scrupulously avoid\rusing any operation except for abstraction and application. Happily, Church\rencoding provides a roadmap… it will however lead us to types and data\rstructures very different than the native python int and bool!\n\rFormal Grammar\rThe subset of Python which constitutes the Pythonic \\(\\lambda\\)-calculus\rcan fully described by this BNF specification:\n\u0026lt;expression\u0026gt; ::= \u0026lt;variable\u0026gt;\r| \u0026quot;(\u0026quot; lambda \u0026lt;var\u0026gt; \u0026quot;:\u0026quot; \u0026lt;expression\u0026gt; \u0026quot;)\u0026quot;\r| \u0026lt;expression\u0026gt; \u0026quot;(\u0026quot; \u0026lt;expression\u0026gt; \u0026quot;)\u0026quot;\rIn plain english, we build up expressions recursively out of variables, lambda\rfunctions, and function applications.\nIn some cases, where it causes no ambiguity, we will omit the parentheses\raround lambda functions, or add parentheses around function application e.g.\r(f(x)). Note that the placement of parentheses is rather different than in\rthe original \\(\\lambda\\)-calculus syntax! Where we might have written \\((f x y)\\) in\rthe convential notation we will write f(x)(y), and where we would have\rwritten \\((f (x y))\\) we will write f(x(y)). Hopefully, the Pythonic notation\rfor function application will actually be more familiar to those of you who\rhave studied modern high level programming languages or pure mathematics.\nAs a convenience, we will also allow ourselves definitions, although we will\rlater show that these definitions are merely a convenience and can be done away\rwith whenever we choose through the simple process of string substitution.\r(The development of the Church encoding proceeds mainly by means of such\rdefinitions.) A definition looks like this:\n\u0026lt;definition\u0026gt; ::= \u0026lt;definition-name\u0026gt; \u0026quot;=\u0026quot; \u0026lt;expression\u0026gt;\rWhere the \u0026lt;expression\u0026gt; on the right is restrictions to have no free\rvariables. That is to say, every variable used is in fact inside the body of a\rlambda function with that variable as a parameter. For example, lambda x: x(x)(x(x)) has no free variables, but lambda x: y(x) has y as a free\rvariable. Furthermore, while definitions can contain other definitions,\rthey cannot ever contain themselves, not even implicitly hidden away in\rsome other definition. Otherwise, the simple string substitution macro\rexpansion of a definition would never terminate! (Later, when we need recursion\rto implement the Fibonacci function, we will need to find a way to do this!)\nThese restrictions exist so that we can think of our extended grammar as\rnothing more than a set of lightweight macros on top of the \\(\\lambda\\)-calculus.\rIn principle, for any given program written in the extended grammar, we can\rsimply substitute in the body of the definition wherever we find the name of\rthe definition. This takes only a finite number of steps and is guaranteed to\rterminate. At the end of this process, we are left with an equivalent program\rwith no definitions or definition names. Furthermore, we can complete this\r“macro preprocessing” step entirely before beginning to run the program. In\rthis sense, it is ancillary to the real calculation.\nBTW, these rules for what constitutes a valid definition (as opposed to an\raxiom) can be traced back to Frege, who needed it because he was in the\rprocess of adding quantifiers over bound variables to logic. It turned\rout be a very fruitful principle; modern mathematics is 99% definitions, with\ronly a handful of axioms holding up the foundation. It’s also very broad - even\rthough the \\(\\lambda\\)-calculus is not a logic, the concept of “definition” remains\rmuch the same.\nTo wrap up, the full extended grammar looks like this:\n\u0026lt;definition\u0026gt; ::= \u0026lt;definition-name\u0026gt; \u0026quot;=\u0026quot; \u0026lt;expression\u0026gt;\r\u0026lt;expression\u0026gt; ::= \u0026lt;variable\u0026gt;\r| \u0026quot;(\u0026quot; lambda \u0026lt;var\u0026gt; \u0026quot;:\u0026quot; \u0026lt;expression\u0026gt; \u0026quot;)\u0026quot;\r| \u0026lt;expression\u0026gt; \u0026quot;(\u0026quot; \u0026lt;expression\u0026gt; \u0026quot;)\u0026quot;\r| \u0026quot;(\u0026quot; \u0026lt;definition-name\u0026gt; \u0026quot;)\u0026quot;\rBoth the original grammar and the extended grammar are strict subsets of\rPython, and as such is runnable on a Python interpreter. This is, perhaps, the\rmost extreme example of programming “into” a language instead of programming\r“in” a language, following McConnell’s distinction.\nNote that if we run an expression in the extended grammar directly in Python,\rthe interpreter does not do the macro expansions as described above… but\rthe results of the calculations will always be identical if we’ve carefully\rfollowed the rules for introducing definitions! Later, we will show examples of\rrunning programs both ways.\n\rChurch Booleans\rWe’ll start with the simplest item on our shopping list: Boolean logic.\rThere are only two Boolean values:\n# Church Booleans\rtrue = lambda x: lambda y: x\rfalse = lambda x: lambda y: y\rNote that these are not the same as Python’s built-in True and False\rconstants; our lowercase true and false appear to Python as callable\robjects of type function, not objects of type bool.\nSome of you may object that these are not values, these are functions. Of\rcourse they are; the \\(\\lambda\\)-calculus is made out of nothing but functions!\rBut that doesn’t prevent us from thinking of some functions as values when it\ris convenient for us. Consider the “objects” of object-oriented programming -\ran object is nothing but a collection of functions, but we usually think of\robjects as values.\nMore generally, it is often convenient to talk about the “type” of different\rlambda expressions. However, because we are technically working in the\r“untyped” \\(\\lambda\\)-calculus, we will have to keep the concept of “type”\rhigh-level and informal for now. (There are also “typed” versions but we\rdon’t need it to actually compute stuff.)\nSince we are keeping things informal, we can use an intuitive definition: the\r“type” of an expression is roughly the number and types of the parameter it\rwould normally be called with. In high level languages this is often called the\rfunction signature. The two Church Booleans we defined both have the same type\r- they expect to be called with two arguments and will then return one or the\rother of those two arguments unchanged.\nConsider the following function, which takes a Boolean argument:\nlambda p: p(a)(b)\rThis piece of code will work equally well if p is true or false - only\rthe behavior will be different. If p is true, it will return a and if p\rif false it will return b. In other words, it has the same semantics\ras an if/else statement or the ?: ternary operator.\nIn general, if we have a predicate (an expression which evaluates to a Boolean),\rwe can always write an if/else statement like:\n((\u0026lt;predicate\u0026gt;)\r(\u0026lt;if-expression\u0026gt;)\r(\u0026lt;else-expression\u0026gt;))\rWhile we will have to wait until after we’ve defined the Church numbers to\rget really useful predicates like less_than, we can go ahead and define\rthe usual Boolean operators purely in terms of our above definitions for true\rand false:\n# Boolean logic\rAND = lambda x: lambda y: x(y)(false)\rOR = lambda x: lambda y: x(true)(y)\rNOT = lambda x: x(false)(true)\rXOR = lambda x: lambda y: x(NOT(y))(y)\rEach of these expects to be passed Boolean values and then returns a single\rBoolean value. We can unpack these using the above equivalence to if/else;\rfor example, AND reads as “if x is true, then return y, else return false”.\rThis will return true only if x and y are both true, so this function\racts like “and”. You can read the other definitions in the same way.\nTo more easily interface with these functions, let’s write some bridge code:\nfrom typing import Callable, Any\rdef church_to_bool(b: Callable) -\u0026gt; bool:\rreturn b(True)(False)\rdef bool_to_church(b: bool) -\u0026gt; Callable:\rreturn true if b else false\rNow that we have these bridge functions, it’s fairly easy to write a test\rdemonstrating that we’ve correctly implemented the truth tables for each of\rBoolean operation:\nfor x in (True, False):\rfor y in (True, False):\rx_c = bool_to_church(x)\ry_c = bool_to_church(y)\rz_c = AND(x_c)(y_c)\rz = church_to_bool(z_c)\rprint(x, \u0026quot;AND\u0026quot;, y, \u0026quot;=\u0026quot;, z)\r\rTrue AND True = True\rTrue AND False = False\rFalse AND True = False\rFalse AND False = False\r\rAt this point I encourage you to try to test some of the other Boolean\roperators, and to write your own, such as NAND or the SUM and CARRY of a\rfull adder for more of a challenge.\nThis has been our first taste of computing the \\(\\lambda\\)-calculus. The pattern\r(which we’ll soon see more of) is simple: use Church encoding to somehow\rtranslate your input values into lambda expressions. Pass those into a lambda\rexpression which represent your program. Finally, reverse the Church encoding\rto recover meaningful values.\n\rChurch Numerals\rThe Church encoding of the natural numbers, called Church numerals, defines the\rnumber \\(n\\) to be a binary function (here, “binary” means taking two arguments)\rwhich takes a function f and an arbitrary value x and applies f to x\rexactly \\(n\\) times:\nzero = lambda f: lambda x: x\rone = lambda f: lambda x: f(x)\rtwo = lambda f: lambda x: f(f(x))\rthree = lambda f: lambda x: f(f(f(x)))\r# ... and so on\rHow do you apply a function zero times? Well, you don’t; you just return the\rvalue x right away. To call it once is f(x), twice is f(f(x)), and so on.\rThis means that Church numbers are not an arbitrary sequence of symbols\rthat only gain semantics because of the relations defined on them (as they are\rin other models) but actually have behavior which is directly related to\rtheir meaning: the \\(n\\)-th Church number has the behavior of repeating a\rcomputation \\(n\\) times.\nFor example, suppose we have a function called greet and we want to call it 3\rtimes. How would we implement the equivalent of a for or while loop in the\rPythonic lambda calculus? Just so:\ndef hello_world(n):\rprint(f\u0026quot;Iteration #{n}: Hello, Lambda Calculus!\u0026quot;)\rreturn n+1\rthree(hello_world)(1)\rIteration #1: Hello, Lambda Calculus!\rIteration #2: Hello, Lambda Calculus!\rIteration #3: Hello, Lambda Calculus!\r4\rThe first time greet() is called, it is called with the 1 we passed in. Each\rsubsequent call is passed the return value from the previous call. The function greet()\rwill be called 3 times in total, printing a message each time. Finally, it returns\ra value of 4.\nOf all the high-level programming languages I am aware of, I think only Ruby\rcomes close to the idea that numbers should literally be their own for\rloops. Even LISP and Haskell require recursion, a separate map function,\ror a loop macro. (For code readability alone this is probably a good\rthing, though. In writing this article I’ve found the lack of traditional\rsignpost statements more confusing than elegant, and have had to use comments\rto indicate where such control flow statements were being used.)\nThis one-to-one correspondence between Church numerals and behaviors makes it\rrelatively easy to define mathematical operations on Church numerals.\rFollowing Peano, the first thing we need is a successor function which can\rincrement a number by one:\nsucc = lambda n: lambda f: lambda x: f(n(f)(x))\rWhy does this work? Well, given an original number n, it first uses n\rto apply f to x \\(n\\) times. It then applies f once more itself. Thus,\rthe final value will be f applied to x \\(n+1\\) times, which is \\(n+1\\) by\rdefinition.\nThis successor function allows us to easily construct new numbers\rad infinitum:\n# 0-10 for convenience\rzero = lambda f: lambda x: x\rone = lambda f: lambda x: f(x)\rtwo = succ(one)\rthree = succ(two)\rfour = succ(three)\rfive = succ(four)\rsix = succ(five)\rseven = succ(six)\reight = succ(seven)\rnine = succ(eight)\rten = succ(nine)\rchurch_digits = [zero, one, two, three, four, five, six, seven, eight, nine]\rWe can now define other mathematical operators:\n# church numerals\rplus = lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x))\rmult = lambda m: lambda n: lambda f: lambda x: m(n(f))(x)\rexp = lambda m: lambda n: n(m)\rpred = (lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u))\rminus = lambda m: lambda n: n(pred)(m)\rOf these, plus and mult are the easiest to understand. plus first applies\rf to x \\(m\\) times, then applies f to the result \\(n\\) times, for a total of\r\\(n+m\\) applications. The multiplication operator mult is similar, but does\rthings in a slightly different order: it first defines a new function g = n(f) which applies f to some value n times, and then applies g to x m\rtimes. Since each call to g ends up calling f \\(n\\) times, the result is\rthat f is applied to x \\(m \\times n\\) times.\nTry to figure out exp for yourself. It’s equivalent to \\(m^n\\). It’s not on the\rmain line of functionality we need for the Fibonacci function, and it’s very\rclever!\npred is the predecessor relation. It subtracts one from a number if possible.\r(Zero is just mapped to zero, as negative numbers are not defined.) It’s\rmore complex than succ but studying it is extremely rewarding,\rbecause it leads to understanding how data structures can be represented in the\r\\(\\lambda\\)-calculus. The basic idea is that we are going to but the value x in a\rbox and replace f with a different function, which I’ll call skip_first. The\rfirst time skip_first is called, it sees that the box has not been opened, so\rit opens that. After that, it sees that the box is already open, so it takes\rthe value out of the box, applies f to it once, and puts it back in the box.\rIt does this \\(n\\) times. At the end, it takes the value of the box. The\rultimate result is that f is applied to x \\(n-1\\) times, because nothing\rhappened the first time. In this analogy, the initial closed “box” is lambda u: x, the new box that is created after each step is lambda h: h(g(f)), and\rthe lambda u: u at the end is the final act of taking the value out of the\rbox.\npred is tricky understand, especially in this elementary form. The Wikipedia\rarticle also has a pretty good explanation too. A good exercise to\rmanually work out the \\(\\beta\\)-reduction of pred(two) to get a feel for it.\rIf it still gives you trouble, I suggest you leave it aside and study the rest\rof theory until you learn how to encode the “pair” data structure. Then pred\rmuch may be defined in a much more natural way. just to implement the\rFibonacci function so decided to take the straight path through the mud.\rNevertheless, there is a switchback trail with a very gentle slope right over\rthere.\nDefining pred was the hard part. The definition of minus in terms of pred\ris much easier: n applies the function pred to m \\(n\\) times, so we\rsubtract one \\(n\\) times, which is the same as subtracting n from m. Easy, yes?\nNow that we have a reasonable set of mathematical operations, let’s do some\rpractical examples. We can do basic operations like \\(2+2\\) or \\(6 \\times 7\\):\nplus(two)(two)\rmult(six)(seven)\rThe problem with these is that what they return is a Church numeral, which is a\rPython Callable. All I see on my screen when I run the above snippets is\ropaque and ambiguous output like this:\n\u0026lt;function __main__.\u0026lt;lambda\u0026gt;.\u0026lt;locals\u0026gt;.\u0026lt;lambda\u0026gt;.\u0026lt;locals\u0026gt;.\u0026lt;lambda\u0026gt;(x)\u0026gt;\rTo translate this back into something we can understand, we need to write\ranother bridge function. But how can we “unpack” a Church number? Recall that\rthe Church encoding of \\(n\\) is a function taking two arguments, a function \\(f\\)\rand a value \\(x\\), and it applies \\(f\\) to \\(x\\) \\(n\\) times: \\(f^n(x)\\). In our Python\renvironment, this works even if \\(x\\) and \\(f\\) are not written in the lambda\rcalculus. Therefore we can follow the spy pattern often used in unit\rtesting and pass in an function which will report how often it was called.\ndef church_to_int(n: Callable) -\u0026gt; int:\rreturn n(lambda x: x+1)(0)\rGoing the other way requires no such tricks. If I want to encode the number\r42 in the \\(\\lambda\\)-calculus, I can use the first ten digits (defined above)\rand our mathematical operations to build up the Church number:\nplus(mult(four)(ten))(two)\rWe can use the same strategy for any number. All we really need to do is\rparse the base 10 representation of a number and build up the Church number in\rstages:\ndef int_to_church(n: int) -\u0026gt; Callable:\rchurch_number = church_digits[ int(str(n)[0]) ]\rfor digit in str(n)[1:]:\rchurch_digit = church_digits[int(digit)]\rchurch_number = plus(mult(church_number)(ten))(church_digit)\rreturn church_number\rWe can now perform non-trivial calculations entirely in the \\(\\lambda\\)-calculus:\n\u0026gt; print(\u0026quot;2 + 2 =\u0026quot;, church_to_int(plus(two)(two)))\r4\r\u0026gt; print(\u0026quot;6 * 7 =\u0026quot;, church_to_int(mult(six)(seven)))\r42\rHere is a much larger (and slower) example:\n\u0026gt; a = int_to_church(1001)\r\u0026gt; b = int_to_church(999)\r\u0026gt; ab = mult(a)(b)\r\u0026gt; print(\u0026quot;1001 * 999 =\u0026quot;, church_to_int(ab))\r999999\rNow, finally, we can implement our sums-of-squares method:\n\u0026gt; a = int_to_church(3)\r\u0026gt; b = int_to_church(4)\r\u0026gt; ss = plus(exp(a)(two))(exp(b)(two))\r\u0026gt; print(\u0026quot;3**2 + 4**2 =\u0026quot;, church_to_int(ss))\r25\rThis isn’t all of number theory of course, but its enough to implement\rour little Fibonacci function!\n\rPredicates Involving Numbers\rThe first and most basic predict test we need is a check for zero. This\rwill form the foundation of all the other predicates:\nis_zero = lambda n: n(lambda x: false)(true)\rThis works because if lambda x: false is called even once, the result will be\rfalse, and this can only be avoided if the function is never called, in which\rcase the original value true will be returned. But the only Church numeral\rwhich never calls its function argument is zero, so the above function\rreturns true only for zero, and false for every other number.\nBy the way, a function which returns a value of type Church Boolean is the\rdefinition of a “predicate” in this context. The word carries no logical or\rsemantic content here.\nThe fact that pred stops at zero (i.e., pred(zero) == zero) implies that\rminus(x)(y) == zero if and only if y is bigger than or equal to x. We can\ruse this fact to define various comparison tests:\nleq = lambda m: lambda n: is_zero(minus(m)(n))\rless_than = lambda m: lambda n: leq(succ(m))(n)\req = lambda m: lambda n: AND(leq(m)(n))(leq(n)(m))\rThese functions are interesting because while the expect their arguments n\rand m to be Church numbers, their return value is a Church Boolean. The main\rthing we wanted was less_than, which we will need for our Fibonacci function.\n\rRecursion\rRather than jumping straight into implementing recursion in the \\(\\lambda\\)-calculus,\rlet’s take it slow and develop the idea in stages. Let’s start with vanilla\rPython recursion:\ndef factorial(n):\rif n \u0026lt;= 1:\rreturn 1\relse:\rreturn n * factorial(n-1)\rThis only works because by the time the interpreter reaches the statement\rfactorial(n-1) the global symbol table already contains an entry for\rfactorial, so Python happily resolves factorial to that function and calls\rit. In other words, it works because Python is doing all the heavy lifting for\rus!\nIn the \\(\\lambda\\)-calculus, there is no global symbol table. Even if there were,\rlambda functions are all anonymous: they don’t have names, so what would you\reven query the symbol table for? The workaround is to pass the function into\ritself as an argument. This is totally legal; x(x) is a perfectly cromulent\rexpression in Pythonic \\(\\lambda\\)-calculus. Continuing with our factorial example a\rlittle further, we have:\ndef _factorial(f, n):\rif n \u0026lt;= 1:\rreturn 1\relse:\rreturn n * f(f, n-1)\rdef factorial(n):\rreturn _factorial(_factorial, n)\r\rNeat. But it’s a little ugly that we have to make the recursive call as f(f, n-1)\rand explicitly pass f back into itself. Why not make f a closure which Currys\rthat first argument for us?\ndef _factorial(f, n):\rif n \u0026lt;= 1:\rreturn 1\relse:\rreturn n * f(n-1)\rdef factorial(n):\rf = lambda m: _factorial(f, m)\rreturn f(n)\rWe can make this more generic and reduce reliance on the global namespace by\rpassing _factorial in as an argument:\ndef call_recursively(f, n):\rg = lambda m: f(g, m)\rreturn g(n)\rdef _factorial(f, n):\rif n \u0026lt;= 1:\rreturn 1\relse:\rreturn n * f(n-1)\rdef factorial(n):\rreturn call_recursively(_factorial, n)\rFinally, the call_recursively function can be abstracted entirely as a Python\rdecorator. (If you’re not familiar with the decorator syntax, @decorator/f is\rsimply syntactic sugar for f = decorator(f) and is a convenient way to apply\ra functor to a function.)\ndef recursive(f):\rdef recursive(n):\rg = lambda m: f(g, m)\rreturn g(n)\rreturn recursive\r@recursive\rdef factorial(f, n):\rif n \u0026lt;= 1:\rreturn 1\relse:\rreturn n * f(n-1)\r\rSo, in vanilla Python, we’ve implemented a reusable utility which enables us to\rconveniently do recursion while avoiding any reference to the global symbol\rtable. As we make the jump to the final form purely in Pythonic\r\\(\\lambda\\)-calculus, we will rename recursive to Y - it is indeed the famous\rY-combinator, the higher-order function which makes recursion (and therefore\ralso iteration) possible in the \\(\\lambda\\)-calculus. As for why it is called\rY, I have no idea - it’s just the standard symbol.\nY = lambda f: (lambda x: x(x))(lambda y: f(lambda z: y(y)(z)))\rWe can also apply exactly one application of the \\(\\beta\\)-reduction rule to\rbring this into an equivalent symmetrical form:\nY = lambda f: (lambda y: f(lambda z: y(y)(z)))(lambda y: f(lambda z: y(y)(z)))\rWhile the symmetrical form is more often seen, I prefer the first version\rbecause I think it more clearly expresses the idea of currying a function with\ritself. However, both do exactly the same thing. Furthermore, neither have any\rfree variables and so meet our requirements for a proper definition.\nIn an ideal world we could now define the recursive factorial function entirely\rin Pythonic \\(\\lambda\\)-calculus like so:\nfactorial = Y(lambda f: lambda n:\r((leq)(n)(one)\r(one)\r(mult(n)(f(pred(n))))))\rHowever, there is a problem: when we run the above function we get a stack\roverflow error. Why this so? Is the algorithm wrong? No: if you executed this\rlambda expression with true \\(\\beta\\)-reduction, it would work fine. The problem\ris that our Church Boolean pseudo-if-else statement is not quite a proper\rif-else! In a language like C or Python, the code inside the selected branch\rwill be executed, but code in the other branch will not even be run. However,\rin the Pythonic \\(\\lambda\\)-calculus, if we write:\n((some-predicate)\r(if-branch)\r(else-branch))\rThen both the if-brach and the else-brach will need to be evaluated\rcompletely before some-predicate can be called, regardless of the value of\rsome-predicate!\nThis is called “eager” evaluation and Python always eagerly evaluates all\rarguments of a function call before performing the function call. Therefore\rin Python, we will always compute both branches, and only at the very end\rwill we discard one of the values. Normally, this wouldn’t cause serious\rproblems because the answer would always be the same (it would just be a little\rslower as it takes time to the work which gets thrown away.) It becomes a\rserious problem in the case of recursion because the else branch is always\revaluated, which calls the function again, which calls the function again, and\rso for forever.\nOne response would be to give up on Python and go abuse some other language\rwhich has lazy evaluation. (It’s not a coincidence that functional\rlanguages like Haskell generally support lazy evaluation!)\rAlternatively, we could write an interpreter for the \\(\\lambda\\)-calculus which\rimplements \\(\\beta\\)-reduction rule, side-stepping Python entirely.\nThese are good options, and for someone writing a very complete implementation\rof the \\(\\lambda\\)-calculus they would be necessities. But today we’re only\rconcerned to write one function, the Fibonacci function, so we can use a much\rsimpler hack to prevent infinite recursion, which I will abbreviate PIR:\n# cheap hack because true and false don\u0026#39;t short circuit.\rdef PIR(f):\rdef wrapper_function(n):\rif church_to_bool(is_zero(n)):\rreturn zero\relse:\rreturn f(n)\rreturn wrapper_function\rWith our little hack in place, we can now implement a working version of the\rfactorial function which does not cause a stack overflow:\nfactorial = Y(lambda f: PIR(\rlambda n:\r((leq)(n)(one)\r(one)\r(mult(n)(f(pred(n)))))))\rLong stacks of parentheses appear to be an operational hazard when working with\r\\(\\lambda\\)-calculus inspired languages.\n\nIn any case, the above function is a little inconvenient to call because it\rrequires a Church numeral as input and returns an opaque Church numeral. Let’s\rwrap it in bridge code:\ndef slow_factorial(n):\rn = int_to_church(n)\rfac = factorial(n)\rreturn church_to_int(fac)\rThis little function correctly computes factorials:\nfor n in range(1, 10):\rprint(n, slow_factorial(n))\r1 1\r2 2\r3 6\r4 24\r5 120\r6 720\r7 5040\r8 40320\r9 362880\r10 3628800\rWhat about our main design objective? I am pleased to report that the above\rfunction is indeedextremely slow: it takes over a minute to calculate \\(10!\\)\ron a fairly new laptop. That works out to 36 million times slower than the\rvanilla Python implementation.\nWe can profile the code to figure out why. Here, I’ve edited the profiler\routput from %prun slow_factorial(9) to give names to the most common calls;\rin the original the function name was always just (\u0026lt;lambda\u0026gt;) distinguished\ronly by line number.\n ncalls tottime percall cumtime percall function\r1564014 0.849 0.000 0.849 0.000 succ\r362889 0.354 0.000 0.542 0.000 plus\r362880 0.188 0.000 0.188 0.000 church_to_int\r260669 0.152 0.000 0.152 0.000 zero\r79211 0.052 0.000 0.052 0.000 pred\r9/1 0.000 0.000 0.003 0.003 factorial\rSo, we actually spend almost all of our time simply incrementing numbers by\rone. Church numbers are easy to define and work with, but they are gloriously\rinefficient, especially as the numbers grow. A more efficient encoding could be\rdefined by using the Boolean algebra we developed earlier to define operations\ron binary strings, but that is not what we are about today.\n\rFinal Fibonacci\rOur shopping list is complete: we now have all the necessary tools to proceed\rto the endgame. All that remains is to implement the Fibonacci algorithm:\nfibonacci = Y(lambda f: PIR(\rlambda n: less_than(n)(two)\r(n)\r(plus\r(f(minus(n)(one)))\r(f(minus(n)(two))))))\rThe less_then(n)(two) is a predicate that resolves to a Church Boolean.\rThis Boolean is then used as an if/else statement returning n for the if\rbranch or the recurance relation for the Fibonacci for the else branch. The\relse branch is simply the sum of \\(F_{n-1}\\) and \\(F_{n-2}\\). The Y-combinator\rensures that f is in fact the same fibonacci function so we can call\rf(n-1) and f(n-2) to calculuate \\(F_{n-1}\\) and \\(F_{n-2}\\).\nBecause we make two separate recursive calls and don’t do any caching, the\rnumber of calls to fibonacci() will grow roughly as \\(\\mathcal{O}(2^n)\\). This\ris of course a terrible algorithm; it’s the same one I called naive_fib() in\rmy earlier article on optimizing the Fibonacci function. However, this\ris entirely in keeping with our goal of writing the slowest possible version!\nAs before, we’ll wrap this in bridge code to handle the translation between\rnative Python integers and Church numerals:\ndef slow_fibonacci(n: int) -\u0026gt; int:\rn = int_to_church(n)\rfib = fibonacci(n)\rreturn church_to_int(fib)\rWe can test that it is correct by exhibiting the first 20 Fibonacci numbers:\nfor n in range(21):\rprint(n, slow_fibonacci(n))\r0 0\r1 1\r2 1\r3 2\r4 3\r5 5\r6 8\r7 13\r8 21\r9 34\r10 55\r11 89\r12 144\r13 233\r14 377\r15 610\r16 987\r17 1597\r18 2584\r19 4181\r20 6765\r\rHow Slow is Slow?\rAs expected, this is rather slow, over 6 seconds to calculate \\(F_{20}\\):\n%time slow_fibonacci(20)\rCPU times: user 6.59 s, sys: 20 ms, total: 6.61 s\rWall time: 6.6 s\rThis is one thousand times slower than the same naive algorithm in implemented\rvanilla Python, and about a million times slower than a good algorithm. Note\rhowever that this is still faster than a human could work it out on paper! As\rrecently as 80 years ago this would have been state-of-the-art.\nA straight line on a log-scale plot shows that the algorithm scales as\r\\(\\mathcal{O}(2^n)\\).\nTiming Slow Fibonacci\n\rThe profiler shows where we were spending our time:\n 1922492 function calls (1833945 primitive calls) in 1.858 seconds\rOrdered by: internal time\rncalls tottime percall cumtime percall function\r1133885 0.513 0.000 0.513 0.000 pred\r17710/1 0.199 0.000 31.615 31.615 fibonacci\r95316 0.173 0.000 3.507 0.000 two\r59896 0.159 0.000 3.636 0.000 mult\r53131 0.118 0.000 30.497 0.001 is_zero\r53130 0.110 0.000 0.280 0.000 minus\r35421/1 0.092 0.000 31.615 31.615 PIR-wrapper\r35420/2 0.083 0.000 31.615 15.807 Y\r119792 0.074 0.000 0.074 0.000 three\r35421 0.072 0.000 0.114 0.000 church_to_bool\r17710 0.057 0.000 10.598 0.001 less_than\r35420 0.052 0.000 0.076 0.000 fibonacci-wrapper\r64077 0.041 0.000 0.041 0.000 zero\r35420 0.024 0.000 0.024 0.000 PIR\r17710 0.022 0.000 0.033 0.000 one\r28655 0.014 0.000 0.014 0.000 false\r24476 0.012 0.000 0.012 0.000 true\r17710 0.012 0.000 0.012 0.000 leq\r17711 0.012 0.000 0.012 0.000 plus\r17710 0.012 0.000 0.012 0.000 succ\r6765 0.006 0.000 0.006 0.000 church_to_int\rUnlike slow_factorial() which deals with numbers that blow up very\rquickly, slow_factorial() deals with relatively smaller numbers which\rmeans that we spent less time simply iterating through succ and more time\rdoing interesting things. Nevertheless, it spends a lot of time doing simple\rsubtractions - this is one of the weak points of the Church numerals.\n\rSlower Than Slow\rHow could we make this even slower? Again, in a fair way, not just sprinkling\rno-ops and sleep statements throughout.\nOne interesting approach would be to implement what is sometimes called a\rmeta-circular evaluator: an interpreter for the \\(\\lambda\\)-calculus written\rentirely within the \\(\\lambda\\)-calculus itself. We could then stack interpreters\rindefinitely, with each layer costing us another factor of a thousand. It would\rbe reminiscent of this art project where a long gear chain is used to build a\rmachine which will take 13.7 billion years for the final gear to complete one\rrotation:\n\nThe machine has a electric motor happily whirring away at one end (click the\rimage for a video showing it action) and a solid block of concrete at the\rother. Normally that would be a recipe for disaster but because each gear steps\rthe revolution speed down by a factor of 10, and because so many gears are\rchained together, the motion is infinitesimal by the end.\nWe’re not actually going to do that, of course. This article is already way too\rlong. But we could.\n\rMacro Expansion\rPerhaps you don’t believe that this is really a \\(\\lambda\\)-calculus program; after\rall, it has all those “definitions” which look suspiciously like named\rfunctions and in fact are being treated as named functions by the Python\rinterpreter! Very suspicious.\nWe can see this is not the case by doing a search for each definition’s name\rand replacing it with its body, and repeating until no named definitions are\rleft. Below is the step-by-step expansion process, with one version per line;\rthe last line contains no definitions (expect our PIR hack) and instead is\rcomposed entirely of lambda functions, function calls, and bound variable\rnames.\nfibonacci = Y(lambda f: PIR(lambda n: (less_than(m)(two))(n)(two)(n)(plus(f(minus(n)(one)))(f(minus(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: leq(succ(m))(n))(n)(two)(n)(plus(f(minus(n)(one)))(f(minus(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: is_zero(minus(m)(n)))(succ(m))(n))(n)(two)(n)(plus(f(minus(n)(one)))(f(minus(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: false)(true))(minus(m)(n)))(succ(m))(n))(n)(two)(n)(plus(f(minus(n)(one)))(f(minus(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))(minus(m)(n)))(succ(m))(n))(n)(two)(n)(plus(f(minus(n)(one)))(f(minus(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n(pred)(m))(m)(n)))(succ(m))(n))(n)(two)(n)(plus(f((lambda m: lambda n: n(pred)(m))(n)(one)))(f((lambda m: lambda n: n(pred)(m))(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n(pred)(m))(m)(n)))(succ(m))(n))(n)(two)(n)(plus(f((lambda m: lambda n: n(pred)(m))(n)(one)))(f((lambda m: lambda n: n(pred)(m))(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(m)(n)))(succ(m))(n))(n)(two)(n)(plus(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(one)))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(m)(n)))(succ(m))(n))(n)(two)(n)((lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(one)))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(two))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(m)(n)))(succ(m))(n))(n)(succ(one))(n)((lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(lambda f: lambda x: f(x))))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(succ(one)))))))\rfibonacci = Y(lambda f: PIR(lambda n: (lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(m)(n)))((lambda n: lambda f: lambda x: f(n(f)(x)))(m))(n))(n)((lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: f(x)))(n)((lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(lambda f: lambda x: f(x))))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)((lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: f(x))))))))\rfibonacci = (lambda f: (lambda x: x(x))(lambda y: f(lambda z: y(y)(z))))(lambda f: PIR(lambda n:(lambda m: lambda n: (lambda m: lambda n: (lambda n: n(lambda x: (lambda x: lambda y: y))(lambda x: lambda y: x))((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(m)(n)))((lambda n: lambda f: lambda x: f(n(f)(x)))(m))(n))(n)((lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: f(x)))(n)((lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)(lambda f: lambda x: f(x))))(f((lambda m: lambda n: n((lambda n: lambda f: lambda x: n(lambda g: lambda h: h(g(f)))(lambda u: x)(lambda u: u)))(m))(n)((lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: f(x))))))))\rThe ultimate expanded version of this function works just as well and runs just\ras fast. Although it does nothing but create closures and call functions,\rsomehow in the end it carries out the computation of a mathematical function.\rTrue, it is a little hard to read in this form, but so is machine code.\nHere is a visualization of the full Fibonacci program (click for a larger\rimage.) This shows every node (either a variable, function call, or lambda\rabstraction) as a binary tree.\n\nIt reminds me a lot of this XKCD comic strip about LISP where\rcons is taken as atomic. Of course, the pair data structure\rcan easily be defined in the \\(\\lambda\\)-calculus as well, making lambda\rabstractions even more than fundamental cons. The essential insight remains\rthe same: all programs and data structures can be reduced to binary trees and\rall information about the program is somehow contained in the very structure of\rthe tree itself.\n\rConclusion\rAll models of computation are equal, but some are more equal than others. In\rtheory, the \\(\\lambda\\)-calculus is only a constant factor away from any other model\rof computation, but the same is not true for the Church encoding: Church\rnumerals are useful because they are very easy to “bootstrap;” that is to say,\rto implement in terms of lower level primitives, while on the other hand it is\rvery difficult to implement more efficient numbers without data structures and\ra ready supply of distinct symbols, which the Church numerals provide.\nIt took us only a few dozen definitions to go from something so spartan that it\rseemed to be missing every convenience of modern programming, to a useful\rlanguage with recursion, for loops, if/else statements, and recursive function\rcalls. Things like closures, Currying, functors/decorators, which are\rconsidered advanced features in other languages, we somehow got for free.\nIf we had carried on defining signed numbers, pairs, lists, associative maps,\rand so on, this parsimony would continue and after a few hundred definitions -\rrather smaller than the standard library of most languages - we would have a\rperfectly functional language. (Pun absolutely intended.)\nSome languages like Forth and io in fact take this exact approach:\rthe language is absolutely minimal and almost everything is moved to the\rstandard library, including the definition of standard constructs like for\rand if/else.\nIf computation is so simple, why are modern languages so complicated? It mostly\rboils down to the impedance mismatch between the \\(\\lambda\\)-calculus and\rthe kinds of electronic computers we can actually build. We can’t actually make\rChurch numerals fast, but we can make a machine which can perform\rmathematical operations on two 64 bit numbers in a single cycle. We can’t write\ra performant \\(\\beta\\)-reducer, but we can write a block of machine code which\rcalls functions by using a calling convention: pops a certain number\rarguments off the stack when called and returns a value on the stack or in a\rparticular register. And so on.\nFor reasons that aren’t entirely clear to me, the \\(\\lambda\\)-calculus has been a\rgood model for thinking about high-level, expressive ways of computing, while\rthe Turing machine has been a good model for designing actual computing\rmachines. While in theory we can always switch from one model of computation\rto another, in practice this can involve a three or four order of magnitude\rreduction is performance. Practical programming languages have to walk the line\rbetween exposing the real capabilities of the machine while also providing\ruseful high level abstractions.\nWhich isn’t to say there aren’t practical languages very much inspired by and\rvery close to the \\(\\lambda\\)-calculus. LISP and Haskell come to mind. After\rwatching some of the SICP lectures on Youtube, I’m convinced the only\rreason Harold Abelson didn’t just teach the whole course in pure\r\\(\\lambda\\)-calculus is because computers back then were still slow enough that\rit would have been just a little too painful, and LISP was chosen as a\rcompromise. (Unfortunately, as we’ve seen today, this is still true. But maybe\rsomeday…) Many JavaScript programs too, particularly those that create lots\rof closures to handle asynchronous callbacks, seem to me to be much closer in\rspirit to the \\(\\lambda\\)-calculus (as opposed to something like C, which uses a\rpointer machine model which is only slightly higher level than a Turing\rmachine.) It’s never been more important to expose CS students to the idea\rand modes of thinking inspired by the \\(\\lambda\\)-calculus and I think it’s\rimportance will only grow as we can better afford the cost of abstractions.\nAs always, all the source code for this article is free and open source.\n\rPostscript\rReading the “macro expanded version” of the function out loud, the similarity\rbetween “lambda” and “llama” made me think of the children’s book \"Llama llama\rred pajama, a poem about a baby llama and his mama. I translated the above\rfunction declaration into a made up agglutinative language to produce this\rnonsense bedtime rhyme… perhaps the only one in the world which is also a\rexecutable program.\n\rHey llama baby hey llama mama hey Ma sleep sleep hey llama ba baby Hey llama ta ba hey ba sleep hey Ta sleep sleep sleep sleep hey Llama baby llama na hey llama ga Llama na hey llama ga llama na Hey llama na na hey llama ma hey Llama ma llama ba ba sleep sleep Hey llama ma llama ba ma sleep Sleep hey hey llama ga llama na Na hey hey llama na llama baby Llama ma na hey llama red pajama Llama haha hey red pajama hey Baby sleep sleep sleep hey llama Drama sleep hey llama dra dra Sleep sleep sleep hey ga sleep Sleep hey ga sleep hey na sleep Sleep sleep hey hey llama na Llama baby llama ma baby hey na Hey baby sleep hey ma sleep sleep Sleep hey ga sleep sleep hey na Sleep sleep hey na sleep hey hey Llama na llama baby llama ma baby Hey na hey baby sleep hey ma Sleep sleep sleep hey llama baby Llama ma baby hey ma sleep sleep Sleep hey na sleep hey hey llama Ga llama na llama baby llama ma Ga hey baby sleep hey na hey baby Sleep hey ma sleep sleep sleep Hey baby hey hey llama ga llama Na na hey hey llama na llama baby Llama ma na hey llama red pajama Llama haha hey red pajama hey Baby sleep sleep sleep hey llama Drama sleep hey llama dra dra Sleep sleep sleep hey ga sleep Sleep hey na sleep hey llama baby Llama ma baby hey ma sleep sleep Sleep sleep hey baby hey hey Llama ga llama na na hey hey Llama na llama baby llama ma na Hey llama red pajama llama haha Hey red pajama hey baby sleep Sleep sleep hey llama drama sleep Hey llama dra dra sleep sleep Sleep hey ga sleep sleep hey na Sleep hey hey llama na llama baby Llama ma baby hey na hey baby Sleep hey ma sleep sleep sleep Hey llama baby llama ma baby hey ma Sleep sleep sleep sleep sleep sleep sleep\r\r\r","date":"July 6, 2019","href":"https://www.oranlooney.com/post/slow-fibonacci/","thumbnail":"/post/slow-fibonacci_files/lead.192x128.jpg","title":"A Seriously Slow Fibonacci Function"},{"content":"Consider the following motivating dataset:\nUnlabled Data\n It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. The question is: is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?\nEvery model we’ve look at so far has assumed that we have a clear definition of the thing we are trying to predict and that we already know the correct answer for every example in the training set. A problem of the form “just find me some kind interesting relationships or structure, any will do” does not fit into this framework because no “true” labels are known in advance. More formally: every problem so far has been a “supervised” learning problem, where the training set consists of labeled pairs \\((X, Y)\\) and the task was to predict \\(Y\\) from \\(X\\). The problem of discovering interesting structure or relationships from unlabeled examples \\(X\\) is called the “unsupervised” learning problem, and calls for a different set of techniques and algorithms entirely.\nTypes of Unsupervised Learning There are two broad approaches to unsupervised learning: dimensionality reduction and cluster analysis.\nIn dimensionality reduction we seek a function \\(f : \\mathbb{R}^a \\mapsto \\mathbb{R}^b\\) where \\(a\\) is the dimension of the original data \\(\\mathbf{X}\\) and \\(b\\) is usually much smaller than \\(a\\). The classic example of a dimensionality reduction algorithm is PCA but there are many others, including non-linear techniques like t-SNE, topic models like LDA, and most examples of representation learning such as Word2Vec. The basic idea is that by reducing to a lower dimensional space we somehow capture the essential characteristics of each data point while getting rid of noise, multicollinearity, and non-essential features. Furthermore, it should be possible to approximately reconstruct the original data point in the original \\(a\\)-dimensional space from just its compressed \\(b\\)-dimensional representation with minimal loss. Depending on the specific technique used, the lower dimensional space may also be designed to have desirable properties like an isotropic/spherical covariance matrix or a meaningful distance function where data points that a human would agree are “similar” are close together. We will return to dimensionality reduction in some future article.\nThe second approach to unsupervised learning is called clustering and is characterized by seeking a function \\(f : \\mathbb{R}^a \\mapsto \\{1,2, ..., k\\}\\) which maps each data point to exactly one of \\(k\\) possible classes. The classic example of a clustering algorithm is \\(k\\)-means. Reducing rich, multivariate data to a small finite number of possibilities seems extreme, but for that same reason it can be extremely clarifying as well. In this article we will implement on particular clustering model called the Gaussian mixture model, or just GMM for short.\n Gaussian Mixture Models The Gaussian mixture model is simply a “mix” of Gaussian distributions. In this case, “Gaussian” means the multivariate normal distribution \\(\\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)\\) and “mixture” means that several different gaussian distributions, all with different mean vectors \\(\\boldsymbol{\\mu}_j\\) and different covariance matrices \\(\\Sigma_j\\), are combined by taking the weighted sum of the probability density functions:\n\\[ \\begin{align} f_{GMM}(\\mathbf{x}) = \\sum^k_{j=1} \\phi_j f_{\\mathcal{N}(\\boldsymbol{\\mu}_j, \\Sigma_j)}(\\mathbf{x}) \\tag{1} \\end{align} \\]\nsubject to:\n\\[ \\sum_{j=1}^k \\phi_j = 1 \\tag{2} \\]\nA single multivariate normal distribution has a single “hill” or “bump” located at \\(\\boldsymbol{\\mu}_i\\); in contrast, a GMM is a multimodal distribution with on distinct bump per class. (Sometimes you get fewer than \\(k\\) distinct local maxima in the p.d.f., if the bumps are sufficiently close together or if the weight of one class is zero or nearly so, but in general you get \\(k\\) distinct bumps.) This makes it well suited to modeling data like that seen in our motivating example above, where there seems to be more than one region on high density.\n\nWe can view this is as a two-step generative process. To generate the \\(i\\)-th example:\nSample a random class index \\(C_i\\) from the categorical distribution parameterized by \\(\\boldsymbol{\\phi} = (\\phi_1, ... \\phi_k)\\). Sample a random vector \\(\\mathbf{X}_i\\) from the multivariate distribution associated to the \\(C_i\\)-th class.  The \\(n\\) independent samples \\(\\mathbf{X}_i\\) are the row vectors of the matrix \\(\\mathbf{X}\\).\nSymbolically, we write:\n\\[ \\begin{align} C_i \u0026amp; \\sim \\text{Categorical}(k, \\boldsymbol{\\phi}) \\tag{3} \\\\ \\mathbf{X}_i \u0026amp; \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{C_i}, \\Sigma_{C_i}) \\tag{4} \\\\ \\end{align} \\]\nTo fit a GMM model to a particular dataset, we attempt to find the maximum likelihood estimate of the parameters \\(\\Theta\\):\n\\[\\Theta = \\{ \\mathbf{\\mu}_1, \\Sigma_1, ..., \\mathbf{\\mu}_k, \\Sigma_k \\} \\tag{5} \\]\nBecause the \\(n \\times m\\) example matrix \\(\\mathbf{X}\\) is assumed to be a realization of \\(n\\) i.i.d. samples from \\(f_{GMM}(\\mathbf{x})\\), we can write down our likelihood function as\n\\[ \\mathcal{L}(\\Theta; \\mathbf{X}) = P(\\mathbf{X};\\Theta) = \\prod_{i=1}^n \\sum_{j=1}^k P(C_i=j) P(\\mathbf{X}_i|C_i=j) \\tag{6} \\]\nWe know that \\(\\mathbf{X}_i\\) has a multivariate normal distribution with parameters determined by the class, so the conditional probability \\(P(\\mathbf{X}_i|C_i=j)\\) can be written down pretty much directly from the definition:\n\\[ P(\\mathbf{X}_i|C_i=j) = \\frac{1}{\\sqrt{(2\\pi)^k |\\Sigma_j|}} \\text{exp}\\Bigg( - \\frac{(\\mathbf{X}_i - \\boldsymbol{\\mu}_j)^T \\Sigma_j^{-1} (\\mathbf{X}_i - \\boldsymbol{\\mu}_j) } {2} \\Bigg) \\tag{7} \\]\nObtaining a formula for \\(P(C_i=j|\\mathbf{X}_i)\\) requires a little more work. We know that the unconditional probability is given by the parameter vector \\(\\boldsymbol{\\phi}\\):\n\\[ P(C_i = j) = \\phi_j \\tag{8} \\]\nSo using Bayes’ theorem, we can write this in terms of equation (7):\n\\[ \\begin{align} P(C_i=j|\\mathbf{X}_i) \u0026amp; = \\frac{P(C_i=j) P(\\mathbf{X}_i|C_i=j)} {P(\\mathbf{X}_i)} \\\\ \u0026amp; = \\frac{ \\phi_j P(\\mathbf{X}_i|C_i=j)} {\\sum_{l=1}^k P(\\mathbf{X}_i|C_i=l)} \\\\ \\end{align} \\tag{9} \\]\nIf we substituted equation (7) into (9) we could get a more explicit but very ugly formula, so I leave that to the reader’s imagination.\nEquations (6), (7), and (9), when taken together, constitute the complete likelihood function \\(\\mathcal{L}(\\Theta;\\mathbf{X})\\). However, these equations have a problem - they depend on the unknown random variable \\(C_i\\). This variable tells us which class each \\(\\mathbf{X}_i\\) was drawn from and makes it much easier to reason about the distribution, but we don’t actually know what \\(C_i\\) is for any \\(i\\). This is called a latent random variable and its presence in our model causes a kind of chicken-and-egg problem. If we knew \\(\\boldsymbol{\\mu}_j\\) and \\(\\Sigma_j\\) for \\(j = (1, 2, ..., k)\\) then we could make a guess about what \\(C_i\\) is by looking at which \\(\\boldsymbol{\\mu}_j\\) is closest to \\(\\mathbf{X}_i\\). If we knew \\(C_i\\), we could estimate \\(\\boldsymbol{\\mu}_j\\) and \\(\\Sigma_j\\) by simply taking the mean and covariance over all \\(X_i\\) where \\(C_i = j\\). But how can we estimate these two sets of parameters together, if we don’t know either when we start?\n The EM Algorithm The solution to our chicken-and-egg dilemma is an iterative algorithm called the expectation-maximization algorithm, or EM algorithm for short. The EM algorithm is actually a meta-algorithm: a very general strategy that can be used to fit many different types of latent variable models, most famously factor analysis but also the Fellegi-Sunter record linkage algorithm, item response theory, and of course Gaussian mixture models.\nThe EM algorithm requires us to introduce a pseudo-parameter to model the unknown latent variables \\(C_i\\). Because \\(C_i\\) can take on \\(k\\) discrete values, this new parameter will be a \\(n \\times k\\) matrix where each element \\(w_{ij}\\) is an estimate of \\(P(C_i = j|\\mathbf{X}_i;\\theta)\\). Each element of this matrix represents the probability that the \\(i\\)-th data point came from cluster \\(j\\). This pseudo-parameter is only used when fitting the model and will be discarded afterwards; in that sense it is not a true parameter of the model.\nThe EM algorithm then proceeds iteratively, with each iteration being divided into two steps: the E-step and the M-step. I will describe these in broad strokes first, so you can get a feel for the overall intent of the algorithm, then we will study each in more detail in the following sections.\nIn the E-step, we use our current best knowledge of the centers and shapes of each cluster to update our estimates of which data point came from which class. Concretely, we hold \\(\\boldsymbol{\\mu}_j\\) and \\(\\Sigma_j\\) fixed and update \\(w_{ij}\\) and \\(\\boldsymbol{\\phi}\\).\nIn the M-step, we use our current best knowledge of which class each point belongs to to update and improve our estimates for the center and shape of each cluster. Concretely, we use \\(w_{ij}\\) as sample weights when updating \\(\\boldsymbol{\\mu}_j\\) and \\(\\Sigma_j\\) by taking weighted averages over \\(X\\). For example, if \\(w_11 = 0.01\\) and \\(w_11 = 0.99\\) we know that the data point \\(X_1\\) is unlikely to be in class 1, but very likely to be in class 2. Therefore, when estimating the center of the first class \\(\\boldsymbol{\\mu}_1\\) we give \\(X_1\\) almost negligible weight, but when estimating the center of the second class \\(\\boldsymbol{\\mu}_2\\) we give \\(X_1\\) almost full weight. This “pulls” the center of each cluster towards those data points which are considered likely to be part of that cluster.\nVisually, the iterative process looks something like this:\n\nWith each iteration, the algorithm improves its estimate of where the clusters are, which in turn allows it to make better guesses about which points are from which clusters, which in turn allows it to further refine its estimate of the center and shape of each cluster, and so on ad infinitum.\nThis process is guaranteed to converge a (local) maximum likelihood because of the ratchet principle: at each step, likelihood can only increase and never decrease. This can be viewed as a type of coordinate ascent. These maxima are not unique, and GMM will tend to converge to different final solutions depending on initial conditions.\nOne resource on GMM and the EM algorithm I used was this Stanford lecture by Andrew Ng. I’ve linked to the part of the lecture where he shows this update step because that is most relevant to implementing the algorithm but the whole lecture is worth watching if you want to understand the concepts. Another good resource on the fundamentals of the EM algorithm is this slide deck; it provides a simple example that can be worked by hand which I found to be a great way to build intuition before tackling the much more complicated problem of applying the EM algorithm to GMM.\nWe will now treat the E-step and M-step for the particular case of the GMM in detail.\n E-step Given the that centroid \\(\\boldsymbol{\\mu}_j\\) and covariance matrix \\(\\Sigma_j\\) for class \\(j\\) is fixed, we can update \\(w_{ij}\\) by simply calculating the probability that \\(X_i\\) came from each class and normalizing:\n\\[ w_{ij} = \\frac{ P(X_i|K=j) }{ P(K_i) } = \\frac{ P(X_i|K=j) }{ \\sum_{l=1}^k P(X_i|K=l) } \\tag{10} \\]\nThe conditional probablity \\(P(\\mathbf{X}_i|K=j)\\) is simply the multivariate normal distribution \\(\\mathbf{X}_i ~ \\mathcal{N}(\\mu_i, \\Sigma_i)\\) so we can use equation (4) above to calculate the probability density for each class, and then divide through by the total to normalize each row of \\(\\mathbf{X}\\) to 1. This gives us a concrete formula for the update to \\(w_ij\\):\n\\[ w_{ij} = \\frac{ f_{\\mathcal{N}(\\mu_i, \\Sigma_i)}(\\mathbf{X}_i) } { \\sum_{l=1}^k f_{\\mathcal{N}(\\mu_l, \\Sigma_l)}(\\mathbf{X}_i) } \\tag{11} \\]\nThe probability of each class \\(\\phi\\) can then be estimated by averaging over all examples in the training set:\n\\[ \\phi_j = \\sum_{i=1}^n w_{ij} \\tag{12} \\]\n M-step Forget about the past estimates we had for \\(\\boldsymbol{\\mu}_j\\) or \\(\\Sigma\\). Unlike gradient descent, the EM algorithm does not proceed by making small changes to the previous iteration’s parameter estimates - instead, it makes a bold leap all the way to the exact estimate - but only in certain dimensions. In the M-step, we will calculate the ML estimates for \\(\\boldsymbol{\\mu}_j\\) or \\(\\Sigma\\) assuming that \\(w_{ij}\\) is held constant.\nHow can we make such a leap? Well, we have a matrix of \\(n\\) observations \\(\\mathbf{X}_i\\) with weights \\(w_i\\) which we believe came from a multivariate distribution \\(\\mathcal{N}(\\vec{\\mu}, \\mathbb{\\Sigma})\\). That means we can use the familiar formulas:\n\\[ \\boldsymbol{\\mu}_j = {1 \\over {n}}\\sum_{i=1}^n w_{ij} \\mathbf{X}_i \\tag{13} \\]\n\\[ \\Sigma_j = \\frac{1}{n} \\sum_{i=1}^n w_{ij} ( \\mathbf{X}_i - \\boldsymbol{\\mu}_j )( \\mathbf{X}_i - \\boldsymbol{\\mu}_j )^T \\tag{14} \\]\nThese are in fact the ML estimate for these parameters for the multivariate normal distribution. As such, we don’t need to worry about learning rate or gradients as we would with gradient descent because these estimates are already maximal! This is one of the neatest things about this algorithm.\n Implementation Turning the above mathematics into a working implementation is straight forward. The below program corresponds almost one-to-one (one line of code for one equation) with the above mathematics. The equations (11), (12) are used in the e_step() method and equations (13) and (14) are used in the m_step() method.\nOne detail I did not treat above is initialization - while \\(\\boldsymbol{\\phi}\\) and \\(w_{ij}\\) can use simple uniform initialization, for \\(\\boldsymbol{\\mu}\\) it is better to choose a random index \\(i_j\\) uniformly from \\(1\\) to \\(n\\) for each class and then initialize \\(\\boldsymbol{\\mu}_j = X_{i_j}\\). This ensures that each cluster centroid is inside the support of the underlying distribution and that they are initially spread out randomly throughout the space.\nimport numpy as np from scipy.stats import multivariate_normal class GMM: def __init__(self, k, max_iter=5): self.k = k self.max_iter = int(max_iter) def initialize(self, X): self.shape = X.shape self.n, self.m = self.shape self.phi = np.full(shape=self.k, fill_value=1/self.k) self.weights = np.full( shape=self.shape, fill_value=1/self.k) random_row = np.random.randint(low=0, high=self.n, size=self.k) self.mu = [ X[row_index,:] for row_index in random_row ] self.sigma = [ np.cov(X.T) for _ in range(self.k) ] def e_step(self, X): # E-Step: update weights and phi holding mu and sigma constant self.weights = self.predict_proba(X) self.phi = self.weights.mean(axis=0) def m_step(self, X): # M-Step: update mu and sigma holding phi and weights constant for i in range(self.k): weight = self.weights[:, [i]] total_weight = weight.sum() self.mu[i] = (X * weight).sum(axis=0) / total_weight self.sigma[i] = np.cov(X.T, aweights=(weight/total_weight).flatten(), bias=True) def fit(self, X): self.initialize(X) for iteration in range(self.max_iter): self.e_step(X) self.m_step(X) def predict_proba(self, X): likelihood = np.zeros( (self.n, self.k) ) for i in range(self.k): distribution = multivariate_normal( mean=self.mu[i], cov=self.sigma[i]) likelihood[:,i] = distribution.pdf(X) numerator = likelihood * self.phi denominator = numerator.sum(axis=1)[:, np.newaxis] weights = numerator / denominator return weights def predict(self, X): weights = self.predict_proba(X) return np.argmax(weights, axis=1)  Model Evaluation We’ll use the famous iris dataset as a test case. This is the same dataset used as a motivating example at the beginning of the article, although I did not name it at that time. The iris dataset has labels, but we won’t expose them to the GMM model. However, we will use these labels in the next section to discuss the question, “were we able to discover the class labels through unsupervised learning?”\nfrom scipy.stats import mode from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt from sklearn.datasets import load_iris iris = load_iris() X = iris.data Fit a model:\nnp.random.seed(42) gmm = GMM(k=3, max_iter=10) gmm.fit(X) Plot the clusters. Each color is a cluster found by GMM:\ndef jitter(x): return x + np.random.uniform(low=-0.05, high=0.05, size=x.shape) def plot_axis_pairs(X, axis_pairs, clusters, classes): n_rows = len(axis_pairs) // 2 n_cols = 2 plt.figure(figsize=(16, 10)) for index, (x_axis, y_axis) in enumerate(axis_pairs): plt.subplot(n_rows, n_cols, index+1) plt.title(\u0026#39;GMM Clusters\u0026#39;) plt.xlabel(iris.feature_names[x_axis]) plt.ylabel(iris.feature_names[y_axis]) plt.scatter( jitter(X[:, x_axis]), jitter(X[:, y_axis]), #c=clusters, cmap=plt.cm.get_cmap(\u0026#39;brg\u0026#39;), marker=\u0026#39;x\u0026#39;) plt.tight_layout() plot_axis_pairs( X=X, axis_pairs=[ (0,1), (2,3), (0,2), (1,3) ], clusters=permuted_prediction, classes=iris.target) GMM Clusters\n Well, the model certainly found something.\nOne thing we can say for sure is that the GMM model does find clusters of related points. It does a particularly good job placing the visually separate points in their own (blue) cluster, but the story with the other two clusters in the upper right is less clear-cut.\n Comparing to True Class Labels Are the clusters discovered by the GMM model meaningful? Are they correct? For a real-world unsupervised learning problem, these questions can be hard to answer.\nHowever, it so happens that the iris dataset we used is actually labeled. True, we didn’t make use of these labels when training the GMM model. Furthermore, those classes are associated with different distributions in the 4 observed variables in a way that closely matches the assumptions of the GMM. So even if we can’t ask about “meaning” and “correctness”, we can at least ask a closely related question: “did this unsupervised learning algorithm (re-)discover the known structure of this (iris) data set?”\nFirst, a bit of book-keeping. The cluster indexes found by the model are in random order. For convenience when comparing them to true class labels, we will permute them to be as similar as possible to true class labels. All this is doing is swapping, say, 0 for 2 so that 0 means the same thing for both the clusters and for the original class labels. It’s not important, but it does make comparisons a little bit easier.\npermutation = np.array([ mode(iris.target[gmm.predict(X) == i]).mode.item() for i in range(gmm.k)]) permuted_prediction = permutation[gmm.predict(X)] print(np.mean(iris.target == permuted_prediction)) confusion_matrix(iris.target, permuted_prediction) 0.96 array([[50, 0, 0], [ 0, 44, 6], [ 0, 0, 50]]) For the random seed 42 (used above when the trained the GMM model) this results in the very promising 96% agreement! However, if we 1,000 random trials, varying the seed each time, we can see that cluster-to-labels agreement actually varies at random from 0.52 to 0.99 with a mean of 0.74.\nAccuracy Histogram\n This is a little disappointing. We started from a dataset which really was the aggregation of three different classes, and while our unsupervised learning algorithm was discover three clusters, the agreement between reality and our model is only around 3/4. That means we can’t reliably reconstruct the true structure of this dataset using this technique. In contrast, a supervised learning algorithm could have easily found a class boundary with an accuracy of 99%. That suggests that if we run an unsupervised learning algorithm on a real-world data set and it finds some clusters for us, we should be suspicious that they represent “true” classes in the real world. In fact, unsupervised learning algorithms are subject to a large number of caveats and limitations which I’ll digress briefly to enumerate.\n Limitations All unsupervised learning methods known today share certain limitations.\nFirst, they tend to rely on the researcher choosing certain arbitrary complexity parameters such as the number of clusters \\(k\\). Worse still, while there are techniques for picking these complexity parameters, they are heuristic and often unsatisfying in practice. It can be very hard to tell if an unsupervised learning method is “overfitting”, because “overfit” doesn’t even have a precise definition for unsupervised learning problems.\nSecond, there are no hard metrics like accuracy or AUC that let you compare models across different families. While each unsupervised learning algorithm will have its own internal metrics which they try to optimize such as variance explained or perplexity, these usually can’t be meaningful compare two models that use two different algorithms or with different complexity parameters. This makes model selection a fundamentally subjective task - to decide that, say, t-SNE is doing a better job than \\(k\\)-means on a given data set, the modeler is often reduced to eyeballing the output.\nThird and finally, the factors and/or clusters discovered by unsupervised learning algorithms are often unsatisfying or counter-intuitive and don’t necessarily line up with human intuition. Another way of saying the same thing is that if a human goes through and creates labels \\(\\mathbf{Y}\\) for the training set \\(\\mathbf{X}\\) after the unsupervised learning algorithm has been applied to it, they are not very likely to come up with the same factors or clusters. In general, humans tend to come up with rules that “make sense” but don’t explain as much variance as possible, while algorithms tend to find “deep” features that do explain a lot of variance but have complicated definitions that are hard to make sense of.\nThese seem like serious criticisms; does this mean we shouldn’t use unsupervised learning? Well, I won’t tell you that you categorically should never use it, but you should know what you’re getting into. By default, it tends to produce low-quality, hard-to-interpret models that cannot really be defended due to number of subjective decisions needed to make them work at all.\nOn the other hand, unsupervised learning can be extremely helpful during exploratory research; also, in the form of representation learning, it can sometimes accelerate learning or improve performance, or allow models to generalize from an extremely limited labeled training set. For example, a sentiment analysis model trained on only a few hundred reviews may only see the word “sterling” once, but if it uses a word embedding model like Word2Vec, it will understand that “stupendous” is broadly a synonym for “good” or “great”, and will therefore be able to correctly classify a future example with the word “stupendous” - which did not appear even once in the training set - as likely having positive sentiment. While success stories like this are possible, in general unsupervised learning requires more expertise, more manual tuning, and more input from domain experts in order to create value, when compared to supervised learning projects.\nUnfortunately, we do not always have the labels necessary for supervised learning, and the datasets available may be too large, too high dimensional, or too sparse to be amenable to traditional techniques; it is in these situations where the benefits of unsupervised learning can outweigh the negatives.\n Conclusion In this article, we have seen how unsupervised learning differs from supervised learning and the challenges that come along with that. We discussed a method for posing an unsupervised learning problem as an maximum likelihood optimization, and described and implemented the EM algorithm often used to solve these otherwise intractable problems. We made the EM algorithm concrete by implementing one particular latent variable model, the Gaussian mixture model, a powerful unsupervised clustering algorithm. We’ve seen first hand that the clusters identified by GMMs don’t always line up with what we believe the true structure to be; this lead to a broader discussion of the limitations of unsupervised learning algorithms and the difficulty getting value out of them.\nIn the next article in this series, we’ll continue our discussion of unsupervised learning algorithms by implementing the other kind of unsupervised learning algorithm besides clustering: a dimensionality reduction algorithm.\n ","date":"June 5, 2019","href":"https://www.oranlooney.com/post/ml-from-scratch-part-5-gmm/","thumbnail":"/post/ml-from-scratch-part-5-gmm_files/lead.192x128.jpg","title":"ML From Scratch, Part 5: Gaussian Mixture Models"},{"content":"Today, let me be vague. No statistics, no algorithms, no proofs. Instead,\rwe’re going to go through a series of examples and eyeball a suggestive\rseries of charts, which will imply a certain conclusion, without actually\rproving anything; but which will, I hope, provide useful intuition.\nThe premise is this:\n\rFor any given problem, there exists learned featured representations\rwhich are better than any fixed/human-engineered set of features, even once\rthe cost of the added parameters necessary to also learn the new features into account.\n\rThis is of course completely unoriginal: it is in in fact the standard just-so\rstory for machine learning that you hear again and again in different contexts.\rThe learned \\(k \\times k\\) kernels of a 2D CNN works better than Haar features or\rSobel filters. A one dimensional CNN and a spectrogram works better than MFCC.\rAn ensemble of decision stumps works better than a single large decision tree.\rAnd so on.\nEven as mythology, there are already plenty of cracks starting to show. The biggest\rand most well known caveat is of course, “given a sufficiently huge amount of\rtraining data.” There are often other caveats too, such as “and you don’t\rcare how slow it is.” For example, the Viola-Jones object detection framework\ruses Haar features and is still widely used because it is so much faster than\rCNN-based approaches, although not as robust or accurate.\rWhile cutting edge CNN-based object detectors are\rstarting to achieve acceptable runtime performance, they’ll probably never be as fast\ras Haar features, simply because they’ll never be able to take advantage of the\rintegral image trick.\nBut let’s zoom out at look at the big picture instead of shopping around for\rcounterexamples and limitations. At a high level, the story is clear. Again and\ragain, on various problems and algorithms,\rwe’ve seen that taking carefully engineered representations, often with very\rattractive mathematical properties which make them tractable for computation\rand mathematical analysis, and just throwing them out to start over from scratch with a\rlearned representation is a winning\rstrategy. It not only works, it often smashes straight through the performance\rceiling where the previous generation of models had\rplateaued. The history of the ImageNet challenge provides plenty of\rconcrete examples of that.\nBut these successes are on very large and complicated problems; is it possible\rto find a simpler example of the phenomenon, so that a student can actually\rwrap their head around it? Or does the phenomenon only manifest once the problem\rhits a certain threshold of complexity? I think it is possible to find\rsuch elementary problems that demonstrate the power of learned representations.\nOnce such example, which happily lends itself to easy visualization,\ris the problem of learning to approximate a one-dimensional function. To\rmake the case in favor of learned representations we will first attempt\rthe problem with several fixed representations and compare those attempts\rwith a learned representation.\nThe Function Approximation Problem\rGiven an i.i.d. random sample \\(\\{(Y_i, X_i)\\}_{i=1}^n\\) where \\(X\\) and \\(Y\\) have joint probability\rdistribution \\(F\\), we wish to find a real-valued function \\(f : \\mathbb{R} \\mapsto \\mathbb{R}\\) such that:\n\\[ E[Y|X] = f(x) \\]\nSince we only have access to the random sample, we cannot hope to find\ran exact solution, but can find the best (in terms of MSE) function from some family of functions \\(\\mathcal{F}\\):\n\\[ \\hat{f} = \\underset{f \\in \\mathcal{F} }{{\\text{argmin}}} \\sum_i^n (f(x) - E[Y|X])^2 \\tag{1} \\]\nThe question then becomes how we choose a family \\(\\mathcal{F}\\)\rthat makes this optimization problem tractable. The obvious answer is to\rparameterize \\(\\mathcal{F}\\) in terms of \\(k\\) real-valued parameters; then\rthe optimization problem is to find the minimum of the loss function \\(J : \\mathbb{R}^k \\mapsto \\mathbb{R}\\)\rwhich can be solved with standard techniques.\nThe standard way to define such a parameterization is to assume that \\(f\\) is\rthe weighted sum of \\(k\\) fixed basis functions \\(\\psi_1, ..., \\psi_k\\) and\rlet \\(\\mathcal{F} = \\text{span} \\{ \\psi_1, ..., \\psi_k \\}\\). Then, for any function \\(f \\in \\mathcal{F}\\),\rwe can always write \\(f\\) as a linear combination of basis functions:\n\\[ f(x) = \\sum_{i=j}^k \\beta_j \\psi_j(x) \\tag{2} \\]\nSubstituting (2) into (1) above, we have an explicit loss function:\n\\[ J(\\beta) = \\sum_{i=1}^n (E[Y|X] - \\sum_{j=1}{k} \\beta_j \\psi_j(x) )^2 \\tag{3} \\]\nWhen \\(J\\) is as small as possible, \\(\\hat{f}\\) is as close as possible to the target\rfunction \\(f\\) as it is possible for any function in \\(V\\) to be. We say that \\(\\hat{f}\\)\ris the best approximation of \\(f\\) for the given choice of basis functions \\(\\psi_1, ..., \\psi_N\\).\nWe will call the choice of parameters that minimize loss \\(\\hat{\\beta}\\) and the corresponding\rfunction \\(\\hat{f}\\):\n\\[ \\begin{align}\r\\hat{\\beta} \u0026amp; = \\text{argmin}_\\beta J(\\mathbf{\\beta}) \\tag{4} \\\\\r\\hat{f}(x) \u0026amp; = \\sum_{j=1}^k \\hat{\\beta}_j \\psi_j(x) \\tag{5}\r\\end{align}\r\\]\nWe won’t dwell too much today on the best way to actually solve this minimization problem\rbut instead just use an off-the-shelf solver to quickly (in terms of programmer time)\rget a workable solution. Instead, we’ll focus on how the choice of basis functions affects\rour ability to approximate a function.\n\rTarget Function\rFor the examples below, we’re going to need some target function \\(t(x) = E[Y|X]\\).\rIt should be continuous,\rbounded on some closed interval, and it’s also convenient if it’s approximately zero on\rboth edges. The unit interval \\([0, 1]\\) is as good a choice for the domain as any. To make the function\rappropriately ugly, we’ll take some polynomial terms plus some sinusoidal terms: that\rwill make it hard to approximate with either Fourier series or a splines, and also give\rus lots of nasty inflections.\nimport matplotlib\rimport matplotlib.pyplot as plt\r%matplotlib inline\rimport numpy as np\rimport math\rnp.warnings.filterwarnings(\u0026#39;ignore\u0026#39;)\rfrom scipy.optimize import minimize\rdef target_function(x):\rreturn x*(x-1) * (np.sin(13*x) + np.cos(23*x)*(1-x))\rx = np.linspace(start=0, stop=1, num=101)\ry = target_function(x) #+ np.random.normal(0, 0.1, size=x.shape)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Arbitrary Smooth Function\u0026quot;)\rplt.plot(x, y, label=\u0026quot;target\u0026quot;)\rplt.legend()\rTarget Function\n\rI wouldn’t say this function is pathological, but it’s juuust hard enough to be interesting.\n\rStep Function Basis\rTo get warmed up, let’s use the above basis function framework to calculate the best possible step\rfunction approximation of a function. Since our target function is continuous\rthis approach if fundamentally flawed but it illustrates the method.\nFirst, we will define a finite set of fixed basis functions:\nN_step = 20\rdef step_function(i):\rreturn lambda x: np.where(x \u0026gt; i/N_step, 1, 0)\rdef sum_of_step_functions(beta):\rdef f(x):\rtotal = np.zeros(shape=x.shape)\rfor i, b in enumerate(beta):\rtotal += step_function(i)(x) * b\rreturn total\rreturn f def square_function_distance(f, g):\rreturn np.sum( (f(x) - g(x))**2 )\rdef step_loss(beta):\rg = sum_of_step_functions(beta)\rreturn square_function_distance(target_function, g)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Step Function Basis\u0026quot;)\rfor i in range(N_step):\rplt.plot(x, step_function(i)(x))\rStep Basis\n\rIn this case, each basis function is a step function and the only\rdifference between them is the position at which the step occurs.\nTo construct our approximation, we choose the best coefficient for\reach basis function:\nbest = minimize(step_loss, x0=np.zeros(shape=N_step))\rbeta_hat = best.x\rif best.status != 0:\rprint(best.message)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Step Function Approximation\u0026quot;)\rplt.plot(x, y, label=\u0026#39;target\u0026#39;)\rplt.step(x, sum_of_step_functions(beta_hat)(x), label=\u0026#39;approx.\u0026#39;)\rplt.legend()\rprint(\u0026quot;best loss:\u0026quot;, step_loss(beta_hat))\r\rbest loss: 0.1127449592291812\n\rUnsurprisingly, this approximation is able to get reasonably close on\reach small interval but is ultimately hampered by its inability to\rrepresent slopes.\nStep Approximation\n\r\rFixed Sigmoid Basis Functions\rSince we know our target function is continuous, it makes sense\rto likewise choose continuous basis functions. Since the step function\rotherwise seem to have worked reasonably well, we’ll simply use a\rsmoothed version of the step function, the so-called sigmoid function.\ndef sigmoid_basis_function(i):\rreturn lambda x: 1/(1+np.exp((i- 10*x)/1.73))\rdef sum_of_sigmoid_functions(beta):\rdef f(x):\rtotal = np.zeros(shape=x.shape)\rfor i, b in enumerate(beta):\rtotal += sigmoid_basis_function(i)(x) * b\rreturn total\rreturn f def sigmoid_loss(beta):\rg = sum_of_sigmoid_functions(beta)\rreturn square_function_distance(target_function, g)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Fixed Sigmoid Basis\u0026quot;)\rfor i in range(10):\rplt.plot(x, sigmoid_basis_function(i)(x))\rSigmoid Basis\n\rNote that the functions in this basis are only distinguished by their offset.\nbest = minimize(sigmoid_loss, x0=np.zeros(shape=10))\rbeta_hat = best.x\rif best.status != 0:\rprint(best.message)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Fixed Sigmoid Approximation\u0026quot;)\rplt.plot(x, y, label=\u0026quot;target\u0026quot;)\rplt.plot(x, sum_of_sigmoid_functions(beta_hat)(x), label=\u0026quot;approx.\u0026quot;)\rplt.legend()\rprint(\u0026quot;best loss:\u0026quot;, sigmoid_loss(beta_hat))\r\rbest loss: 0.2857660082499814\n\rSigmoid Approximation\n\rWhile more visually appealing, this hasn’t really done better than the step function basis.\n\rOrthogonal Basis Functions\rFamilies of orthogonal functions have a key property that\rmakes them especially useful as basis functions: you can determine\rthe optimal coefficient \\(\\beta_j\\) without considering any of\rthe other elements of \\(\\mathbf{\\beta}\\).\nThe Fourier series is one well-known example. The basis functions\rare \\(sin(nx)\\) and \\(cos(nx)\\) for \\(n\u0026gt;0\\) plus the constant function.\ndef fourier_basis_function(i):\rif i == 0:\rreturn lambda x: np.full_like(x, 0.5)\relse:\rn = (i+1)//2\rif i % 2 == 1:\rreturn lambda x: np.sin(n*x)\relse:\rreturn lambda x: np.cos(n*x)\rdef sum_of_fourier_functions(beta):\rdef f(x):\rtotal = np.zeros(shape=x.shape)\rfor i, b in enumerate(beta):\rtotal += fourier_basis_function(i)(x) * b\rreturn total\rreturn f\rdef fourier_loss(beta):\rg = sum_of_fourier_functions(beta)\rreturn square_function_distance(target_function, g)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Fourier Basis\u0026quot;)\rfor i in range(5):\rtheta = x * 2 * math.pi\rplt.plot(theta, fourier_basis_function(i)(theta))\rplt.axhline(y=0, color=\u0026#39;k\u0026#39;, linewidth=1)\rThere are faster ways to compute the coefficients in the particular\rcase of the Fourier series, but we’ll just brute force like always\rfor consistencies sake.\nFourier Basis\n\rbest = minimize(fourier_loss, x0=np.zeros(shape=21))\rbeta_hat = best.x\rif best.status != 0:\rprint(best.message)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Fourier Approximation\u0026quot;)\rplt.plot(x, y, label=\u0026quot;target\u0026quot;)\rplt.plot(x, sum_of_fourier_functions(beta_hat)(x), label=\u0026quot;approx.\u0026quot;)\rplt.legend()\rprint(\u0026quot;best loss:\u0026quot;, fourier_loss(beta_hat))\r\rbest loss: 0.15528347938817644\n\rFourier Approximation\n\rThe fit isn’t particularly great, even using 21 parameters, which is equivalent\rto adding up ten sine waves each with different amplitudes and frequencies.\rThat’s to be expected: Fourier series are pretty bad at approximating polynomials, and\rare even worse at approximating functions with discontinuities.\nAn orthogonal family of basis functions can work really well when they are well\rsuited to your problem – for example, when the target function is known to be\rsolution to some differential equation, and each basis function is likewise a\rsolution to that same differential equation. But they are often a very poor\rchoice when we know little about the target function.\nWhile there might be a theoretical guarantee that we can approximate any\rfunction given an unlimited number of Fourier basis functions, this can require an\runreasonably large number of parameters before\ra good fit is achieved. But every parameter we add to the model increases our\rchances of overfitting! We need to look for a way of approximating functions\rwell while keeping the number of parameters under control.\n\rAdaptive Basis Functions\rFinally, we come to our ringer: the adaptive basis function. Within the\rcontext of function approximation, adaptive basis functions are a clear example\rof a learned representation. Kevin Murphy’s book has a good chapter on\radaptive basis function models, but a very simple working definition is that\rthey are models where the basis functions themselves are parameterized,\rand not just the weights out front.\nThe way to ensure that each basis function added to the model is\radding value and isn’t just dead weight is to give each basis function\rits own parameters, which we will learn in parallel with the coefficients.\rNote that this means we are leaving the additive assumption behind. While\rthe model may still superficially look like an additive model:\n\\[ f(x) = \\sum_{i=j}^N \\beta_j \\psi_j(x;\\theta_j) \\]\nEach \\(\\psi_j\\) is now a parameterized function rather than a fixed basis function.\rThis makes computing gradients much harder, and almost always means that the new\roptimization problem is no longer convex.\nThere is also a trade-off in the number of parameters used: while we have fewer \\(\\beta_j\\)\rparameters, we also have new \\(\\theta_j\\) parameters. Hopefully there will be some\rsweet spot where each adaptive basis function is doing the work of many fixed basis functions!\nA good choice for adaptive basis functions is the sigmoid. We can add parameters that shift\rit left or right, make it wider or narrower:\ndef learned_basis_function(bias, width):\rreturn lambda x: 1/(1+np.exp((bias - x)/width))\rdef sum_of_learned_functions(beta):\rbeta = beta.reshape( (beta.size//3,3) )\rdef f(x):\rtotal = np.zeros(shape=x.shape)\rfor i, b in enumerate(beta):\rtotal += learned_basis_function(b[1], b[2])(x) * b[0]\rreturn total\rreturn f def learned_basis_loss(beta):\rg = sum_of_learned_functions(beta)\rreturn square_function_distance(target_function, g)\rplt.figure(figsize=(16,10))\rplt.title(\u0026quot;Learned Sigmoid Basis\u0026quot;)\rfor i in [1, 3, 5, 7, 9]:\rbias = i/10\rfor width in [0.1, 0.2, 0.3]:\rplt.plot(x, learned_basis_function(bias, width)(x))\rHere is only a small sample of what is possible with these basis functions. In\rfact, there are infinitely many possible adaptive sigmoid functions -\ralthough we will be forced to choose just a small number (\\(k=7\\) below) to\rconstruct our approximation.\nLearned Sigmoid Basis\n\rNote that a very narrow sigmoid is basically a step function, while a very wide\rsigmoid is basically linear! It’s like we’re getting a two-for-one deal - the\rspace of possible functions \\(\\mathcal{F}\\) now includes the span of the sum\rof all possible step functions and all possible linear functions, as well as\rall the smooth sigmoid functions in between. This is a very robust representation\rthat should be able to model very complex real-world relationships.\nAlso note that \\(k=7\\) is not quite arbitrary – because we have 3 parameters\rper adaptive basis function, this is roughly the same number of parameters as\rthe above examples. At first it may seem like that’s not nearly enough.\rRecall that 7 fixed sigmoid functions did a very poor job! But\rremember, these are adaptive. During training, each of the seven can be shifted\rand scaled independently of the others. This allows the model to move each to\rthe perfect place where it can do the most good.\nk = 7\rbest_loss = float(\u0026#39;inf\u0026#39;)\rbeta_hat = np.zeros( shape=(k, 3) )\rfor iteration in range(10):\rbeta_zero = np.random.normal(0, 0.01, size=(k,3))\rbeta_zero[:, 1] = np.linspace(0, 1, k)\rbeta_zero[:, 2] = np.ones(shape=k) * 0.2\rprint(\u0026#39;fitting attempt\u0026#39;, iteration)\rbest = minimize(learned_basis_loss, x0=beta_zero)\rcandidate_beta = best.x.reshape( (k,3) )\rcandidate_loss = learned_basis_loss(candidate_beta)\rif candidate_loss \u0026lt; best_loss:\rbest_loss = candidate_loss\rbeta_hat = candidate_beta\rprint(\u0026#39;beta:\u0026#39;, beta_hat)\rprint(\u0026quot;best loss:\u0026quot;, learned_basis_loss(beta_hat))\rif best.status != 0:\rprint(best.message)\r\rbest loss: 0.00012518241681862751\n\rLearned Sigmoid Approximation\n\rOK, that went from zero to sixty pretty quickly. This is so absurdly good that\ryou have to squint to even see, yes, the blue line for the target is still\rthere, it’s just mostly covered by the orange line for the approximation.\rWe’re using roughly the same number of parameters as before, so why is this so\rmuch better?\nThe only answer that I can give you is that adaptive basis functions are an example\rof a learned representation, and that by picking (learning) 7 basis functions that were “perfect”\rfor this specific problem, we can build a much better model with just a handful of them.\nBut here’s the punchline: this representation – a linear combination of\radaptive sigmoid functions – is exactly the same as a neural network with one hidden layer.\nIn particular, a neural network with one input node, one hidden layer with a sigmoid activation function,\rand one output node with a linear activation function. In diagram form for \\(k = 7\\), each\rof the 21 gray connection lines corresponds to exactly one of the parameters of the model:\nNeural Network Architecture\n\r(Astute readers may notice a missing bias node on the hidden layer; that is because the above implementation\rdoes not in fact have a corresponding parameter. But is is a completely inconsequential difference\rand could have been easily added to the above code if it was in any way important.)\nThere is a famous universal approximation theorem for this exact\rnetwork architecture which states that\rthis type of neural network can approximate any continuous function on\r\\(\\mathbb{R}^n\\). This is an asymptotic result, so it doesn’t directly explain the\rincreased power of this model relative to say, the step function basis, but\rit sort of wiggles it’s eyebrows in that direction.\nWe’ve seen first-hand that not only can models on this form approximate an continuous\rfunction asymptotically as the number of hidden units becomes very large, they\rcan often do an excellent job with a limited number of parameters.\nI should point out that we didn’t use backpropagation to fit this model,\rbut for such a small model it hardly matters. In some sense, what we’ve done\rhere is pretend like it’s 1970 and train a neural network using older methods.\rHowever, backprop can be viewed purely as a trick to make training faster - we\rdon’t need it to understand or discuss the expressive power of this model.\nHere’s another problem: because the basis functions are in some sense\rinterchangeable, there’s nothing to stop us from swapping \\(i\\) and \\(j\\) and\rgetting an equivalent solution. When we have \\(N\\) basis functions, there are\r\\(N!\\) equivalent points. So not only is our function not convex, but it in fact\ris guaranteed to have at least \\(N!\\) distinct minima! Of course, this is not\ractually a problem, because any solution is equally good - they are all equal\rto the global minima. However, in addition to the \\(N!\\) distinct local optima\rintroduced by the symmetry in our representation, there can also be lots of\rlocal minima which are not as good. The decision to use a learned representation\ralmost always comes with a corresponding loss in convexity and consequently\rwe can no longer guarantee convergence to a global minima.\n\rConclusion\rDid I convince you that adaptive basis functions - and by extension learned representations in general -\r“just work?” I’m not\reven sure I convinced myself. I’m left with the nagging feeling that if I had just\rchosen a different set of basis functions, or spent more time thinking about\rthe specifics of the problem, I could have gotten the same performance. This\ris the trap of hand-engineered features - while of course you could spend\rendless time dreaming up and trying out new features, it’s not exactly a good\ruse of your time. Meanwhile, an adaptive algorithm running on some Tesla K80\rGPU can try billions of possibilities overnight.\nLearned representations allow us to have very flexible models that\rcan approximate essentially arbitrary functions with relatively few parameters.\rThere’s no question these models do “more with less” when compared to traditional\rmodels but this power didn’t come for free: we gave up convexity, we gave up linearity,\rwe gave up the ability to impose domain knowledge, we’ve given up some of our ability to\rreason about our models.\nThis, then, is the blessing and the curse of modern machine learning: adaptive\rbasis functions work. Learned representations work. This is a blessing because we\rcan reap the benefits of using them, and a curse because our lack of understanding hampers\rfuture progress.\nHopefully, this will be a temporary state of affairs. These models are now\rattracting a huge amount of attention. We’re learning, for example, that\rnon-convexity may be a non-issue in very high dimensions because local minima\rare in fact rather rare relative to saddle points. While saddle points can be a\rchallenge for gradient descent optimizers, the algorithm as a whole doesn’t\rtend to get permanently stuck in them the same way they can get stuck in local\rminima. There is also some empirical evidence that the existence of lots of\rlocal minima is not really a practical problem if most local minima achieve\rperformance equal to – or very close to – the global minima.\nIt would be nice to see these somewhat scattered observations coalesce into\rsome nice theorems. Today, we don’t yet have results as strong as the\runiversal approximation theorem and much of the work so far as been\rhighly empirical. But it’s also important to remember it’s only been in the\rlast ten years that the importance of these kinds of models has been\rrecognized. Hopefully someday a deep convolutional neural network will be\ras well understood as linear regression is today, but even if research\rproceeds at the same pace, this could easily take over a hundred years.\n\r","date":"May 21, 2019","href":"https://www.oranlooney.com/post/adaptive-basis-functions/","thumbnail":"/post/adaptive-basis-functions_files/lead.192x128.jpg","title":"Adaptive Basis Functions"},{"content":"So far in this series we’ve followed one particular thread: linear regression\r-\u0026gt; logistic regression -\u0026gt; neural network. This is a very natural progression of\rideas, but it really represents only one possible approach. Today we’ll switch\rgears and look at a model with completely different pedigree: the decision\rtree, sometimes also referred to as Classification and Regression\rTrees, or simply CART models. In contrast to the earlier progression,\rdecision trees are designed from the start to represent non-linear features and\rinteractions.\nIn this article, we will be looking at only one algorithm for fitting trees\rto data: the greedy recursive partitioning algorithm. In future\rarticles, we will also look at various algorithms that combine many decision\rtrees to create state-of-the-art classifiers, but today let’s just build\ra solid foundation.\nThe recursive partitioning algorithm is very intuitive. We start by finding\ra single feature and a single split point which divides our data in two. This\ris a rule of the form:\n\\[\rX_i \u0026lt; C\r\\]\nAll of the training data for which this rule is true we place in the left\rsubset; and everything else in the right subset. Dividing a set into\rnon-overlapping subsets so that the union of the sets is the original set\ris called a partition. We then recursively apply the same algorithm\rto both the left and right subset. Hence, recursive partitioning.\nWe aren’t choosing these features and split points randomly – rather, we\rchoose them to maximize some condition, which can be informally understood as\rmaking both subsets less balanced than the original. (We will formalize this below.)\rIf one side has only positive classes and the other only has negative,\rthen that’s perfect and we’re done. Usually, though, a single rule can only\rincrease the imbalance a small amount. That’s OK, because it still helps improve our prediction\ra little. But if we want a good prediction, we’ll have to use more than one rule.\nThe way we do that is as follows. As we continue to apply the algorithm\rrecursively, we grow a binary tree structure where each node contains a rule of\rthe form \\(X_i \u0026lt; C\\), although \\(i\\) and \\(C\\) will be different for each node. For a\rnew data point, we can than start at the root node and trace a path down to a\rleaf node by taking the left fork when the condition is true, and the right\rfork when it is false. When we reach a leaf node, we can count how many\rtraining examples for that leaf node were in which class, and then predict the\rnew data point has the most common class. In this way, we can reach a\rdecision by following the logic of the tree. Hence decision tree.\nIn the next few sections, we’ll develop these ideas in detail. Note that\runlike the previous articles in this series, this is not motivated by statistics\rbut belongs to what Leo Breiman, inventor of the random forest algorithm based\ron decision trees, calls the “algorithmic culture” of statistical modeling.\nGini Impurity and Information Gain\rThe first thing we need to pin down is what we mean by “better” and “less\rbalanced.” There are two competing definitions; I’ll describe both and then\rpick one to use for the implementation.\nThe first, Gini impurity, is defined as the probability that a randomly chosen element\rwill be misclassified if we randomly choose a prediction from the distribution\rof classes itself.\n\\[\rI_G(\\mathbf{p}) = \\sum_{i} \\mathbf{p}_i \\sum_{j \\neq i} \\mathbf{p}_j \\]\nIn the binary case, we usually simplify the notation and refer\rto \\(\\mathbf{p}_1 = p\\) and \\(\\mathbf{p}_2 = 1-p\\), yielding this\rmuch simpler expression:\n\\[\r\\begin{align}\rI_G(p) \u0026amp; = p(1-p) + (1-p)p = 2p(1-p) \\\\\r\\Delta I_G \u0026amp; = I_{\\text{parent}} - p_{\\text{left}} I_{\\text{left}} - p_{\\text{right}} I_{\\text{right}}\r\\end{align}\r\\]\nInterestingly enough, this is not how we will actually make predictions –\rwe will in fact always choose the most likely class.\nAn example can make this much more concrete. Suppose we have 100 observations\rwith two balanced classes; exactly 50 each. If our decision tree consists of\ronly a root, we have to make the constant prediction 0.5 for all possible\rinputs, so our Gini impurity is 0.5. But once we split the root node, we’ll\rhave two leafs, and these will unbalanced classes, say 40/10 and 10/40. The\rGini impurity of these two nodes is \\(2 \\times .2 \\times .8 = .32\\). When we make\ra prediction, every input will be assigned to one of these two leaf nodes. If\rit ends up in the left node, we will predict class 0 and be right 80% of the\rtime. If it ends up in the right node, we will predict class 1 and still be\rright 80% of the time. Accuracy has improved from 50% to 80%. However, if\rinstead of always choosing the most likely class, we instead made our\rpredictions randomly, sampling from a Bernoulli distribution with parameter \\(p = 0.2\\) for the left node and \\(p = 0.8\\) for the right node, we would be right\r68% of time. This is the converse of Gini impurity. It’s a trivial theorem that\rwe can always minimize error and maximum accuracy by choosing the most likely\rclass, so that’s what we’ll use for decision rule. But Gini impurity does a\rbetter job of capturing / representing how much we’ve learned, so that’s what\rwe’ll use to train.\nSecond, there’s a competing metric called entropy which is often\rused instead of Gini impurity. There’s a long derivation where entropy is\rdefined as the log of the probability mass function of a multinomial\rdistribution, information is defined as the opposite of entropy, and\rinformation gain is defined as the difference between the entropy of the parent\rnode and the weighted sum of the entropies of all child nodes, but long story\rshort, for the simple case with just two classes the formula looks like this:\n\\[\r\\begin{align}\rH \u0026amp; = -p \\ln p - (1-p) \\ln (1-p) \\\\\r\\Delta H \u0026amp; = H_{\\text{parent}} - p_{\\text{left}} H_{\\text{left}} - p_{\\text{right}} H_{\\text{right}}\r\\end{align}\r\\]\nBoth functions are symmetric about the line \\(x = 0.5\\) and both are strongly\rconcave. This turns out to be very important, because it means it’s always possible\rto choose a good cut point. Other metrics, such as accuracy, don’t have this property\rbut instead give the exact same “goodness” score for many different candidate splits.\nModulo a scaling factor, entropy has almost the same shape as the Gini impurity:\nThese small differences in shape don’t usually result in different decisions\rabout which feature to choose or where to make the split, so decision trees\rtrained on one or the other will often be identical and usually have identical\rperformance. We will use Gini impurity because it is slightly cheaper to\rcalculate a square than a log.\n\rFinding The Best Cut Point\rAt each stage, we have two decisions to make: which feature to use\rfor the cut, and the exact value to cut out. Each rule is of the\rform\n\\[\rX_i \\leq C\r\\]\nWhile it would be possible to simply brute force our way through all possible\rcut points, calculating Gini impurity from scratch each and every time, this is\rhugely slower than a more efficient (but slightly harder to understand)\rvectorized algorithm. We, of course, will choose the path of most resistance\rand highest performance.\ndef best_split_point(X, y, column):\r# sorting y by the values of X makes\r# it almost trivial to count classes for\r# above and below any given candidate split point. ordering = np.argsort(X[:,column])\rclasses = y[ordering]\r# these vectors tell us how many of each\r# class are present \u0026quot;below\u0026quot; (to the left)\r# of any given candidate split point. class_0_below = (classes == 0).cumsum()\rclass_1_below = (classes == 1).cumsum()\r# Subtracting the cummulative sum from the total\r# gives us the reversed cummulative sum. These\r# are how many of each class are above (to the\r# right) of any given candidate split point.\r#\r# Because class_0_below is a cummulative sum\r# the last value in the array is the total sum.\r# That means we don\u0026#39;t need to make another pass\r# through the array just to get the total; we can\r# just grab the last element. class_0_above = class_0_below[-1] - class_0_below\rclass_1_above = class_1_below[-1] - class_1_below\r# below_total = class_0_below + class_1_below\rbelow_total = np.arange(1, len(y)+1)\r# above_total = class_0_above + class_1_above\rabove_total = np.arange(len(y)-1, -1, -1)\r# we can now calculate Gini impurity in a single\r# vectorized operation. The naive formula would be:\r#\r# (class_1_below/below_total)*(class_0_below/below_total)\r# # however, divisions are expensive and we can get this down\r# to only one division if we combine the denominator term.\rgini = class_1_below * class_0_below / (below_total ** 2) + \\\rclass_1_above * class_0_above / (above_total ** 2)\rgini[np.isnan(gini)] = 1\r# we need to reverse the above sorting to\r# get the rule into the form C_n \u0026lt; split_value. best_split_rank = np.argmin(gini)\rbest_split_gini = gini[best_split_rank]\rbest_split_index = np.argwhere(ordering == best_split_rank).item(0)\rbest_split_value = X[best_split_index, column]\rreturn best_split_gini, best_split_value, column\r\rBuilding the Tree\rThe fundamental building block of a tree is the “Node.” In our implementation,\revery node starts life as a leaf node, but when it the .split() method is\rinvoked, it mutates into a branch node with two new leaf nodes underneath. The\rsplit is made by calculating the optimal split point for each feature, then\rchoosing the feature and split point which minimizes Gini impurity. This\rcontinues recursively for both children until a node is perfectly pure or the\rmaximum depth parameter is reached.\nclass Node:\rdef __init__(self, X, y):\rself.X = X\rself.y = y\rself.is_leaf = True\rself.column = None\rself.split_point = None\rself.children = None\rdef is_pure(self):\rp = self.probabilities()\rif p[0] == 1 or p[1] == 1:\rreturn True\rreturn False\rdef split(self, depth=0):\rX, y = self.X, self.y\rif self.is_leaf and not self.is_pure():\rsplits = [ best_split_point(X, y, column) for column in range(X.shape[1]) ]\rsplits.sort()\rgini, split_point, column = splits[0]\rself.is_leaf = False\rself.column = column\rself.split_point = split_point\rbelow = X[:,column] \u0026lt;= split_point\rabove = X[:,column] \u0026gt; split_point self.children = [\rNode(X[below], y[below]),\rNode(X[above], y[above])\r]\rif depth:\rfor child in self.children:\rchild.split(depth-1)\rWe will will also make our Node class responsible for predicting probabilities\r(but not classes.) To obtain predictions from a branch node, we simply use the\rlearned rule to decide whether to descend to the left or right child. When we\rreach a leaf, we can return a probability based on the proportion of classes in\rthe leaf.\ndef probabilities(self):\rreturn np.array([\rnp.mean(self.y == 0),\rnp.mean(self.y == 1),\r])\rdef predict_proba(self, row):\rif self.is_leaf:\rreturn self.probabilities()\relse:\rif row[self.column] \u0026lt;= self.split_point:\rreturn self.children[0].predict_proba(row)\relse:\rreturn self.children[1].predict_proba(row)\rThis prediction step can also be vectorized by applying a separate vectorized\rfilter for each leaf node. However, in a tree of depth \\(k\\), this requires\rcalculating \\(2^k\\) separate filters, each comprised of the logical AND of\r\\(k\\) separate comparisons. This is not usually faster than just applying\rthe rules row-by-row.\n\rInterface\rThe above Node class can be used directly to fit models but as we’ve done\relsewhere in the series we give our model a user-friendly, scikit-learn style\rinterface. The class keeps track of only a single “root” Node, and relies on\rthat root node’s recursive .split() and .predict_proba() methods to reach\rdeeper nodes.\nclass DecisionTreeClassifier:\rdef __init__(self, max_depth=3):\rself.max_depth = int(max_depth)\rself.root = None\rdef fit(self, X, y):\rself.root = Node(X, y)\rself.root.split(self.max_depth)\rdef predict_proba(self, X):\rresults = []\rfor row in X:\rp = self.root.predict_proba(row)\rresults += [p]\rreturn np.array(results)\rdef predict(self, X):\rreturn (self.predict_proba(X)[:, 1] \u0026gt; 0.5).astype(int)\r\rTesting\rThe scikit-learn breast cancer dataset is a good choice for testing decision\rtrees because it is high dimensional and highly non-linear.\n# a small classification data set with 30 to get with. breast_cancer = load_breast_cancer()\rX = breast_cancer.data\ry = breast_cancer.target\rmodel = DecisionTreeClassifier(max_depth=4)\rmodel.fit(X, y)\ry_hat = model.predict(X)\rp_hat = model.predict_proba(X)[:,1]\rThe models out-of-the-box (by “out-of-the-box” I mean, “without need for\rhyper-parameter selection via cross-validation”) performance is quite good:\nprint(confusion_matrix(y, y_hat))\rprint(\u0026#39;Accuracy:\u0026#39;, accuracy_score(y, y_hat))\rTrue Class\rP N\rPredicted P 193 19\rClass N 18 339\rAccuracy: 0.9349736379613357\rThis confusion matrix and accuracy are only part of the story - in particular, they are\rperformance we see if we choose to define a positive test result as \\(p \u0026gt; 0.5\\). We can get\ra broader view the models performance over a range of possible thresholds with an ROC curve:\nROC Curve\n\rAn AUC of .96 is pretty respectable.\nWe can also look at the results as a function of the predictor variable \\(X\\). Since there are 30 separate\rfeatures, we will just look at a representative sample. For each pair of predictor variables, we’ll plot\rtrue positives in green, true negatives in blue, and misses in red.\nplt.figure(figsize=(16,30))\rmarkers = [\u0026#39;o\u0026#39;, \u0026#39;x\u0026#39;]\rred = (1, 0.2, 0.2, 0.5)\rgreen = (0.3, 0.9, 0.3, 0.3)\rblue = (0.2, 0.4, 0.8, 0.3)\rfor i in range(28):\rplt.subplot(7, 4, i+1)\rfor cls in [0, 1]:\rmask = (y == cls) \u0026amp; (y == y_hat)\rplt.scatter(\rx=X[mask,i], y=X[mask,i+1], c=[blue if positive else green for positive in y[mask]],\rmarker=markers[cls]\r)\rmask = (y == cls) \u0026amp; (y != y_hat)\rplt.scatter(\rx=X[mask,i], y=X[mask,i+1], c=red,\rmarker=markers[cls],\rzorder=10\r)\rDecision Tree Pairs\n\rA simple text-based visualization of our tree can be done by\radding a formatted() method to the Node() class:\ndef formatted(self, indent=0):\rif self.is_leaf:\rs = \u0026quot;Leaf({p[0]:.3f}, {p[1]:.3f})\u0026quot;.format(p=self.probabilities())\relse:\rs = \u0026quot;Branch(X{column} \u0026lt;= {split_point})\\n{left}\\n{right}\u0026quot;.format(\rcolumn=self.column, split_point=self.split_point,\rleft=self.children[0].formatted(indent+1),\rright=self.children[1].formatted(indent+1))\rreturn \u0026quot; \u0026quot; * indent + s\rdef __str__(self):\rreturn self.formatted()\rdef __repr__(self):\rreturn str(self)\rThe breast cancer decision tree has the following structure,\rwhere greater indentation corresponds to greater depth in the tree.\nBranch(X22 \u0026lt;= 89.04)\rBranch(X6 \u0026lt;= 0.0)\rLeaf(0.000, 1.000)\rBranch(X16 \u0026lt;= 0.0009737)\rLeaf(0.000, 1.000)\rBranch(X16 \u0026lt;= 0.001184)\rLeaf(0.000, 1.000)\rBranch(X19 \u0026lt;= 0.004651)\rLeaf(0.013, 0.987)\rLeaf(0.000, 1.000)\rBranch(X22 \u0026lt;= 96.42)\rBranch(X6 \u0026lt;= 0.004559)\rLeaf(0.000, 1.000)\rBranch(X6 \u0026lt;= 0.01063)\rLeaf(0.000, 1.000)\rBranch(X9 \u0026lt;= 0.05913)\rLeaf(0.059, 0.941)\rLeaf(0.086, 0.914)\rBranch(X26 \u0026lt;= 0.3169)\rBranch(X22 \u0026lt;= 117.7)\rBranch(X14 \u0026lt;= 0.006133)\rLeaf(0.109, 0.891)\rLeaf(0.273, 0.727)\rBranch(X19 \u0026lt;= 0.002581)\rLeaf(0.875, 0.125)\rLeaf(1.000, 0.000)\rBranch(X20 \u0026lt;= 27.32)\rBranch(X20 \u0026lt;= 27.32)\rLeaf(0.902, 0.098)\rLeaf(0.000, 1.000)\rLeaf(1.000, 0.000)\rNote that several paths down the tree lead to immediately to large, totally\rpure leaf nodes. That’s because in this particular dataset, there are large\rregions of the input space which can be unambiguously classified. However, as\rwe get closer to the true decision boundary, the predictions become more\rprobabilistic, and we may only be able to say that perhaps 87.5% of cases will\rbe negative.\n\rConclusion\rToday we saw a simple and intuitive algorithm tackle a difficult, highly\rnon-linear problem and achieve surprisingly good out-of-the-box performance\rafter only a few seconds of training time.\nUnfortunately, decision trees are exponentially data-hungry: to further improve\rperformance (without overfitting) we would need to add more nodes to our model,\rbut each layer that we add more than doubles the amount of data we need before\rour leaves have too few data points to reliably split. On this small dataset,\rwe can’t go beyond three or four layers.\nAnother issue is that the decision boundary of a decision tree is a series of\rorthogonal, axis-aligned hyperplanes. This is rarely a well-motivated boundary\r– the real world contains diagonals and curves! – and as such would not be\rexpected to generalize well. With a very deep tree, a diagonal or curved\rboundary can be approximated, yet this can require a large amount of data close\rto the decision boundary. However, decision trees can do very well when given\rdiscrete features.\nThe problems with decision trees stem from the fact they “believe” that the\rleft hand should not know what the right hand is doing – yet in many cases it\rwould make sense to pick the same decision rule for both sides of a decision\rtree. In fact, while decision trees are occasionally used directly on datasets,\rtheir real importance is in their use as the main ingredient in two\rstate-of-the-art ML algorithms that do exactly this! Both random forest\rand extreme gradient boosting are examples of additive models built by\rcombining different trees together. This allows them to have many different\rpartially overlapping regions. We will look at these models in a future\rarticle; for now, let me just mention that there are some good arguments\rthat suggest that any set of weak learners can be turned into a\rstrong leaner when combined together in the right way, and these\rpractical algorithms appear to “work” because of these deeper theorems.\n\r","date":"March 1, 2019","href":"https://www.oranlooney.com/post/ml-from-scratch-part-4-decision-tree/","thumbnail":"/post/ml-from-scratch-part-4-decision-tree_files/lead.192x128.jpg","title":"ML From Scratch, Part 4: Decision Trees"},{"content":"A common example of recursion is the function to calculate the \\(n\\)-th Fibonacci number:\ndef naive_fib(n):\rif n \u0026lt; 2:\rreturn n\relse:\rreturn naive_fib(n-1) + naive_fib(n-2)\rThis follows the mathematical definition very closely but it’s performance is\rterrible: roughly \\(\\mathcal{O}(2^n)\\). This is commonly patched up with dynamic\rprogramming. Specifically, either the memoization:\nfrom functools import lru_cache\r@lru_cache(100)\rdef memoized_fib(n):\rif n \u0026lt; 2:\rreturn n\relse:\rreturn memoized_fib(n-1) + memoized_fib(n-2)\ror tabulation:\ndef table_fib(n):\rif n \u0026lt; 2:\rreturn n\rtable = [-1] * (n+1)\rtable[0] = 0\rtable[1] = 1\rfor i in range(_2, n+1):\rtable[i] = table[i-1] + table[i-2]\rreturn table[n]\rObserving that we only ever have to use the two most recent Fibonacci numbers,\rthe tabular solution can easily be made iterative, resulting in a large space\rsavings:\ndef iterative_fib(n):\rprevious, current = (0, 1)\rfor i in range(2, n+1):\rprevious, current = (current, previous + current)\rreturn current\rAnd that, oddly enough, is often where it stops. For example, this presentation\rof solving the Fibonacci sequence as an interview question presents the\rabove two solutions and then… nothing. Not so much as an off-hand mention\rthat better solutions might exist. Googling around, I got the impression this\ris a fairly common (but by no means universal) misconception, perhaps because\rteachers use the Fibonacci function to illustrate the idea of dynamic\rprogramming but are not interested in spending too much time going too far into\rthe specifics of the mathematics.\nWhich is a shame, because it only gets more interesting the deeper we go.\nI should also clarify that we are particularly interested in calculating\rlarge Fibonacci numbers - say, the one-millionth or one-billionth.\nFair warning: this is a bit of rabbit hole, with no other purpose than to\roptimize the hell out something for which there is frankly no practical use.\rBut we get to do a bit of linear algebra and try out some pretty interesting\roptimization techniques; that’s what I call a good time!\nMatrix Form\rThere exist several closed-form solutions to Fibonacci sequence which gives\rus the false hope that there might be an \\(\\mathcal{O}(1)\\) solution. Unfortunately\rthey all turn out to be non-optimal if you want an exact solution for a large \\(n\\).\rWe will use to so-called “matrix form” instead, which we will now describe in some detail.\nRecall that the \\(n\\)-th Fibonacci number is given by the recurrence relation:\n\\[\r\\begin{align}\rF_0 \u0026amp;= 0 \\\\\rF_1 \u0026amp;= 1 \\\\\rF_n \u0026amp;= F_{n-1} + F_{n-2}\r\\end{align}\r\\]\nDefine the first Fibonacci matrix to be:\n\\[\r\\mathbf{F}_1 = \\begin{bmatrix}\r1 \u0026amp; 1 \\\\\r1 \u0026amp; 0 \\end{bmatrix}\r\\]\nAnd define the \\(n\\)-th Fibonacci matrix to be the \\(n\\)-th power:\n\\[\r\\mathbf{F}_n = \\mathbf{F}_1^n\r\\]\nI didn’t just pluck this out of thin air - there’s a general way\rto turn any linear recurrence relation into a matrix which I’ll\rdescribe in a moment. But first let’s prove the following theorem, which\rjustifies our definition:\n\\[\r\\forall n \\in \\mathbb{N}, \\mathbf{F}_n = \\begin{bmatrix}\rF_{n+1} \u0026amp; F_n \\\\\rF_n \u0026amp; F_{n-1}\r\\end{bmatrix}\r\\]\nWe proceed by induction. For the case of \\(n = 1\\), the theorem is true by inspection because we know \\(F_0 = 0\\) and \\(F_1 = F_2 = 1\\).\nSuppose it is true for \\(n-1\\). Then we have:\n\\[\r\\mathbf{F}_n = \\mathbf{F}_1^{n} = \\mathbf{F}_1^{n-1} \\mathbf{F}_1 = \\begin{bmatrix}\rF_n \u0026amp; F_{n-1} \\\\\rF_{n-1} \u0026amp; F_{n-2}\r\\end{bmatrix}\r\\begin{bmatrix}\r1 \u0026amp; 1 \\\\\r1 \u0026amp; 0 \\end{bmatrix}\r\\]\nMultiplying these two matrices, we have:\n\\[\r\\mathbf{F}_n = \\begin{bmatrix}\rF_n + F_{n-1} \u0026amp; F_{n} \\\\\rF_{n-1} + F_{n-2} \u0026amp; F_{n-1} \\end{bmatrix}\r\\]\nWe can use the Fibonacci definition twice (once for each element of the first column) to\rget:\n\\[\r\\mathbf{F}_n = \\begin{bmatrix}\rF_{n+1} \u0026amp; F_{n} \\\\\rF_{n} \u0026amp; F_{n-1} \\end{bmatrix}\r\\]\nTherefore if the theorem is true for \\(n-1\\), it is also true for \\(n\\). We have\ralready shown it is true for \\(n = 1\\), so by mathematical induction it is true\rfor all \\(n \\geq 1\\). Q.E.D.\nA brief word about where this matrix representation came from. Wikipedia has a\rgood explanation for how any linear recurrence relation can be\rexpressed in matrix form and I’ve described it myself in a prior\rarticle. Essentially, we use the first dimension to store the current\rvalue, and the rest of the vector as shift registers to “remember”\rprevious states. The recurrence relation is encoded along the first row and the\rones along the subdiagonal roll the history forward. It’s actually easier\rto see in higher dimensions, so here’s an example of encoding a linear\rrecurrence relationship which uses the four most recent numbers instead of just\rtwo:\n\\[\ry_{n+1} = c_0 y_n + c_1 y_{n-1} + c_2 y_{n-2} + c_3 y_{n-3} \\\\ \\iff \\\\\r\\begin{bmatrix}\ry_{n+1} \\\\\ry_{n} \\\\\ry_{n-1} \\\\\ry_{n-2} \\\\\ry_{n-3} \\\\\r\\end{bmatrix} = \\begin{bmatrix}\rc_0 \u0026amp; c_0 \u0026amp; c_1 \u0026amp; c_2 \u0026amp; c_3 \\\\\r1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\ry_n \\\\\ry_{n-1} \\\\\ry_{n-2} \\\\\ry_{n-3} \\\\\ry_{n-4} \\\\\r\\end{bmatrix}\r\\]\nIf we squint at \\(\\mathbf{F}_1\\), we can see it has this form too:\rthe first row is \\([ 1 \\,\\, 1 ]\\) because recurrence relation is simply the\rsum of the previous two, while the second row \\([ 1 \\,\\, 0 ]\\) contains the \\(1\\) on the\rsubdiagonal which “remembers” the previous value. The effect is\rto advance the state of the algorithm in almost the exact same way as the\rinterative_fib() above:\n\\[\r\\begin{bmatrix}\r1 \u0026amp; 1 \\\\\r1 \u0026amp; 0 \\end{bmatrix}\r\\begin{bmatrix}\rF_n \\\\\rF_{n-1}\r\\end{bmatrix}\r=\r\\begin{bmatrix}\rF_n + F_{n-1} \\\\\rF_{n}\r\\end{bmatrix}\r=\r\\begin{bmatrix}\rF_{n+1} \\\\\rF_{n}\r\\end{bmatrix}\r\\]\nAt first this may not seem at all helpful. But by framing the problem as taking\rthe exponent of a matrix instead of repeated addition, we can derive two much\rfaster algorithms: a constant time \\(\\mathcal{O}(n)\\) approximate solution using\reigenvalues, and a fast \\(\\mathcal{O}(n \\log n)\\) exact solution.\n\rEigenvalue Solution\rNote that the matrix \\(\\mathbf{F}_1\\) is symmetric and real-valued. Therefore it\rhas real eigenvalues which we’ll call \\(\\lambda_1\\) and \\(\\lambda_2\\). The eigenvalue\rdecomposition allows us to diagonalize \\(\\mathbf{F}_1\\) like so:\n\\[\r\\mathbf{F}_1 = \\mathbf{Q} \\mathbf{\\Lambda}\r\\mathbf{Q}^T\r=\r\\mathbf{Q} \\begin{bmatrix}\r\\lambda_1 \u0026amp; 0 \\\\\r0 \u0026amp; \\lambda_2\r\\end{bmatrix}\r\\mathbf{Q}^T\r\\]\nWriting \\(\\mathbf{F}_1\\) in this form makes it easy to square it:\n\\[\r\\begin{align}\r\\mathbf{F}_1^2 \u0026amp; = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T \\\\\r\u0026amp; = \\mathbf{Q} \\mathbf{\\Lambda}^2 \\mathbf{Q}^T \\\\\r\u0026amp; = \\mathbf{Q} \\begin{bmatrix}\r\\lambda_1^2 \u0026amp; 0 \\\\\r0 \u0026amp; \\lambda_2^2\r\\end{bmatrix}\r\\mathbf{Q}^T\r\\end{align}\r\\]\nor to raise it to an arbitrary power:\n\\[\r\\mathbf{F_n} = \\mathbf{F}_1^n = \\mathbf{Q} \\mathbf{\\Lambda}^n \\mathbf{Q}^T = \\mathbf{Q} \\begin{bmatrix}\r\\lambda_1^n \u0026amp; 0 \\\\\r0 \u0026amp; \\lambda_2^n\r\\end{bmatrix}\r\\mathbf{Q}^T\r\\]\nWe can calculate the two eigenvalues analytically by solving the\rcharacteristic equation \\((1-\\lambda)\\lambda - 1 = 0\\). Since this is a quadratic\rpolynomial, we can use the quadratic equation to obtain both solutions in closed form:\n\\[\r\\begin{align}\r\\lambda_1 \u0026amp; = \\frac{1 + \\sqrt{5}}{2} \\\\\r\\lambda_2 \u0026amp; = \\frac{1 - \\sqrt{5}}{2} \\end{align}\r\\]\nWhere the largest eigenvalue is in fact \\(\\phi\\), the golden ratio. The\rmatrix formulation is an easy way to see famous connection between the\rFibonacci numbers and \\(\\phi\\). To calculate \\(F_n\\) for large values of \\(n\\), it\rsuffices to calculate \\(\\phi^n\\) and then do some constant time \\(\\mathcal{O}(1)\\)\rbookkeeping, like so:\nimport numpy as np\rdef eigen_fib(n):\rF1 = np.array([[1, 1], [1, 0]])\reigenvalues, eigenvectors = np.linalg.eig(F1)\rFn = eigenvectors @ np.diag(eigenvalues ** n) @ eigenvectors.T\rreturn int(np.rint(Fn[0, 1]))\rSo there you have it – a \\(\\mathcal{O}(1)\\) algorithm for any Fibonacci number.\rThere’s just one tiny little problem with it: \\(\\phi\\), being irrational, is not\rparticularly convenient for numerical analysis. If we run the\rabove Python program, it will use 64-bit floating point arithmetic and will\rnever be able to precisely represent more than 15 decimal digits. That only\rlets us calculate up to \\(F_{93}\\) before we no longer have enough precision to exactly represent it. Past \\(F_{93}\\), our\rclever little “exact” eigenvalue algorithm is good for nothing but a rough\rapproximation!\nNow, we could use a high precision rational numbers, but that approach turns out\rto always require strictly more space and time that just sticking to integers.\rSo, abandoning the eigenvalue approach on the garbage heap of ivory tower\rtheory, let’s turn our attention to simply calculating the powers of an integer\rmatrix.\n\rFast Exponentiation\rSo far, all we’ve done is reformulate our problem so that instead of calculating\r\\(n\\) terms in a sequence using simple addition, we now have to multiply \\(n\\)\rmatrices together. We’ve made things worse! Multiplication is slower than\raddition, especially for large numbers, and computing the production of two \\(2 \\times 2\\) matrices requires eight multiplications!\nRemain calm. There’s a trick to calculating large powers quickly. Imagine\rwe want to calculate \\(x^n\\) where \\(n\\) is a power of two: \\(n = 2^m\\). If\rwe square \\(x\\), then square it again, and keep doing that \\(m\\) times, we get\n\\[ ((x^2)^2...)^2 = x^{2^m} = x^n \\]\nIn other words, we only need to perform \\(m = \\log_2 n\\) matrix multiplications to\rcalculate \\(x^n\\).\nWe can generalize this to calculate any large power \\(n\\) (not necessary a power\rof two) by first finding the largest power of two less than \\(n\\) and factoring\rit out:\n\\[ x^n = x^{2^m} x^{n-2^m} \\]\nThe left factor can be calculated by repeated squaring and the right factor by\rcan calculated by recursively applying the same trick. However, we will never\rneed to do that more than \\(\\log_2 n\\) times and each time the power of two gets\rsmaller.\nThe upshot is that we can calculate \\(x^n\\) in \\(\\mathcal{O}(\\log n)\\)\rmultiplications. This is mostly commonly seen in cryptography such as the RSA\ralgorithm and Diffie-Hellman key exchange where it is done modulo\rsome large but fixed sized integer, making all the multiplications roughly\requal cost. Here, we are using multiple precision integers which are doubling\rin size with each multiplication. That means abstract “multiplications” are the\rwrong thing to count. We won’t get \\(\\mathcal{O}(\\log n)\\) runtime\rperformance because the top multiplications keep getting more expensive.\rNevertheless, the squaring by exponentiation trick hugely reduces the amount of\rwork we have to do relative to the naive iterative solution.\n\rMatrix Implementation\rFun fact: Python has multiple precision baked in. If if an arithmetic operation\ron Python’s int() type exceed the normal limits of a 64-bit integer, Python\rwill transparently substitute a high precision type. This makes Python a\rconvenient language for working with very large numbers.\nNow, we could just rely on NumPy’s matrix multiplication, like so:\nF1 = numpy.array([[1,1],[1,]], dtype=\u0026#39;object\u0026#39;) numpy.linalg.matrix_power(F1, n)\rThis works. (Although strangely enough matrix multiplication with the @\roperator doesn’t work when dtype='object'.) As much as love numpy though,\rI don’t think we need to drag it in as a dependency just to multiply \\(2 \\times 2\\) matrices when we’re not even using native integer types.\nPlus, we’ll see see in a second that there are some optimizations we can make\rthat wouldn’t be possible if we let NumPy handle everything for us. So for\rnow, let’s implement the naive matrix algorithm in native Python; we’ll come\rback and refactor in the next section.\nFirst, for testing and benchmarking purposes, we’ll write a non-optimized\rversion that just implements matrix powers in a straightforward way:\ndef matrix_multiply(A, B):\ra, b, c, d = A\rx, y, z, w = B\rreturn (\ra*x + b*z,\ra*y + b*w,\rc*x + d*z,\rc*y + d*w,\r)\rdef naive_matrix_power(A, m):\rif m == 0:\rreturn [1, 0, 0, 1]\rB = A\rfor _ in range(m-1):\rB = matrix_multiply(B, A)\rreturn B\rdef naive_matrix_fib(n):\rreturn naive_matrix_power(F1, n)[1]\rBut we’ll immediately want to move on to a version which implements\rthe fast exponentiation by repeated squares described above:\ndef matrix_power(A, m):\rif m == 0:\rreturn [1, 0, 0, 1]\relif m == 1:\rreturn A\relse:\rB = A\rn = 2\rwhile n \u0026lt;= m:\r# repeated square B until n = 2^q \u0026gt; m\rB = matrix_multiply(B, B)\rn = n*2\r# add on the remainder\rR = matrix_power(A, m-n//2)\rreturn matrix_multiply(B, R)\rF1 = [1, 1, 1, 0]\rdef matrix_fib(n):\rreturn matrix_power(F1, n)[1]\r\rImplicit Matrix Form\rThe above has reasonably good asymptotic performance but it bothers me that\rit’s doing 8 multiplications each time. Luckily, because all Fibonacci matrices\rare of a special form, we really only need to keep track of two elements in the\rright-hand column of the matrix. I call this this the “implicit matrix form.”\rHere is a Fibonacci matrix described with just two numbers, \\(a\\) and \\(b\\):\n\\[\r\\mathbf{F}_n = \\begin{bmatrix}\r\\color{lightgrey} a + b \u0026amp; a \\\\\r\\color{lightgrey} a \u0026amp; b\r\\end{bmatrix}\r\\]\nWe can easily work out closed forms for multiplying and squaring matrices\rin this form. While the full expressions are a little complex - we never\ractually need to explicitly calculate the left-hand column, a fact I\rwill indicate by graying those columns out:\n\\[\r\\begin{align}\r\\begin{bmatrix}\r\\color{lightgrey} a + b \u0026amp; a \\\\\r\\color{lightgrey} a \u0026amp; b\r\\end{bmatrix}\r\u0026amp;\r\\begin{bmatrix}\r\\color{lightgrey} x + y \u0026amp; x \\\\\r\\color{lightgrey} x \u0026amp; y\r\\end{bmatrix} \u0026amp; =\r\\begin{bmatrix}\r\\color{lightgrey} a(2x+y) + b(x+y) \u0026amp; a(x+y) + bx \\\\\r\\color{lightgrey} a(x+y) + bx \u0026amp; ax + by\r\\end{bmatrix} \\\\\r\u0026amp;\r\\begin{bmatrix}\r\\color{lightgrey} a + b \u0026amp; a \\\\\r\\color{lightgrey} a \u0026amp; b\r\\end{bmatrix}^2\r\u0026amp; =\r\\begin{bmatrix}\r\\color{lightgrey} 2a^2 + 2ab + b^2 \u0026amp; a^2 + 2ab \\\\\r\\color{lightgrey} a^2 + 2ab \u0026amp; a^2 + b^2 \\end{bmatrix}\r\\end{align}\r\\]\nUsing the implicit matrix form, we can multiply two\rdifferent Fibonacci matrices with just four multiplications, and we can\rsquaring a matrix with only three! It’s only a constant time speed-up but every\rlittle bit helps.\ndef multiply(a, b, x, y):\rreturn x*(a+b) + a*y, a*x + b*y\rdef square(a, b):\ra2 = a * a\rb2 = b * b\rab = a * b\rreturn a2 + (ab \u0026lt;\u0026lt; 1), a2 + b2\rdef power(a, b, m):\rif m == 0:\rreturn (0, 1)\relif m == 1:\rreturn (a, b)\relse:\rx, y = a, b\rn = 2\rwhile n \u0026lt;= m:\r# repeated square until n = 2^q \u0026gt; m\rx, y = square(x, y)\rn = n*2\r# add on the remainder\ra, b = power(a, b, m-n//2)\rreturn multiply(x, y, a, b)\rdef implicit_fib(n):\ra, b = power(1, 0, n)\rreturn a\rIt would of course be possible to derive these relationships without ever\rintroducing the Fibonacci matrices, but I think they provides a valuable\rfoundation for intuition. Without that foundation, the above program seems a\rlittle arbitrary.\nYou may be wondering why I square numbers as a*a instead of a**2 or\rpow(a, 2), and why I use ab\u0026lt;\u0026lt;1 instead of 2*ab or ab+ab to double\rthem. The answer is simple - I benchmarked the various forms and found these\rexpressions to be very slightly faster, at least when using large mpz()\robjects (which we’ll get to in a moment.)\n\rCython\rAnother thing to try – something which usually helps a lot –\ris to try converting our program to Cython.\nUnfortunately, the one type that we want to use, Python’s native int() type, is\rrepresented by Cython as a C-style int - fixed precision signed integer. It\rdoesn’t have Python’s ability to transparently handle large numbers. We can\reither use the native C long in which case we run into precision problems after \\(F_{93}\\),\ror we can continue to use the Python int() type in which case we gain only a modest\rspeed up.\n%%cython\rcdef cython_multiply(a, b, x, y):\rreturn x*(a+b) + a*y, a*x + b*y\rcdef cython_square(a, b):\ra2 = a * a\rb2 = b * b\rab = a * b\rreturn a2 + (ab \u0026lt;\u0026lt; 1), a2 + b2\rcdef cython_power(a, b, int m):\rcdef int n = 2\rif m == 0:\rreturn (0, 1)\relif m == 1:\rreturn (a, b)\relse:\rx, y = a, b\rwhile n \u0026lt;= m:\r# repeated square until n = 2^q \u0026gt; m\rx, y = cython_square(x, y)\rn = n*2\r# add on the remainder\ra, b = cython_power(a, b, m-n//2)\rreturn cython_multiply(x, y, a, b)\rcpdef cython_fib(n):\ra, b = cython_power(1, 0, n)\rreturn a\rprint(cython_fib(103))\rWe still get a good boost for small numbers, but\rthe benefit of this quickly becomes irrelevant for large numbers.\nNever fret, though, because we can use something even better.\n\rThe GNU Multiple Precision Arithmetic Library\rThe GNU Multiple Precision Arithmetic Library, or GMP for short, is\rnothing short of a work of art. Often used for calculating \\(\\pi\\) to a number\rof decimal places described as “silly” by their own documentation, GMP\ris able to add, multiply, divide and perform arithmetic on larger and larger\rnumbers until your computer runs out of RAM. The multiplication algorithm used\rstarts with Karatsuba - and then they get serious.\nIt’s almost embarrassingly easy to convert our algorithm to use GMP because the\rmpz() type is a drop-in replacement for int():\nimport gmpy2\rfrom gmpy2 import mpz\rdef gmp_fib(n):\ra, b = power(mpz(1), mpz(0), mpz(n))\rreturn a\rNote that we didn’t have to define the power() or multiply() functions\ragain: this implementation re-uses the exact same functions we wrote for Python\rnative types when implementing implicit_fib() above. Every Python function is\ra type-agnostic template function.\nYou may also wonder why the large integer type is called mpz: the “mp” is for\r“multiple precision”, just like the “MP” in “GMP,” while the “z” stands for\r\\(\\mathbb{Z}\\), the conventional name for the set of integers. There is also mpq\rfor the set of rationals \\(\\mathbb{Q}\\) and so on.\n\rDynamic Programming Redux\rThe GMP version is really quite extraordinarily fast, but if we look at the\rcall graph we can still see some redundant effort. It turns out that we\rare recalculating each power of two every time we need it, resulting\rin this ever widening tree-shaped DFG:\nnaive DFG for fib(103)\n\rWe can fix this with - you guessed it - dynamic programming! With dynamic\rprogramming, it’s a good idea to only cache the results of sub-problems which\rare likely to be re-used. Here, we can be reasonably certain that the only\rresults worth caching are the powers of two, so we refactor that to its\rown function and apply memoization there.\n# improve the algorithm slightly by caching\r# and re-using powers of two. @lru_cache(100)\rdef dynamic_repeated_squares(a, b, n):\r# n must be a power of two. if n == 0:\rreturn (0, 1)\relif n == 1:\rreturn (a, b)\rreturn square(*dynamic_repeated_squares(a, b, n//2))\rdef dynamic_power(a, b, m):\rif m == 0:\rreturn (0, 1)\relif m == 1:\rreturn (a, b)\relse:\r# hit the cache for powers of 2\rn = 2\rwhile n \u0026lt;= m:\rn = n*2\rn = n // 2\rx, y = dynamic_repeated_squares(a, b, n)\r# add on the remainder\ra, b = dynamic_power(a, b, m-n)\rreturn multiply(x, y, a, b)\rdef dynamic_fib(n):\ra, b = dynamic_power(mpz(1), mpz(0), mpz(n))\rreturn a\rWith the caching added for powers of two, we get a much smaller DFG, now an acyclic graph\rwith no duplicate effort at all:\nDFG for fib(103) with dynamic programming\n\rIt should be clear from graph that in the worst case scenario, where \\(n = 2^m -1\\), the cached algorithm performs a maximum of \\(2m\\) multiplications, compared to the\r\\(m(m-1)/2\\) needed for the algorithm without caching. Despite this, the benefit\rof the cache is surprisingly minor: maybe 10% in practice. That’s because\ralmost all the time is spent in a\rhandful of very large multiplications – the smaller ones just don’t matter as\rmuch. “Logical multiplications” just isn’t the right operation to count. When\rdealing with multiple precision numbers we need to look at the number of bytes\rmultiplied, and the number of bytes doubling with each multiplication. I’ve heard those\rtwo effects more or less cancel out and the final algorithm is \\(\\mathcal{O}(n \\log n)\\)\rbut won’t venture to prove it myself. It seems to roughly hold empirically:\revery time \\(n\\) goes up by a factor of 10, time increases by about 20. (See the\rbenchmarks below.)\n\rC++ Fibonacci\rNow that we’ve exhausted my ideas for algorithmic optimizations, there’s really\ronly one thing approach left: micro-optimization. So far we’ve been working in\rPython, but Python has a reputation for being slow and we did see a small\rspeed-up when we started using Cython. The GMP library is native to C; maybe a\rC or C++ program would eliminate all the Python overhead?\nTo find out, I ported the above logic pretty faithfully to C++, almost line-for-line:\n// memoized version\rImplicitMatrix repeatedSquares(int n)\r{\r// 0 squares means the original basis matrix f1\rstatic std::vector\u0026lt;ImplicitMatrix\u0026gt; cache = { {1, 0} };\r// repeatedly square as often as necessary.\rwhile (n \u0026gt;= cache.size() ) {\rcache.push_back( square(cache.back()) );\r}\r// the n-th element is always f1^n.\rreturn cache[n];\r}\rImplicitMatrix power(\rconst ImplicitMatrix\u0026amp; x,\rconst bigint\u0026amp; m)\r{\rif ( m == 0 ) {\rreturn {0, 1};\r} else if ( m == 1 ) {\rreturn x;\r}\r// powers of two by iterated squaring\r// ImplicitMatrix powerOfTwo = x;\rbigint n = 2;\rint n_squares_needed = 0;\rwhile ( n \u0026lt;= m ) {\rn = n*2;\rn_squares_needed++;\r//powerOfTwo = square(powerOfTwo);\r}\rImplicitMatrix powerOfTwo = repeatedSquares(n_squares_needed);\r// recurse for remainder\rImplicitMatrix remainder = power(x, m-n/2);\rreturn multiply(powerOfTwo, remainder);\r}\rI installed these libraries on Debian/Ubuntu like so:\nsudo apt install libboost-all-dev libgmp-dev\rThe above program was built like so:\ng++ -std=c++17 -O3 -o fib main.cpp -lgmp\rNote that -O3 tells the compiler to apply maximum optimization\rto the program. That’s also why we need the volatile keyword -\rthe optimizer notices my program doesn’t actually do anything\rand optimizes the whole thing away!\nThe results we mildly disappointing:\n~/fib$ time ./fib 10000003\rreal 0m0.427s\ruser 0m0.360s\rsys 0m0.060s\r~/fib$ time ./fib 1000000003\rreal 1m24.088s\ruser 1m22.550s\rsys 0m1.430s\rIf this is any faster than the Python version, it can’t be be measured. This\rresult isn’t actually too surprising - at this point, 99.9% of computation time\ris spent in the GMP multiplication routines, and only a few microseconds are\rspent in Python. So we’re not going to squeeze any more performance out that way.\n\rFinal Python Fibonacci\rOur performance testing has revealed something interesting - there is no one\rimplementation which strictly dominates all the others over all possible\rinputs. The simple algorithms tend to win when \\(n\\) is small, while more complex\ralgorithms are able to pull ahead when \\(n\\) is large.\nA common way to squeeze as much performance as possible across all possible\rinputs is to use a hybrid algorithm which selects an algorithm from a family\rbased on heuristics that estimate which should perform best in which regions.\rA hybrid solution is the Annie Oakley solution: “Anything you can do I can do better; I can do anything better than you.”\rProbably the most famous hybrid algorithm in use today is Timsort.\nWe will use earlier benchmarks to define three regions:\n\r\rRegion\rName\rAlgorithm\rImplementation\r\r\r\rn \u0026lt;= 92\rSmall\rTable Lookup\rPython\r\r92 \u0026lt; n \u0026lt;= \\(2^{12}\\)\rMedium\rImplicit Matrix\rCython\r\rn \u0026gt; \\(2^{12}\\)\rLarge\rImplicit Matrix\rGMP\r\r\r\rFor the first region, we introduce a pre-calculated table indexed at zero which\rstores every Fibonacci number small enough to fit into 64-bits.\nsmall_fib = [\r0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597,\r2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418,\r317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465,\r14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296,\r433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976,\r7778742049, 12586269025, 20365011074, 32951280099, 53316291173,\r86267571272, 139583862445, 225851433717, 365435296162, 591286729879,\r956722026041, 1548008755920, 2504730781961, 4052739537881, 6557470319842,\r10610209857723, 17167680177565, 27777890035288, 44945570212853,\r72723460248141, 117669030460994, 190392490709135, 308061521170129,\r498454011879264, 806515533049393, 1304969544928657, 2111485077978050,\r3416454622906707, 5527939700884757, 8944394323791464, 14472334024676221,\r23416728348467685, 37889062373143906, 61305790721611591, 99194853094755497,\r160500643816367088, 259695496911122585, 420196140727489673,\r679891637638612258, 1100087778366101931, 1779979416004714189,\r2880067194370816120, 4660046610375530309, 7540113804746346429\r]\rPast that, we will use either the Cython or GMP implementation depending on\rwhether a better constant or better asymptotic performance is more beneficial.\ndef hybrid_fib(n):\rif n \u0026lt;= len(small_fib):\rreturn small_fib[n]\relif n \u0026lt;= 2**12:\rreturn cython_fib(n)\relse:\rreturn gmp_fib(n)\rAnd that, as they say, is my final answer.\n\rBenchmarking Results\rAs I’ve been implementing these, I’ve been informally testing and benchmarking them with\rIPython’s %timeit magic. But now that we have a large number of candidate\rimplementations, ad hoc testing is becoming tiresome. Let’s benchmark all of our\rfunctions across a wide range of inputs to see which emerges as the leader. All of\rthese are measured at \\(2^k-1\\) to force worst-case performance from the main algorithms.\ncompeting Fibonacci implementations\n\rWe can make a few observations:\n\rThe naive implementation’s \\(\\mathcal{O}(2^n)\\) performance hits a wall around\r100, after which it’s no longer practical.\rThe table based method actually runs out of memory before its runtime\rperformance becomes a problem.\rThe eigen_fib() implementation is basically constant time - until it starts\roverflowing once it can no longer represent its solution as a 64-bit floating\rpoint number.\rThe best asymptotic performance is from the version using both GMP and the\rdynamic programming cache.\rBy construction, the “hybrid” algorithm traces out lower bound - constant\runtil 92, then hugs the Cython curve for a while, then switches to the\rdynamic GMP solution for large numbers.\r\r\rFeynman Fuse Problem\rWe’ve made a lot of progress, and we’ve hit what I call a “fuse problem,” after\rthis anecdote from Surely You’re Joking, Mr. Feynman!:\n\rThe problem was to design a machine like the other one - what they called a\rdirector - but this time I thought the problem was easier, because the\rgunner would be following behind in another machine at the same altitude.\rThe gunner would set into my my machine his altitude and an estimate of his\rdisance behind the other airplane. My machine would automatically tilt the\rgun up at the correct angle and set the fuse.\nAs director of this project, I would be making trips down to Aberdeen to\rget the firing tables. However, they already had some preliminary data and\rit turned out that the fuses they were going to use were not clock fuses,\rbut powder-train fuses, which didn’t work at those altitudes - they fizzled\rout in the thin air.\nI thought I only had to correct for the air resistance at different\raltitudes. Instead my job was to invent a machine that would make the\rshell explode at the right moment, when the fuse won’t burn!\nI decided that was too hard for me and went back to Princeton.\n\rWork on a problem long enough, and every problem is a fuse problem; that is to\rsay, it becomes apparent that a fundamental shift in approach and a completely\rdifferent skill set is necessary to make any further progress.\nIn our case, the problem is no longer to calculate Fibonacci numbers – the\rproblem is now to find a way to multiply large integers together efficiently.\rAs far as I can tell, GMP is already state-of-the-art when it comes to\rthat, and tends to come out ahead on most benchmarks.\nIn fact, it’s recently come to my attention that GMP in fact has a dedicated\rFibonacci benchmark. I can’t compete with that! So I think we’ve taken\rit as far as we can reasonably go.\n\rConclusion\rWhen I started this project, I would not have believed that my laptop could\rcalculate the millionth Fibonacci number in a fraction of a second. Certainly the first few\ralgorithms we looked at couldn’t come close to that. But my surprise should\rcome as no surprise.\nNew algorithms are being discovered all the time. When I graduated,\rquicksort was considered state-of-the-art. Since then, Timsort has\rsupplanted it in a number of standard libraries such as Java’s. Some\rpeople believe improvements in algorithms are outpacing Moore’s law.\rSometimes an algorithm comes along and just blows everything else out of the water,\rlike John Platt’s Sequential Minimal Optimization. For a decade after\rthat was invented, SVM’s were considered one of the best off-the-shelf\rclassifiers, until even better algorithms came along. Even today, the\rbest way to fit an ElasticNet model is to reduce it to an SVM and use a\rfast solver based on SMO.\nNew algorithms take even dedicated professionals by surprise. Kolmogorov\r– perhaps one of the greatest mathematicians of all time – actually stated\rthe conjecture that multiplication was necessarily \\(\\mathcal{O}(n^2)\\) in a\rlecture and a few weeks later his student Karatsuba showed how a few\rsimple additions and subtractions would allow three multiplications to do the\rwork of four, decreasing the bound to \\(\\mathcal{O}(n^{\\log_2 3})\\). So simple,\ryet until 1960 not one mathematician had ever thought of it. And yet it is this\rtrick (and later more complicated versions in the same vein) that account for\rthe almost magical speed of the GMP library. It’s also closely related to the\rsame divide-and-conquer strategy for matrix multiplication that makes linear\ralgebra libraries like OpenBLAS so fast.\nThe lower bounds on good algorithms can often seem impossible. It doesn’t sound\rpossible to search a length \\(n\\) string for a substring of length \\(m\\) in less\rthan \\(\\mathcal{O}(nm)\\), but Rabin-Karp and other similar algorithms do it\rin \\(\\mathcal{O}(n+m)\\) through the clever use of a rolling hash. It doesn’t\rsound possible to store and retrieve items in less than \\(\\mathcal{O}(\\log n)\\),\rbut hash tables do it in amortized \\(\\mathcal{O}(1)\\). Obviously, there’s\rabsolutely no way to estimate the cardinality of the union of two possibly\roverlapping sets in less than \\(\\mathcal{O}(n \\log n)\\)… unless you use\rHyperLogLog. Bloom filters let you (for the price of a small change of a false\rpositive but no chance of a false negative) test set membership while using\ronly \\(\\mathcal{O}(\\log n)\\) space. How can it possibly do that? Hash functions\ragain. In my own work, I frequently rely on sophisticated gradient descent\ralgorithms to fit models that would take hours or days to fit on the\rsame hardware if naive algorithms were used. All of these algorithms are\rsomewhere between magical and impossible. Yet they all work, both in theory and\rpractice.\nAs good as today’s hardware is, it’s often the algorithm that makes the\rimpossible possible.\nThe code for today’s article is available as a Jupyter notebook if you’d\rlike to hack on it. You will need to install GMP, its Python wrapper\rgmpy2, and Cython. I am sure there is another order of magnitude\rof performance to be found somewhere.\n\r","date":"February 19, 2019","href":"https://www.oranlooney.com/post/fibonacci/","thumbnail":"/post/fibonacci_files/lead.192x128.jpg","title":"A Fairly Fast Fibonacci Function"},{"content":"In today’s installment of Machine Learning From Scratch we’ll build on the logistic regression from last time to create a classifier which is able to automatically represent non-linear relationships and interactions between features: the neural network. In particular I want to focus on one central algorithm which allows us to apply gradient descent to deep neural networks: the backpropagation algorithm. The history of this algorithm appears to be somewhat complex (as you can hear from Yann LeCun himself in this 2018 interview) but luckily for us the algorithm in its modern form is not difficult - although it does require a solid handle on linear algebra and calculus.\nI am indebted to the many dedicated educators who taken the time to prepare in-depth, easy-to-understand, and mathematically rigorous presentations of the subject and will not attempt to yet another; my intention with this article is simply to derive the backpropagation algorithm, implement a working version from scratch, and to discuss the practical implications of introducing are more powerful representation.\nRepresentation A complete description of typical fully-connected feed-forward \\(L\\)-layer neural network can be given in just four equations: two boundary conditions for the input and output layers, and two recurrence relationships for the connections between layers:\n\\[ \\begin{split} a^{(0)} \u0026amp; = X \\\\ z^{(i)} \u0026amp; = W^{(i)} a^{(i-1)} + b^{(i)} \\\\ a^{(i)} \u0026amp; = \\sigma(z^{(i)}) \\\\ \\hat{y} \u0026amp; = a^{(L)} \\end{split} \\]\nHere \\(\\sigma(x)\\) is a sigmoid function, \\(W^{(i)}\\) are matrices of weights connecting layers, \\(b^{(i)}\\) are bias vectors, \\(X\\) is the given matrix of data with one row per observation and one column per feature, and the final activation \\(a^{(L)}\\) is also our prediction \\(\\hat{y}\\).\n(A brief aside about notation. A superscript inside of parentheses is a layer index; the parentheses are meant to distinguish it from an exponent. This notation is used so that ordinary subscripts can be used to refer to the individual elements of \\(W\\) and \\(b\\), for example, \\(W_{jk}^{(i)}\\) is the element in the \\(j\\)-th row of the \\(k\\)-th column of the weight matrix \\(W^{(i)}\\) for the \\(i\\)-th layer.)\nAll that’s really going on here is that we are alternating matrix multiplication with the element-wise application of a non-linear function, \\(\\sigma(x)\\) in this case. Start with a signal \\(X\\). To propagate the signal through the network, multiply by a matrix \\(W^{(1)}\\); add in the bias, apply the activation function. Multiply by \\(W^{(2)}\\), add in the bias, apply the application function. Multiply by \\(W^{(2)}\\), add in the bias, apply the application function. Repeat until you reach the output layer.\nNote that because of the restrictions on matrix multiplication, we can determine the number of rows and columns in each matrix \\(W^{(i)}\\) by noting that it much have a number of columns equal to the number of rows in \\(W^{(i-1)}\\). In particular that means \\(W^{(1)}\\) must have a number of columns equal to the number of features in the dataset \\(X\\), while \\(W^{(L)}\\) has only a single output row. We call this shared dimension the “number of nodes” in that layer of the neural network; you will see me use this terminology in the code below especially when initializing the network.\nThe parameters of the model are all the elements of every connection matrix \\(W^{(i)}\\) plus the elements of the bias vectors \\(b^{(i)}\\). In symbols:\n\\[ \\Theta = (W^{(1)} ... W^{(L)}, b^{(1)} ... b^{(L)}) \\]\nA quick aside about the total number of parameters: Since every element of every weight matrix for every layer is a separate parameter, large neural networks tend to have a lot of parameters. This implies that neural nets have high VC dimension, which in turn implies that they tend to badly overfit unless the number of data points in the training set is a high multiple of the number of parameters. This is the fundamental reason why “deep neural networks” and “big data” go hand-in-hand.\nReturning to the mathematics of our representation, let’s make this abstract recurrence relation concrete by showing explicit examples for small \\(L\\). For example, with zero hidden layers (\\(L=1\\)) a neural network reduces to the equation for logistic regression:\n\\[ \\hat{y} =\\sigma(W^{(1)} X + b^{(1)}) \\]\nIf a zero-hidden-layer neural network is also trained with log-loss, both the model’s representation and fitted parameters will be exactly the same as logistic regression. We can view LR as a special case of neural nets or equivalently neural nets as a generalization of LR.\nWith one hidden layer (\\(L=2\\)) this expands to:\n\\[ \\hat{y} =\\sigma(W^{(2)} \\sigma(W^{(1)} X + b^{(1)}) + b^{(2)}) \\]\nA logistic regression of logistic regressions, if you will. As the chain grows longer the same pattern is repeated:\n\\[ \\hat{y} =\\sigma(W^{(3)} \\sigma(W^{(2)} \\sigma(W^{(1)} X + b^{(1)}) + b^{(2)}) + b^{(3)}) \\]\nThese examples are only included for the sake of concreteness. The recursive definitions will allow us to reason about a sequential neural network with any number of layers.\n Fitting To fit the model to data, we find the parameters which minimize loss: \\(\\hat{\\Theta} = \\text{argmin} \\, J(\\Theta;X)\\). Just as with logistic regression we use binary cross-entropy (a.k.a. log-loss) which means our loss function \\(J\\) is given by:\n\\[ J = \\frac{1}{N} \\sum_i^N y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) \\]\nNote that we have introduced a \\(1/N\\) scale factor. We are free to do this because multiplying by a positive constant does not change the optimization problem. We do this so that the gradient will be an average over all the training examples and therefore will be invariant w.r.t. training set size. This turns out to be convenient because it means we will not need to change our learning rate when fitting larger or smaller datasets.\nOne condition which must be true at a local minima is that \\(\\nabla_\\Theta J = 0\\). That gives us the equations:\n\\[ \\frac{\\partial J}{\\partial W^{(i)}} = 0, \\frac{\\partial J}{\\partial b^{(i)}} = 0 \\]\nThe notation used here is from matrix calculus, and we are taking partial derivatives with respect to a matrix (for \\(W\\)) or a vector (for \\(b\\).) The matrix cookbook may help with this notation.\nGiven the forward pass equations given above, we can easily calculate the partial derivatives for the individual components. For the derivation of \\(\\partial J / \\partial a^{(L)}\\) you can follow pretty much the same proof given in the previous article on logistic regression. For the others it is easy to verify from the above definitions that:\n\\[ \\begin{split} \\color{blue} \\frac{\\partial J}{\\partial a^{(L)}} \u0026amp; \\color{blue} = \\frac{\\partial J}{\\partial \\hat{y}} = \\frac{1}{N} (y - \\hat{y}) \\\\ \\color{green} \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} \u0026amp; \\color{green} = a^{(i)} \\circ (1-a^{(i)}) \\\\ \\color{green} \\frac{\\partial z^{(i)}}{\\partial a^{(i-1)}} \u0026amp; \\color{green} = W^{(i)} \\\\ \\color{maroon} \\frac{\\partial z^{(i)}}{\\partial W^{(i)}} \u0026amp; \\color{maroon} = (a^{(i-1)})^T \\end{split} \\]\nFor the element-wise derivative of the sigmoid we use the slightly non-obvious fact that \\(\\sigma\u0026#39;(x) = \\sigma(x) (1-\\sigma(x))\\) which we proved in Part 2. Also, take special note of the transpose on the last equation! This follows immediately from proposition (70) in the matrix cookbook but its importance is to introduce the inner product that sums over all the training examples; all the other terms have a number of rows equal to \\(N\\) (the number of training examples) and it is only in this last step that we reduce dimensionality to match that of \\(W\\). The practical implication is that the activations and backpropagated error terms we carry around during the calculation require an amount of memory proportional to \\(N\\). This is part of the reason why mini-batch gradient descent is a good idea when training neural networks.\nFrom now on, we will be describing the equations for \\(\\nabla J\\) in terms of partial derivatives - if you want to know how to actually calculate anything concretely, refer to these four equations.\nTo take a partial derivative of \\(J\\) with respect to any parameter in any layer we can use the chain rule. For \\(W^{(L)}\\) we have:\n\\[ \\frac{\\partial J}{\\partial W^{(L)}} = \\Bigg( \\color{blue} \\frac{\\partial J}{\\partial a^{(L)}} \\color{black} \\circ \\color{green} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\color{black} \\Bigg) \\cdot \\color{maroon} \\frac{\\partial z^{(L)}}{\\partial W^{(L)}} \\]\nWhere the dot product is defined as \\(x \\cdot y = (x^T y)^T\\). This corresponds to the usual definition when \\(x\\) and \\(y\\) are vectors but is extended to matrices. You can think of it as summing over all the examples in the training set. This notation isn’t 100% standard but I’m going to use it anyway because it lets us write out the chain rule in the usual left-to-right manner.\nNext, let’s do \\(W^{(L-1)}\\):\n\\[ \\frac{\\partial J}{\\partial W^{(L-1)}} = \\Bigg( \\underbrace{ \\color{blue} \\frac{\\partial J}{\\partial a^{(L)}} \\color{black} \\circ \\color{green} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} }_{Old} \\color{green} \\circ \\color{black} \\underbrace{ \\color{green} \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}} \\circ \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}} }_{\\text{New}} \\color{black} \\Bigg) \\cdot \\color{maroon} \\frac{\\partial z^{(L-1)}}{\\partial W^{(L-1)}} \\]\nThen \\(W^{(L-2)}\\):\n\\[ \\frac{\\partial J}{\\partial W^{(L-2)}} = \\Bigg( \\underbrace{ \\color{blue} \\frac{\\partial J}{\\partial a^{(L)}} \\color{black} \\circ \\color{green} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\circ \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}} \\circ \\frac{\\partial a^{(L-1)}}{\\partial z^{(L-1)}} }_{Old} \\color{green} \\circ \\color{black} \\underbrace{ \\color{green} \\frac{\\partial z^{(L-1)}}{\\partial a^{(L-2)}} \\circ \\frac{\\partial a^{(L-2)}}{\\partial z^{(L-2)}} \\color{black} }_{New} \\Bigg) \\cdot \\color{maroon} \\frac{\\partial z^{(L-2)}}{\\partial W^{(L-2)}} \\]\nBy now you should be starting to see a pattern emerge: as we go back layer by layer, the left-most part of the equation (blue and green) for layer \\(i\\) is always the same as the blue and green part from layer \\(i+1\\) plus two more new green factors, and capped off by the final maroon factor.\nThink of it like a snake: We always start with the (blue) derivative of our loss function with respect to the prediction. This only happens once so stays at the “head” of the equation. At the (maroon) “tail”, we always take a derivative with respect to the parameters of interest. And in between we between we have a growing (green) “body” of partial derivatives. To capture this insight in symbols, let’s introduce a new recurrence relation:\n\\[ \\begin{split} \\color{purple} \\delta^{(L)} \u0026amp; = \\color{blue} \\frac{\\partial J}{\\partial a^{(L)}} \\color{black} \\circ \\color{green} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\color{black} \\\\ \\color{purple} \\delta^{(i-1)} \u0026amp; = \\color{purple} \\delta^{(i)} \\color{black} \\circ \\Bigg( \\color{green} \\frac{\\partial z^{(i)}}{\\partial a^{(i-1)}} \\circ \\frac{\\partial a^{(i-1)}}{\\partial z^{(i-1)}} \\color{black} \\Bigg) \\\\ \\frac{\\partial J}{\\partial W^{(i)}} \u0026amp; = \\color{purple} \\delta^{(i)} \\color{black} \\circ \\color{maroon} \\frac{\\partial z^{(L-2)}}{\\partial W^{(L-2)}} \\end{split} \\]\nIt should also be clear that we can implement this iteratively if we start at layer \\(L\\) and work backwards: If we save the result of the blue and green parts from layer \\(i\\), we can add on another pair of green partial derivates to grow the “body” and quickly compute layer \\(i-1\\). Note also that in order to calculate \\(\\color{green} \\partial a / \\partial z\\) we need to know the activations which are most easily obtained by first performing a forward pass and remembering the activations for use in the backwards pass.\nFinally, I’d like to point out that the justification for introducing the (technically extraneous) concept of \\(z\\) is found in how absolutely obvious and clear it makes the separate steps of the forward and backwards pass. If we had to take \\(\\partial a^{(i)} / \\partial a^{(i-1)}\\) directly without going through \\(z\\) we would have to think about a non-linear function of a matrix all at once; with \\(z\\) we’re able to view it as the element-wise application of a non-linear function (which is Calculus 101) and the partial derivative of a linear expression with respect to a matrix (which is Matrix Calculus 101; see the Cookbook).\n Implementation The above equations are straight-forward to turn into working code. The only wrinkle is that while above we represented the bias as separate vectors \\(b^{(i)}\\), in the implementation we will instead implement the bias by assuring that the matrix \\(X\\) and all intermediate activations \\(a^{(i)}\\) have a constant 1 in their first column. Thus, in the Python implementation the first column of each \\(W^{(i)}\\) plays the role of the bias vector. This simplifies the code in some ways but complicates it in others; pay attention to where we are stacking the bias node (or removing it during the backwards pass) and the apparent off-by-one “mismatch” in matrix dimensions this introduces.\nimport numpy as np def sigmoid(z): return 1 / ( 1 + np.exp(-z) ) class NeuralNetwork: def __init__(self, n_hidden=(100,), learning_rate=1.0, max_iter=10, threshold=0.5): # a list containing the number of nodes in # each hidden layer. self.n_hidden = n_hidden # the input layer, output layer, and hidden layers. self.n_layers = len(n_hidden) + 2 # gradient descent parameters self.learning_rate = float(learning_rate) self.max_iter = int(max_iter) self.threshold = float(threshold) Initializing a neural network correctly turns out to be very important. If we punt and simply initialize all weights to a constant, the network will flat out not work to any meaningful degree. That’s new; linear regression and logistic regression certainly didn’t exhibit that behavior. Why is this the case? Well, if all the weights in a given layer are exactly the same at iteration \\(i\\), then the backpropagated error for each node with be exactly the same, so the gradient descent update will be exactly the same, so all the weights in the layer at iteration \\(i+1\\) will be exactly the same. By induction, this will be true for all iterations. Because all the weights of given layer are constrained to be the same, we effectively have only one free parameter! Our model will never learn the complex representations we want it to. Luckily, this critical point is almost surely unstable, so we can break symmetry by simply initializing the weights slightly differently. There are a couple popular ways to do this (one due to Glorot and Bengio and another due to He et al.) but since I don’t claim to understand either of those in great detail, to conform with the constraints of the “from scratch” project I’ll do something I do understand and just randomly initialize them randomly distributed by \\(\\mathcal{N}(0, 0.1^2)\\). That suffices to break symmetry.\n def _random_initialization(self): # a small amount of randomization is necessary to # break symmetry; otherwise all hidden layers would # get updated in lockstep. if not self.n_hidden: layer_sizes = [ (self.n_output, self.n_input+1) ] else: layer_sizes = [ (self.n_hidden[0], self.n_input+1) ] previous_size = self.n_hidden[0] for size in self.n_hidden[1:]: layer_sizes.append( (size, previous_size+1) ) previous_size = size layer_sizes.append( (self.n_output, previous_size+1) ) self.layers = [ [np.random.normal(0, 0.1, size=layer_size), sigmoid] for layer_size in layer_sizes ] Fitting is straight-forward: for every iteration we do one forward-pass to calculated activations, then one backwards pass to calculated gradients and update all the weights. You may notice that we’ve reverted to batch gradient descent; today’s focus is on the representation of neural networks and the backpropagation algorithm, so we’ll keep everything else as simple as possible. You can read about more sophisticated gradient descent methods in the previous article in this series.\n def fit(self, X, y): self.n_input = X.shape[1] self.n_output = 1 y = np.atleast_2d(y).T self._random_initialization() # fitting iterations for iteration in range(self.max_iter): self.forward_propagation(X) self.back_propagation(y) def predict(self, X): y_class_probabilities = self.predict_proba(X) return np.where(y_class_probabilities[:,0] \u0026lt; self.threshold, 0, 1) def predict_proba(self, X): self.forward_propagation(X) return self._activations[-1] The forward pass follows our above recursive definition of the neural network very closely. We initialize activation with the given predictors \\(a^{(0)} = X\\) then iteratively compute \\(a^{(1)}\\), \\(a^{(2)}\\) until we reach \\(a^{(L)} = \\hat{y}\\).\n def forward_propagation(self, X): # we will store the activations calculated at each layer # because these can be used to efficiently calculate # gradients during backpropagation. self._activations = [] # initialize the activation with the given data activation = X # forward propagation through all layers for W, activation_function in self.layers: bias = np.ones( (activation.shape[0], 1) ) activation = np.hstack([bias, activation]) self._activations.append(activation) activation = activation_function(activation @ W.T) # the final activation layer does not have a bias node added. self._activations.append(activation) For the backwards pass, we use error to mean \\(\\partial J / \\partial z_i\\) and delta to mean \\(\\partial J / \\partial W_i\\). We use the recurrence relations we derived from the chain rule to iteratively calculate the update for each layer counting down from \\(L\\) to \\(1\\).\n def back_propagation(self, y): # this function relies on self._activations calculated # by self.forward_propagation() N = y.shape[0] # the final prediction is simply activation of the final layer. y_hat = self._activations[-1] # this first error term is based on the gradient of the loss function: # log-loss in our case. Subsequently, error terms will be based on the # gradient of the sigmoid function. error = y_hat - y # we can see where the backpropagation algorithm gets its name: we # start at the last layer and work backwards, propagating the error # term from each layer to the previous one. for layer in range(self.n_layers-2, -1, -1): # calculate the update (delta) for the weight matrix a = self._activations[layer] delta = (error.T @ a) / N # every layer except the output layer has a bias node added. if layer != self.n_layers-2: delta = delta[1:, :] # propogate the error term back to the previous layer W = self.layers[layer][0] if layer \u0026gt; 0: # every layer except the output layer has a bias node added. if layer != self.n_layers-2: error = error[:, 1:] # the a(1-a) is the gradient of the sigmoid function. error = (error @ W) * (a * (1-a)) # update weights W -= self.learning_rate * delta  Testing Real world data are messy. Instead of shopping around for a toy data set which exhibits all the properties we want, we’ll cook up an idealized data set that is designed to only be solvable by a non-linear classifier.\nfrom sklearn import datasets X_full, y_full = datasets.make_classification( n_samples=5000, n_features=20, n_informative=15, n_redundant=3, n_repeated=0, n_classes=2, flip_y=0.05, class_sep=1.0, shuffle=True, random_state=42) # 80/20 train/test split. train_test_split = int(0.8 * len(y_full)) X_train = X_full[:train_test_split, :] y_train = y_full[:train_test_split] X_test = X_full[train_test_split:, :] y_test = y_full[train_test_split:] This synthetic dataset has 4,000 examples in the training set and 1,000 in the test set. There are two classes with equal prevalence. There are 20 features, but only about half of these have non-zero mutual information with the class. 5% of examples simply have their class flipped, so the Bayes rate will be less than 0.05; in other words, the ceiling for accuracy is less than 95%. Each vertex is assigned to one class or the other and then data is sampled from a standard multivariate Gaussian distribution centered at each vertex. In particular, because vertices are only one standard deviation away from each other these distributions will overlap a good deal and further reduce the Bayes rate. The problem is highly non-linear and classifiers with linear decision boundaries will struggle; however it not at all pathological and a good non-linear classifier should be able to achieve something quite close to the Bayes rate.\nWhen we fit a model, the number of hidden layers and the node of nodes in each layer is a hyperparameter. For example, two hidden layers with 20 nodes in the first layer and 5 in the second would be [20, 5].\nnn = NeuralNetwork(n_hidden=[8, 3], max_iter=2000) nn.fit(X_train, y_train) y_hat = nn.predict(X_train) p_hat = nn.predict_proba(X_train) np.mean(y_train == y_hat) If we have no hidden layers at all then our neural network reduces to logistic regression. In particular that means it can only learn an linear decision boundary. How well does that do on the synthetic dataset we cooked up?\nnn = NeuralNetwork(n_hidden=[], max_iter=2000) nn.fit(X_train, y_train) y_hat = nn.predict(X_train) p_hat = nn.predict_proba(X_train) y_test_hat = nn.predict(X_test) p_test_hat = nn.predict_proba(X_test) np.mean(y_train == y_hat), np.mean(y_test == y_test_hat)  (0.71924999999999994, 0.71699999999999997)\n Not so hot: about 72% accuracy, and an AUC of 0.7866. That’s pretty far away from the Bayes rate we estimated above; therefore we are probably underfitting.\nfrom sklearn.metrics import roc_curve, roc_auc_score from matplotlib import pyplot as plt %matplotlib inline fpr, tpr, threshold = roc_curve(y_test, p_test_hat) plt.figure(figsize=(16,10)) plt.step(fpr, tpr, color=\u0026#39;black\u0026#39;) plt.fill_between(fpr, tpr, step=\u0026quot;pre\u0026quot;, color=\u0026#39;gray\u0026#39;, alpha=0.2) plt.xlabel(\u0026quot;False Positive Rate\u0026quot;) plt.ylabel(\u0026quot;True Positive Rate\u0026quot;) plt.title(\u0026quot;ROC Curve\u0026quot;) plt.plot([0,1], [0,1], linestyle=\u0026#39;--\u0026#39;, color=\u0026#39;gray\u0026#39;) plt.text(0.45, 0.55, \u0026#39;AUC: {:.4f}\u0026#39;.format(roc_auc_score(y_test, p_test_hat))) plt.minorticks_on() plt.grid(True, which=\u0026#39;both\u0026#39;) plt.axis([0, 1, 0, 1]) ROC Curve for trivial NN\n On the other extreme, after trying a couple of hyperparameters, I found that [8,3] worked reasonably well.\nnn = NeuralNetwork(n_hidden=[8, 3], max_iter=2000) ROC Curve for trivial NN\n The [8, 3] model achieves 0.96 AUC and 95% accuracy on the training set and 91% accuracy on the test set. This is enormously better than what was possible with a linear decision boundary.\nThis new model has two hidden layers of 8 nodes and 3 nodes respectively; all layers are fully connected, so there’s a \\(9 \\times 3\\) matrix with 27 parameters connecting them, plus the \\(4 \\times 1\\) connecting to the output node and the \\(21 \\times 8\\) matrix connecting to the input layer, making for 199 parameters in all. Therefore it’s no surprise that its able to fit the training set much more closely, but it is very pleasant surprise that this means its test set performance is also much improved!\nWe can get a better intuition for the relationship between the complexity of our neural network and performance by plotting test set AUC as a function of the number of node used:\nROC Curve for trivial NN\n There’s a clear elbow in the graph around 10 nodes. Below that, the model makes steady gain in performance as more nodes to the model and its representation becomes better equipped to deal with the non-linearity of the true distribution. Above 10 nodes, making the model more complex is not able to further reduce generalization error; This suggests that the [8,3] model we found earlier is about as good as we can do.\nAs a rule of thumb, the more data available for training, the more features, and the more non-linear/interaction effects in the true population, the more that elbow gets pushed to the right; in other words, The model doesn’t start to overfit until much later. For best results, the complexity of the neural network should mirror the complexity of the underlying problem. One advantage of neural networks is that they give us an easy way to make the model arbitrarily “smarter” simply by adding more layers and more neurons. In fact, not only can we simply throw more neurons and layers at a problem, we also have a wide variety of specialized layers like CNNs which can make a neural network more suited to a particular problem. The neural network is a very “modular” learning algorithm in this sense and the flexibility means in can be adapted to a wide variety of problems.\nUnfortunately, the strategy of bigger, smarter neural networks is only really viable when we have a ton of training data. Smart neural networks trained on small datasets overfit horribly long before they learn to generalize. Neural networks don’t solve the bias-variance trade-off for us, and they certainly aren’t a free lunch. But they do provide a framework for creating models with low bias even on very large and difficult problems… then it’s our job to keep the variance in check.\n Conclusion That was backpropagation from scratch, our first look at neural networks. We saw how a sequential feed-forward network could be represented as alternating linear and non-linear transforms. We saw how to use the chain rule to calculate the gradient of the loss function with respect to any parameter in any layer of model, and how to calculate these gradients efficiently using the backpropagation algorithm. We demonstrated that a neural network could solve non-linear classification problems that logistic regression struggled with. And finally we saw how we could tune the number of layers and nodes in the neural network to take advantage of large datasets.\nThe specific neural network presented in this article is incredibly simple relative to the neural networks used to solve real world problems. For example, we haven’t yet talked about the vanishing gradient problem and how it can be solved with rectified linear units or batch normalization, how convolutional layers or pooling can help when the data have a natural spatial structure, how residual networks wire layers together as acyclic digraphs, how recurrent neural networks use short-term memory to handle arbitrary sequences of inputs, or a thousand other topics. Yet today, the state-of-the-art algorithm used to train all of these varied species of neural networks is backpropagation, exactly as presented here.\nIn practice, it isn’t usually necessary to actually do the calculus by hand. Modern machine learning frameworks like Tensorflow or PyTorch prominently feature automatic differentiation as a core capability. The chain rule is straightforward to apply mechanically - the hard part is keeping track of all those indices! So it makes sense to have software handle it.\nIn the next installment of the Machine Learning From Scratch series (coming soon!) we will change tact and look at a completely different approach to non-linear classification: decision trees and the recursive partition algorithm. We will see how these two apparently diametrically opposed approaches can both be viewed as examples of adaptive basis functions, and how this point-of-view unifies disparate topics in machine learning.\n ","date":"February 3, 2019","href":"https://www.oranlooney.com/post/ml-from-scratch-part-3-backpropagation/","thumbnail":"/post/ml-from-scratch-part-3-backpropagation_files/lead.192x128.jpg","title":"ML From Scratch, Part 3: Backpropagation"},{"content":"In this second installment of the machine learning from scratch\rwe switch the point of view from regression to classification: instead of\restimating a number, we will be trying to guess which of 2 possible classes a\rgiven input belongs to. A modern example is looking at a photo and deciding if\rits a cat or a dog.\nIn practice, its extremely common to need to decide between \\(k\\) classes where\r\\(k \u0026gt; 2\\) but in this article we’ll limit ourselves to just two classes - the\rso-called binary classification problem - because generalizations to many\rclasses are usually both tedious and straight-forward. In fact, even if the\ralgorithm doesn’t naturally generalize beyond binary classification (looking at\ryou, SVM) there’s a general strategy for turning any binary\rclassification algorithm into a multiclass classification algorithm called\rone-vs-all. Let’s agree to set aside the complexities of the multiclass\rproblem and focus on binary classification for now.\nThe binary classification problem is extremely central in machine learning and\rin this series we’ll be looking at no fewer than four different “serious”\rclassification algorithms. In an undergraduate machine learning class, you’d\rprobably work through a few “non-serious” or “toy” algorithms that have only\rhave historical or pedagogical value: the one-unit perceptron, linear\rdiscriminant analysis, the winnow algorithm. We will omit those and jump\rstraight to the simplest classification that is in widespread use: logistic\rregression.\nI say, “simplest,” but most people don’t think of LR as “simple.” That’s\rbecause they’re thinking of it use within the context of statistical analysis\rand the design and analysis of experiments. In those contexts, there’s a ton of\rassociated mathematical machinery that goes along with validating and\rinterpretting logistic regression models, and that stuff is complicated. A\rgood book on that side of logistic regression is Applied Logistic Regression\rby Hosmer et al.. But if you simply want to fit data and make predictions then\rlogistic regression is indeed a very simple model: as we’ll see, the heart of\rthe algorithm is only a few lines of code.\nDespite it’s simplicity, it’s important for three reasons. First, it can be surprisingly\reffective. It’s not uncommon for some state-of-the-art algorithm to\rsignificantly contribute to global warming by running a hyperparameter grid\rsearch over a cluster of GPU instances in the cloud, only to end up with a\rfinal model with only slightly lower generalization error than logistic\rregression. Obvious, this isn’t always true, otherwise there would be\rno need to study more advanced models. For example, LR tends to get only ~90%\raccuracy on the MNIST handwritten digit classification problem, which is much\rlower than either humans or deep learning. But in the many cases for which it\ris true, it’s worth asking if the problem is amenable to more advanced\rmachine learning techniques at all.\nThe second reason logistic regression is important is that it provides a\rimportant conceptual foundation for neural networks and deep learning, which\rwe’ll visit later in this series.\nThe third and final reason is that it cannot be solved with linear algebra\rso serves as a legitimate reason to introduce one of the most\rimportant tools in machine learning: gradient descent. Just as in part\r1, we’ll take the hard path through the mud and develop (step-by-step)\ran algorithm which could actually be used in production without too much\rembarrassment.\nThe Logistic Function\rBefore we get to the regression model, let’s take a minute to make\rsure we have a good understanding of the logistic function and some\rof its key properties.\nThe logistic function (also called the sigmoid function) is given by:\n\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]\nIt looks like this:\nFirst, we note that its domain is \\((-\\infty, +\\infty)\\) and its\rrange is \\((0, 1)\\). Therefore its output will always be a valid\rprobability. Note also that \\(\\sigma(0) = 0.5\\) so if we interpret\rthe output as a probability, all negative numbers map to probabilities\rthat are unlikely, while all positive numbers map to probabilities that\rare likely, while zero maps to exactly even odds.\nA few lines of simple algebra will show that its inverse function (also called\rthe “logit” function) is given by\n\\[ \\sigma^{-1}(x) = \\ln{\\frac{x}{1-x}} \\]\nNote that if \\(x = e^p\\) where \\(p\\) is a probability between 0 and 1,\rthen we have\n\\[ \\sigma^{-1}(e^p) = \\frac{p}{1-p} \\]\nWhere the right hand side is an odds ratio. So one interpretation of the\rlogistic function is that it is the bijection between log odds ratios to probabilities.\nFor example, if something has a probability of 0.2, then it has 4:1 odds,\rtherefore the odds ratio is 4, therefore the log odds ratio of \\(\\ln 4 = -1.39\\).\rSo \\(\\sigma(0.2) = -1.39\\).\nThe logistic function also has a pretty interesting derivative. The slickest\rway to show the following result is to use implicit differentiation, but\rI’ll show a longer and less magical derivation which only uses the chain rule\rand basic algebra.\n\\[ \\begin{split}\r\\frac{d}{dx} \\sigma(x) \u0026amp; = \\frac{d}{dx} \\frac{1}{1 + e^{-x}} \\\\\r\u0026amp; = \\frac{-e^{-x}}{( 1+ e^{-x})^2} \\\\\r\u0026amp; = \\frac{1}{1 + e^{-x}} \\frac{1 + e^{-x} - 1}{ 1+ e^{-x}} \\\\\r\u0026amp; = \\frac{1}{1 + e^{-x}} \\Big( \\frac{1 + e^{-x}}{1+ e^{-x}} - \\frac{1}{ 1+ e^{-x}} \\Big) \\\\\r\u0026amp; = \\frac{1}{1 + e^{-x}} \\Big( 1 - \\frac{1}{ 1+ e^{-x}} \\Big) \\\\\r\u0026amp; = \\sigma(x) ( 1 - \\sigma(x) ) \\end{split}\r\\]\nSo we can see that \\(\\sigma\u0026#39;(x)\\) can expressed as a simple quadratic function of\r\\(\\sigma(x)\\). It’s not often that a derivative can be most conveniently\rexpressed in terms of the original function, but it turned out to be the case\rhere. This apparently useless fact is actually quite important numerically\rbecause it means we can calculate \\(\\sigma\u0026#39;(x)\\) with only a single multiplication\rassuming that we already know \\(\\sigma(x)\\).\nFor our Python implementation, we will need a vectorized implementation of\r\\(\\sigma()\\). This function applies the sigmoid function element-wise to every\relement in a numpy.ndarray of any shape.\nimport numpy as np\rdef sigmoid(z):\rreturn 1 / (1 + np.exp(-z))\r\rStatistical Motivation\rLet \\(Y\\) be a discrete random variable with support on \\(\\{0, 1\\}\\) and let \\(X\\) be a\rrandom \\(m\\)-vector. Let’s assume that the joint probability distribution \\(F_{X,Y}\\)\rof \\(X\\) and \\(Y\\) has the following sigmoid form for some real-valued vector \\(\\Theta\\):\n\\[ E[Y|X] = P(Y=1|X) = \\sigma(X\\Theta) \\]\nNow let’s say \\(\\mathbf{X}\\) is an \\(n \\times m\\) matrix and \\(\\mathbf{y}\\) is an \\(n\\)-vector such that\r\\((\\mathbf{y}_i, \\mathbf{X}_i)\\) are \\(n\\) realizations sampled independently from\r\\(F_{X,Y}\\); then we can write down the likelihood function:\n\\[ L(\\Theta; \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^n P(Y=1|X=\\mathbf{X}_i)^{\\mathbf{y}_i} P(Y=0|X=\\mathbf{X}_i)^{(1 - \\mathbf{y}_i)} \\]\nI should probably explain the notational trick used here: because \\(y \\in \\{0, 1\\}\\), the first factor will be reduced to a constant \\(1\\) if \\(y = 0\\) and\rlikewise the second term will be reduced to a constant \\(1\\) if \\(y = 1\\). So\rputting \\(y\\) and \\(1-y\\) in the exponent is merely a compact way of writing the two mutually exclusive scenarios\rwithout an explicit if/else.\nWe can simplify this expression by taking the log of both sides and working\rwith the log-likelihood \\(\\ell\\) from now on:\n\\[ \\ell(\\Theta; \\mathbf{X},\\mathbf{y}) = \\ln L = \\sum_{i=1}^n \\mathbf{y}_i \\ln P(Y=1|X=\\mathbf{X}_i) + (1 - \\mathbf{y}_i) \\ln (1 - P(Y=1|X=\\mathbf{X}_i) \\]\nSubstituting \\(\\hat{\\mathbf{y}} = P(Y=1|X)\\):\n\\[ \\ell(\\Theta; \\mathbf{X},\\mathbf{y}) = \\ln L = \\sum_{i=1}^n \\mathbf{y}_i \\ln \\hat{\\mathbf{y}}_i + (1 - \\mathbf{\\mathbf{y}}_i) \\ln (1 - \\hat{\\mathbf{y}}_i) \\]\nBecause \\(\\ln()\\) is monotonically increasing, it suffices to minimize\r\\(\\ell(\\Theta; \\mathbf{X}, \\mathbf{y})\\) with respect to \\(\\Theta\\) in order to find the maximum\rlikelihood estimate. Because \\(\\ell\\) is convex and has a continuous derivative, we\rcan find this maximum by solving \\(\\nabla \\ell = 0\\).\n\\[ \\begin{align}\r\\frac{\\partial \\ell}{\\partial \\Theta} \u0026amp; = \\sum_{i=1}^n \\frac{\\mathbf{y}_i}{\\hat{\\mathbf{y}}_i} \\frac{\\partial \\hat{\\mathbf{y}}_i}{\\partial \\Theta} -\r\\frac{(1-\\mathbf{y}_i)}{(1-\\hat{\\mathbf{y}}_i)} \\frac{\\partial \\hat{\\mathbf{y}}_i}{\\partial \\Theta}\r\\end{align}\r\\]\nWe can use our earlier lemma \\(\\sigma\u0026#39;(x) = \\sigma(x)(1-\\sigma(x))\\) for the partial derivative of \\(\\hat{y}\\). Note also that because \\(\\hat{y}_i = \\sigma(\\mathbf{X}_i^T \\Theta)\\), we will pick up an additional \\(\\mathbf{X}_i\\) from the chain rule when differentiating by \\(\\Theta\\).\n\\[ \\begin{align}\r\\frac{\\partial \\ell}{\\partial \\Theta} = \\sum_{i=1}^n \\frac{\\mathbf{y}_i}{\\hat{\\mathbf{y}}_i} \\hat{\\mathbf{y}}_i (1-\\hat{\\mathbf{y}}_i) \\mathbf{X}_i - \\frac{(1-\\mathbf{y}_i)}{(1-\\hat{\\mathbf{y}}_i)} \\hat{\\mathbf{y}}_i (1-\\hat{\\mathbf{y}}_i) \\mathbf{X}_i \\end{align}\r\\]\nWe can use simple algebra to simplify this a great deal, and in the end we are left with:\n\\[ \\begin{align}\r\\frac{\\partial \\ell}{\\partial \\Theta} \u0026amp; = \\sum_{i=1}^n \\mathbf{X}_i ( \\mathbf{y}_i (1 - \\hat{\\mathbf{y}}_i) - (1 - \\mathbf{y}_i) \\hat{\\mathbf{y}}_i ) \\\\\r\u0026amp; = \\sum_{i=1}^n \\mathbf{X}_i ( \\mathbf{y}_i - \\mathbf{y}_i \\hat{\\mathbf{y}}_i - \\hat{\\mathbf{y}}_i + \\mathbf{y}_i \\hat{\\mathbf{y}}_i) \\\\\r\u0026amp; = \\sum_{i=1}^n \\mathbf{X}_i ( \\mathbf{y}_i - \\hat{\\mathbf{y}}_i ) \\\\\r\u0026amp; = \\mathbf{X}^T ( \\mathbf{y} - \\hat{\\mathbf{y}} ) \\tag{1} \\\\\r\\end{align}\r\\]\nEquation (1) is the clearest way to express the gradient, but because \\(\\hat{\\mathbf{y}}\\) is an implicit function of \\(\\mathbf{X}\\) and\r\\(\\Theta\\) is can appear a little magical. A fully explicit version is:\n\\[\r\\frac{\\partial \\ell}{\\partial \\Theta} = \\mathbf{X}^T ( \\mathbf{y} - \\sigma(\\mathbf{X} \\Theta) ) \\tag{2}\r\\]\nIn plain english, this says: go through the rows of \\(\\mathbf{X}\\) one by one. For each row, figure out if the prediction \\(\\hat{\\mathbf{y}}\\)\rwas too high or too low by computing the difference \\(\\mathbf{y}_i - \\hat{\\mathbf{y}}_i\\). If it’s too high, subtract the row \\(X_i\\) from\rthe gradient, and if it’s too low, then add the row \\(X_i\\) to the gradient, in each case weighting by the magnitude in the difference.\nIn theory, we can now find the minimum \\(\\hat{\\Theta}\\) by simply solving equation (3):\n\\[\r\\mathbf{X}^T ( \\mathbf{y} - \\sigma(\\mathbf{X} \\hat{\\Theta}) ) = 0 \\tag{3}\r\\]\nUnfortunately, unlike the normal equation of ordinary least squares, equation (3) cannot be solved by the\rmethods of linear algebra due to the presence of the non-linear \\(\\sigma\\) function.\rHowever, it is amenable to numerical methods, which we will cover in great detail below.\nYou may remark that equation (3) is almost suspiciously neat, but it’s not really an accident. The reason\rwhy we chose the logistic function in the first place was precisely so this\rresult would drop out at the end. Indeed, there are other choices with an equally\rgood theoretical basis, yet we chose logit because it’s simple and fast to calculate.\nOne such alternative, preferred by some statisticians and scientific\rspecializations, is the probit link function, which uses the CDF of the\rnormal distribution instead of sigmoid. However, in practice these curves are\rextremely similar, and if you showed me an unlabeled plot with both of them I\rcould not for the life of me tell you which is which:\nIt’s worth bearing in mind that logistic regression is so popular, not because\rthere’s some theorem which proves it’s the model to use, but because it is\rthe simplest and easiest to work with out of a family of equally valid choices.\n\rGradient Descent\rThe state-of-the-art algorithm that we will use to solve (3) has a large number of moving parts and is\rsomewhat overwhelming to understand at once. Therefore, we will implement it in layers,\radding sophistication at each layers as well as taking benchmarks that will\rconcretely demonstrate the value of each added complication. The layers will be:\n(Batch) Gradient Descent\rMinibatch Stochastic Gradient Descent\rNesterov Accelerated Gradient\rEarly Stopping\r\r\rBatch Gradient Descent\rSince we have both the loss function \\(J\\) we want to minimize and its gradient \\(\\nabla J\\)\rwe can use an algorithm called gradient descent to find a minimum.\nGradient descent is an iterative method that simply updates an approximation of\r\\(\\hat{\\Theta}\\) by taking a smalls step in the direction of steepest descent. Let us\rdenote this sequence of approximations \\(\\hat{\\Theta}_t\\). Initialize \\(\\hat{\\Theta}_0\\)\rarbitrarily and use the following update rule for \\(t\u0026gt;0\\):\n\\[ \\hat{\\Theta}_{i+1} := \\hat{\\Theta}_i - \\alpha \\nabla \\ell (\\hat{\\Theta}_i) \\]\nFor some suitable learning rate \\(\\alpha\\) this will always converge, and because\r\\(\\ell\\) is convex it will in fact always converge to the unique global minimum \\(\\hat{\\Theta}\\)\rwhich is the maximum likelihood estimate for \\(\\Theta\\).\nclass LogisticClassifier:\rdef __init__(self, learning_rate=0.1, tolerance=1e-4, max_iter=1000):\r# gradient descent parameters\rself.learning_rate = float(learning_rate)\rself.tolerance = float(tolerance)\rself.max_iter = int(max_iter)\r# how to construct a the design matrix\rself.add_intercept = True\rself.center = True\rself.scale = True\rself.training_loss_history = []\rdef _design_matrix(self, X):\rif self.center:\rX = X - self.means\rif self.scale:\rX = X / self.standard_error\rif self.add_intercept:\rX = np.hstack([ np.ones((X.shape[0], 1)), X])\rreturn X\rdef fit_center_scale(self, X):\rself.means = X.mean(axis=0)\rself.standard_error = np.std(X, axis=0)\rdef fit(self, X, y):\rself.fit_center_scale(X)\r# add intercept column to the design matrix\rn, k = X.shape\rX = self._design_matrix(X)\r# used for the convergence check\rprevious_loss = -float(\u0026#39;inf\u0026#39;)\rself.converged = False\r# initialize parameters\rself.beta = np.zeros(k + (1 if self.add_intercept else 0))\rfor i in range(self.max_iter):\ry_hat = sigmoid(X @ self.beta)\rself.loss = np.mean(-y * np.log(y_hat) - (1-y) * np.log(1-y_hat))\r# convergence check\rif abs(previous_loss - self.loss) \u0026lt; self.tolerance:\rself.converged = True\rbreak\relse:\rprevious_loss = self.loss\r# gradient descent\rresiduals = (y_hat - y).reshape( (n, 1) )\rgradient = (X * residuals).mean(axis=0)\rself.beta -= self.learning_rate * gradient\rself.iterations = i+1\rdef predict_proba(self, X):\r# add intercept column to the design matrix\rX = self._design_matrix(X)\rreturn sigmoid(X @ self.beta) def predict(self, X):\rreturn (self.predict_proba(X) \u0026gt; 0.5).astype(int)\rGrab some test-only dependencies:\n# dependencies for testing and evaluating the model\rfrom sklearn.datasets import load_breast_cancer\rfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\rimport matplotlib.pyplot as plt\rfrom sklearn.datasets import load_breast_cancer\rfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\rimport matplotlib.pyplot as plt\r%matplotlib inline\rIf we fit this naive model to a smallish dataset:\nmodel = LogisticClassifier(tolerance=1e-5, max_iter=2000)\r%time model.fit(X_train, y_train)\rmodel.converged, model.iterations, model.loss\r\rCPU times: user 2.78 s, sys: 3.31 s, total: 6.09 s\rWall time: 3.07 s\n(True, 1094, 0.063013434109990218)\n\rSo it took 1000 iterations and 3 seconds to converge. That’s not great, and we’ll see how\rto improve it in just a minute. First, though, let’s take a look at how the model performs\ron the test set:\np_hat = model.predict_proba(X_test)\ry_hat = model.predict(X_test)\raccuracy_score(y_test, y_hat)\r\r0.9707602339181286\n\r97% accuracy is quite good. We can get a slightly deeper look at this result by looking\rat the confusion matrix for the \\(p\u0026lt;0.5\\) decision rule:\n\r\r\rActual Positive\rActual Negative\r\r\r\rPredicted Positive\r109\r3\r\rPredicted Negative\r2\r57\r\r\r\rThis tells us we’re doing roughly equally well at classifying negatives and positives\rso our high accuracy is not due simply to unbalanced class prevalence - the model has\rreal insight. Nevertheless, we can try out different breakpoints to see if that makes\rany difference.\nfpr, tpr, threshold = roc_curve(y_test, p_hat)\rfpr = np.concatenate([[0], fpr])\rtpr = np.concatenate([[0], tpr])\rthreshold = np.concatenate([[0], threshold])\rplt.figure(figsize=(10,6))\rplt.step(fpr, tpr, color=\u0026#39;black\u0026#39;)\rplt.xlabel(\u0026quot;False Positive Rate\u0026quot;)\rplt.ylabel(\u0026quot;True Positive Rate\u0026quot;)\rplt.title(\u0026quot;ROC Curve\u0026quot;)\rplt.plot([0,1], [0,1], linestyle=\u0026#39;--\u0026#39;, color=\u0026#39;gray\u0026#39;)\rplt.text(0.4, 0.6, \u0026#39;AUC: {:.4f}\u0026#39;.format(roc_auc_score(y_test, p_hat)))\rAUC ROC Curve\n\rThe models final performance seems quite good, but it’s not really possible to\rtell from the above graphs if it’s as good as we can do or not. In particular,\rwe could be overfitting or underfitting. It also seems to take a long time for\rthe algorithm to converge (1000 epochs and 3 seconds to converge on just 400\rdata points in a low-dimensional space? Really?) One powerful diagnostic tool\rfor evaluating these kinds of problems is to plot both training and validation\rloss as a function of iteration. (To do this, it is necessary to instrument\rthe above code to record these two measurements at the end of every iteration\rin the fit() function, but I’ve omitted such details in the code above.)\nplt.figure(figsize=(16,6))\rplt.ylim(0, 1)\rplt.xscale(\u0026#39;log\u0026#39;)\rplt.plot(\rrange(1, model.iterations+1), model.training_loss_history, label=\u0026#39;Training Loss\u0026#39;)\rplt.plot(\rrange(1, model.iterations+1), model.validation_loss_history, label=\u0026#39;Validation Loss\u0026#39;)\rplt.xlabel(\u0026quot;iteration\u0026quot;)\rplt.ylabel(\u0026quot;loss-loss\u0026quot;)\rplt.title(\u0026quot;Training/Validation Loss Curve\u0026quot;)\rplt.legend()\rplt.grid(True, \u0026#39;both\u0026#39;)\rplt.plot(\r[model.training_loss_history.index(min(model.training_loss_history))], [min(model.training_loss_history)], marker=\u0026#39;o\u0026#39;, markersize=7, markerfacecolor=\u0026#39;none\u0026#39;, color=\u0026quot;blue\u0026quot;)\rplt.plot(\r[model.validation_loss_history.index(min(model.validation_loss_history))], [min(model.validation_loss_history)], marker=\u0026#39;o\u0026#39;, markersize=7, markerfacecolor=\u0026#39;none\u0026#39;, color=\u0026quot;orange\u0026quot;)\rTraining/Validation Loss Curve\n\rNote the logarithmic scale used for the x-axis. While we get most of our\rperformance in the first 100 iterations, we continue to make incremental\rprogress up past 1000, at which point we reach our \\(10^{-5}\\) tolerance for\rconvergence. Because the validation curve also decreases monotonically we at\rleast know we are not overfitting.\nSo, the batch gradient descent algorithm is finding parameters which minimize\rtraining loss to 5 decimal places, which in turn allow it to achieve 97% accuracy on\rthe validation set. There’s no evidence of overfitting. The only real concern\ris the slow convergence and poor runtime performance. If it takes us 3 seconds\rto fit 400 data points, how are we going to deal with 400,000 or 40 million?\rWhy is this algorithm so slow, and is there anything we can do about it?\n\rMini-batch Stochastic Gradient Descent\rStochastic gradient descent may also have properties conceptually similar to\rsimulated annealing which possibly allows it to “jump” out of shallow local\rminima giving it a better chance of finding a true global minimum. (The concept\rof a “shallow” vs. “deep” local minimum of a log-likelihood is related to the\rCramer-Rao bound which implies that our estimate of parameters \\(\\hat{\\Theta}\\)\rwill have less variance under a hypothetical resampling of the training set\rfrom the true population if we are at the bottom of a “deep” local minimum\rwith high curvature than if we merely in a “shallow” local minimum with low\rcurvature. This in turn relates to the bias-variance tradeoff in the\rcontext of statistical learning theory. Therefore “shallow” minima\rare bad because if we refit we get very different parameters which increases\rexpected generalization error which is probably the single most meaningful\rmeasure of a models performance in real-world scenarios.) Whether or\rnot SGD really has this property or not isn’t something I feel qualified\rto weigh in on, but it’s a good story, isn’t it?\nThe reason batch gradient descent is slow is that we evaluate the gradient\rover the entire training set each time we update the parameters. Not only\ris this fairly expensive, but it violates a basic principle for making gradient\rdescent algorithm converge rapidly:\n\rGradients evaluated at a point close to a minima provide more information\rabout the true location of the minima than gradients evaluated far from a\rminima.\n\r(This principle actually leads to two important optimizations; we’ll see the\rother shortly.)\nA consequence is that if we have the opportunity to take lots of cheap steps\rinstead one expensive step, we should take it. The first step will get us\rsomewhat closer to a minima which makes the later gradient evaluations more\rvaluable.\nBecause our loss and gradient functions are written as a sum over the rows of\rsome training set, we have a natural way to divide up the work. One approach is\rto go as far as possible and treat every individual row in the training set as\ra separate step. This is called stochastic gradient descent. While stochastic\rgradient descent is fairly stellar early on and can get close to a true minima\rin just a few minutes, it struggles to converge to an exact value later on when\rit is close to the final solution. Instead, it can oscillate back and forth\raround a minima as every row moves it in a contradictory direction. Another\rissue with SGD is that we can’t really take advantage of vectorization. 10\rsingle row updates can easily take 5 times as long as a single vectorized\roperation on 10-row matrices.\nA good compromise is something called mini-batch gradient descent. We pick a\rbatch size, say between 5 and 100 records each, and partition our training set\rinto as many batches as necessary. It’s OK if some batches are bigger or\rsmaller than the chosen batch size. We do want to ensure that each example is\rincluded in one and only one batch, though. It’s also recommended to randomize\rwhich examples are placed in which batch every time we pass through the data.\rThe benefits are two-fold: not only are batch steps more likely to in roughly\rthe right direction towards the minima without the back-and-forth pathology of\rSGD, but we will also get to exploit whatever vectorization primitives our\rhardware offers. If we’re doing 64-bit floating point arithmetic on an Intel\rCPU chip with the AVX instruction set, we may see a 4X speed up, for\rexample. The exact benefit, if any should be determined experimentally.\nA single pass through all of the batches means that each example in the\rtraining set has contributed to the gradient exactly once. We call that\rone epoch. It should be clear that one epoch requires roughly the\requivalent amount of processing as a single iteration of batch gradient\rdescent. Therefore, some of the things we did at the end of each iteration\rfor batch gradient descent, like convergence checking, we will instead\rdo only at the end of each epoch when doing mini-batch stochastic gradient\rdescent.\nLeaving all the surrounding code more or less the same, we can implement\rmini-batch SGD by adding an inner loop inside our fit function.\ndef fit(self, X, y):\rself.fit_center_scale(X)\r# add intercept column to the design matrix\rn, k = X.shape\rX = self._design_matrix(X)\r# used for the convergence check\rprevious_loss = -float(\u0026#39;inf\u0026#39;)\rself.converged = False\r# initialize parameters\rself.beta = np.zeros(k + (1 if self.add_intercept else 0))\rmomentum = self.beta * 0\rfor i in range(self.max_iter):\rshuffle = np.random.permutation(len(y))\rX = X[shuffle, :]\ry = y[shuffle]\r# if batch_size does not evenly divide n, we\u0026#39;ll one more\r# batch of size less than batch_size at the end.\rrunt = (1 if n % self.batch_size else 0)\rfor batch_index in range(n // self.batch_size + runt):\rbatch_slice = slice(\rself.batch_size * batch_index, self.batch_size * (batch_index + 1) )\rX_batch = X[batch_slice, :]\ry_batch = y[batch_slice]\ry_hat = sigmoid(X_batch @ self.beta)\r# gradient descent\rresiduals = (y_hat - y_batch).reshape( (X_batch.shape[0], 1) )\rgradient = (X_batch * residuals).mean(axis=0)\rmomentum = 0.8 * momentum + self.learning_rate * gradient\rself.beta -= momentum\r# with minibatch, we only check convergence at the end of every epoch. y_hat = sigmoid(X @ self.beta)\rself.loss = np.mean(-y * np.log(y_hat) - (1-y) * np.log(1-y_hat))\rif abs(previous_loss - self.loss) \u0026lt; self.tolerance:\rself.converged = True\rbreak\relse:\rprevious_loss = self.loss\rself.iterations = i+1\rLet’s repeat the above tests to see if that improved things:\nmodel = LogisticClassifier(tolerance=1e-5, batch_size=8)\r%time model.fit(X_train, y_train)\rmodel.converged, model.iterations, model.loss\r\rCPU times: user 1.36 s, sys: 1.23 s, total: 2.59 s\rWall time: 1.31 s\n(True, 373, 0.049284666911268454)\n\rIt now takes 1.4 seconds to converge instead of 4 seconds: a 2X wall-clock\rspeed up. It only takes 373 epochs (full passes through the data) instead of the 1,000 for batch gradient\rdescent. And the final result has achieved similar test performance:\nTraining/Validation Loss Curve Minibatch\n\r\rNesterov Accelerated Gradient\rBut we aren’t even close to done. The next bell-and-whistle on our “Making SGD\rAwesome” whistle-stop tour is a clever idea called momentum. While there are\rseveral competing approaches to implementing momentum, we’ll implement a version\rcalled Nesterov Accelerated Gradient.\nThe basic idea behind momentum is very simple. We want to take the gradient\rcontribution of a given batch and spread it over a number of updates. This\ris similar to nudging a golf ball in a given direction and allowing it to\rroll to a stop, hence the name, momentum. How can we achieve this is in a fair\rway and give the same weight to all batches? By relying on the convergence\rproperty of the geometric series. Let’s say we checked the gradient and decided\rthat we wanted to update our parameter \\(\\Theta\\) by a vector \\(v = (+2,-2)\\).\rInstead of applying that all in one update, we could apply \\(v_0 = (+1,-1)\\)\rthe first step, \\(v_1 = (+\\frac{1}{2}, -\\frac{1}{2})\\) the second step, \\(v_2 = (+\\frac{1}{4}, -\\frac{1}{4})\\)\rand so on out to infinity. Then by the limit of the geometric series, we\rknow that the total contribution after a long time converges to \\((+2,-2)\\).\rMore generally, we can choose any decay rate less than one and still enjoy\rconvergence.\n\\[\r\\sum_{n=0}^\\infty a^n = 1 + a + a^2 + a^3 + ... = \\frac{1}{1-a} \\,\\, \\forall a \\in \\mathbb{R}, 0 \\leq a \\le 1\r\\]\nThat leads to a rule (in pseudocode) like:\nmomentum = zeros(shape=beta.shape)\rfor epoch in range(1000):\rfor batch in partition_data(X, 8):\rmomentum = momentum * decay_rate - learning_rate * gradient(beta, batch)\rbeta += momentum\rWhich is fine, but it turns out we can do strictly better for no extra work if\rwe remember our principle from earlier: gradients taken near a minima are more\rvaluable than those taken further away. In the above pseudocode, we evaluation\rthe gradient at the beta from the previous step, even though we know that\rpart of the update will be to add in the old momentum. So why don’t we do that\rfirst and then take the gradient at that point instead? It’s a little bit\rlike peaking ahead.\nTo help you remember, some mnemonic imagery:\n\rCaptain Nesterov stands on the bridge of his submarine. The situation is tense.\rIn the waters above, a Canadian sub-hunter is searching for them. The captain\rknows that if he can gently land his sub exactly at the lowest point of a\rnearby indentation in the sea floor, the sub-hunters scan will read them as a\rnatural formation. But if the submarine isn’t at the bottom of the indentation,\rthen they stick out like a sore thumb when the Canadians compare them to their\rprevious topographical survey maps. The captain also knows that he can use at\rmost one ping to measure the depth and angle of the sea floor below them.\rAfter that, the sub-hunter will be on high alert and be able to triangulate\rthem on the second ping. Life and death for himself and his entire crew is on\rthe line. The ship glides in silent mode through the inky depths. Nervous, the\rinexperienced sonar operator cracks. “Do you want me to ping, sir?” “No”, Captain\rNesterov replies, “not yet.” the submarine glides on momentum for another\rtense minute, gradually slowing. Only when the helmsman reports that their\rspeed as dropped another 10% to just 4 knots does he order the ping. By now,\rthe ship has glided to almost the exact center of the depression, and one\rfinal course correction sees the ship safely nestled on the sandy ocean floor\rless than a meter from the lowest point of the depression.\n\rThe code for this is straight-forward. Occasionally you’ll see versions\rof this where the author has bent over backwards to ensure that the prior momentum\rterm is incorporated just once, but this is best left as an exercise for the reader.\rIt has no impact on performance unless a terribly large number of parameters are\rin play.\ndef fit(self, X, y):\rself.fit_center_scale(X)\r# add intercept column to the design matrix\rn, k = X.shape\rX = self._design_matrix(X)\r# used for the convergence check\rprevious_loss = -float(\u0026#39;inf\u0026#39;)\rself.converged = False\rself.stopped_early = False\r# initialize parameters\rself.beta = np.zeros(k + (1 if self.add_intercept else 0))\rmomentum = self.beta * 0\rfor i in range(self.max_iter):\rshuffle = np.random.permutation(len(y))\rX = X[shuffle, :]\ry = y[shuffle]\r# if batch_size does not evenly divide n, we\u0026#39;ll one more\r# batch of size less than batch_size at the end.\rrunt = (1 if n % self.batch_size else 0)\rfor batch_index in range(n // self.batch_size + runt):\rbatch_slice = slice(\rself.batch_size * batch_index, self.batch_size * (batch_index + 1) )\rX_batch = X[batch_slice, :]\ry_batch = y[batch_slice]\rbeta_ahead = self.beta + self.momentum_decay * momentum\ry_hat = sigmoid(X_batch @ beta_ahead)\r# gradient descent\rresiduals = (y_hat - y_batch).reshape( (X_batch.shape[0], 1) )\rgradient = (X_batch * residuals).mean(axis=0)\rmomentum = self.momentum_decay * momentum - self.learning_rate * gradient\rself.beta += momentum\r# with minibatch, we only check convergence at the end of every epoch. y_hat = sigmoid(X @ self.beta)\rself.loss = np.mean(-y * np.log(y_hat) - (1-y) * np.log(1-y_hat))\rif abs(previous_loss - self.loss) \u0026lt; self.tolerance:\rself.converged = True\rbreak\relse:\rprevious_loss = self.loss\rself.iterations = i+1\rWith our new accelerated momentum, we once again measure performance:\nmodel = LogisticClassifier(\rtolerance=1e-5, validation_set=(X_test, y_test))\r%time model.fit(X_train, y_train)\rmodel.converged, model.stopped_early, model.iterations, model.loss\r\rCPU times: user 100 ms, sys: 140 ms, total: 240 ms\rWall time: 123 ms\n(True, False, 30, 0.046659741686488669)\n\rAnother simple change to our algorithm, another impressive speed-up. We’re now converging\rten times faster than the version without Nesterov Accelerated Momentum, and 25 times\rfaster than the naive batch gradient descent.\nIn fact, we’re now converging so fast and efficiently on the training set that a new problem\rhas emerged. Take a look at these loss curves:\nTraining/Validation Loss Curve\n\rA new phenomenon is occurring: after just a few iterations test set performance\rfirst levels off and then actually begins to get worse even as training\rperformance continues to improve. This is actually a classic illustration of\rthe well-known bias-variance trade-off. In fact, the reason we’ve been obsessively\rrecording and plotting training and validation loss for every experiment\ris because we were on the lookout for the exact phenomenon. And now our caution\rhas paid dividends, because we’ve caught a example of overfitting.\nOne solution to this kind of overfitting - I mean the kind that gets worse and\rworse as the number of iterations increases - is to treat max_iterations as a\rhyperparameter and to use a grid search to find the optimal number of\riterations. Or to add regularization, to to play around with batch size, etc.\rBut there’s a rather clever solution which is as close to a free lunch as\ranything I know in machine learning, in the sense that it saves computation\rtime, minimizes generalization error, and basically costs nothing and has no\rdownside except for a loss in theoretical rigor. This piece of (black?) magic\ris called early stopping.\n\rEarly Stopping\rThe basic idea behind early stopping is taken from looking at the shape of the\rabove validation loss curve. If we want to minimize validation loss, and we\rknow that it goes down for a while then starts to rise, why don’t we just\rstop once it starts to go back up? Really the only wrinkle is that because\rthe validation loss curve is a little noisy we don’t want to stop the first\rtime we see validation loss rise even a little bit but rather wait to make sure\rit’s the start of an upward trend before we pull the rip-cord. We can do that\rsimply by waiting for a certain number of epochs with no improvement in\rvalidation loss.\nOne theoretical problem with early stopping is that the parameters estimated in\rthis way are not the maximum likelihood estimates! This makes it harder to\rreason about them from a statistical point of view. One justification is that\ralthough we haven’t found an MLE, we are performing empirical risk\rminimization and have found a step of parameters that generalize\roptimally to the validation set. That in turn raises more questions because we\rhaven’t actually minimized empirical risk globally but only along the path of\rsteepest of descent traced out by gradient descent. Ultimately this is a\rpragmatic technique recommended mainly by its simplicity, improved validation\rand test set performance, and decreased training times.\nHere is the “final” version of the code for the LogisticClassifier, including\rimplementation details omitted above:\nclass LogisticClassifier:\rdef __init__(self, learning_rate=0.1, tolerance=1e-4, max_iter=1000, batch_size=32, momentum_decay=0.9, early_stopping=3, validation_set=(None,None)):\r# gradient descent parameters\rself.learning_rate = float(learning_rate)\rself.tolerance = float(tolerance)\rself.max_iter = int(max_iter)\rself.batch_size=32\rself.momentum_decay = float(momentum_decay)\rself.early_stopping = int(early_stopping)\rself.X_validation, self.y_validation = validation_set\r# how to construct a the design matrix\rself.add_intercept = True\rself.center = True\rself.scale = True\rself.training_loss_history = []\rdef _design_matrix(self, X):\rif self.center:\rX = X - self.means\rif self.scale:\rX = X / self.standard_error\rif self.add_intercept:\rX = np.hstack([np.ones((X.shape[0], 1)), X])\rreturn X\rdef fit_center_scale(self, X):\rself.means = X.mean(axis=0)\rself.standard_error = np.std(X, axis=0)\rdef fit(self, X, y):\rself.fit_center_scale(X)\r# add intercept column to the design matrix\rn, k = X.shape\rX = self._design_matrix(X)\r# used for the convergence check\rprevious_loss = -float(\u0026#39;inf\u0026#39;)\rself.converged = False\rself.stopped_early = False\r# initialize parameters\rself.beta = np.zeros(k + (1 if self.add_intercept else 0))\rmomentum = self.beta * 0 # trick to get the same shape and dtype as beta\rfor i in range(self.max_iter):\rshuffle = np.random.permutation(len(y))\rX = X[shuffle, :]\ry = y[shuffle]\r# if batch_size does not evenly divide n, we\u0026#39;ll one more\r# batch of size less than batch_size at the end.\rrunt = (1 if n % self.batch_size else 0)\r# if batch_size does not evenly divide n, we\u0026#39;ll one more\r# batch of size less than batch_size at the end.\rrunt = (1 if n % self.batch_size else 0)\rfor batch_index in range(n // self.batch_size + runt):\rbatch_slice = slice(\rself.batch_size * batch_index, self.batch_size * (batch_index + 1) )\rX_batch = X[batch_slice, :]\ry_batch = y[batch_slice]\rbeta_ahead = self.beta + self.momentum_decay * momentum\ry_hat = sigmoid(X_batch @ beta_ahead)\r# gradient descent\rresiduals = (y_hat - y_batch).reshape( (X_batch.shape[0], 1) )\rgradient = (X_batch * residuals).mean(axis=0)\rmomentum = self.momentum_decay * momentum - self.learning_rate * gradient\rself.beta += momentum\r# with minibatch, we only check convergence at the end of every epoch. y_hat = sigmoid(X @ self.beta)\rself.loss = np.mean(-y * np.log(y_hat) - (1-y) * np.log(1-y_hat))\rself.training_loss_history.append(self.loss)\r# early stopping\rif self.check_validation_loss():\rself.stopped_early = True\rbreak if abs(previous_loss - self.loss) \u0026lt; self.tolerance:\rself.converged = True\rbreak\relse:\rprevious_loss = self.loss\rself.iterations = i+1\rdef predict_proba(self, X):\r# add intercept column to the design matrix\rX = self._design_matrix(X)\rreturn sigmoid(X @ self.beta) def predict(self, X):\rreturn (self.predict_proba(X) \u0026gt; 0.5).astype(int)\rdef check_validation_loss(self):\r# validation set loss\rif not hasattr(self, \u0026#39;validation_loss_history\u0026#39;):\rself.validation_loss_history = []\rp_hat = self.predict_proba(self.X_validation)\rloss = np.mean(-self.y_validation * np.log(p_hat) - \\\r(1-self.y_validation) * np.log(1-p_hat))\rself.validation_loss_history.append(loss)\r# AUC ROC history\rif not hasattr(self, \u0026#39;auc_history\u0026#39;):\rself.auc_history = []\rauc = roc_auc_score(self.y_validation, p_hat)\rself.auc_history.append(auc)\rt = self.early_stopping\rif t and len(self.validation_loss_history) \u0026gt; t * 2:\rrecent_best = min(self.validation_loss_history[-t:])\rprevious_best = min(self.validation_loss_history[:-t])\rif recent_best \u0026gt; previous_best:\rreturn True\rreturn False\rOK, one last time: how’s the performance?\nmodel = LogisticClassifier(\rtolerance=1e-5, early_stopping=3,\rvalidation_set=(X_test, y_test))\r%time model.fit(X_train, y_train)\rmodel.converged, model.stopped_early, model.iterations, model.loss\r\rCPU times: user 40 ms, sys: 30 ms, total: 70 ms\rWall time: 49.1 ms\r(False, True, 10, 0.053536116001088603)\n\rConvergence is crazy fast. 49 ms? Is that some kind of joke? We started with\r3 seconds and now you’re telling me its 49 ms? As in, 60X faster? No, this\ris actually fairly typical. That’s why it’s so important to use a good\roptimizer instead of relying on naive methods. Luckily, with modern frameworks,\rstate-of-the-art optimizers are usually available off-the-rack.\nTest set performance (accuracy, AUC) has not suffered:\nAUC ROC Curve\n\rThe loss curves are exactly the same as before… until the curve starts climbing\rupward for a few iterations, at which point we pull the plug. In this case,\rwe stopped after just 10 iterations, compared to the 1000 needed for the batch\rgradient descent.\nTraining/Validation Loss Curve\n\r\rConclusion\rThat was logistic regression from scratch. In this article, we’ve learned about\ra simple but powerful classifier called logistic regression. We derived the\requations for MLE and, in our attempts to solve these equations numerically,\rdeveloped an incredibly powerful piece of technology: Mini-batch SGD with\rearly stopping and NAG. This optimizer is actually more important than logistic\rregression because it turns out it can be re-used for a wide variety of models.\nThere are variations on SGD that we haven’t talked about, in particular\radaptive variations which don’t need a learning_rate hyperparameter or have\rschedules for changing learning_rateor momentum_decay over time. But\rthere’s no general, one-size-fits-all solution which is strictly better than\rthe technique presented here. You will of course find endless papers and\rbenchmarks suggesting that one technique or another is better; but they’re\rare generally talking about differences less that 2X, not the 60X gained\rby these more fundamental techniques.\nI myself often use Adam as my go-to optimizer on new datasets, simply\rbecause it works even when the data isn’t centered and scaled and I don’t have\rto fiddle around with learning rate. The idea isn’t to argue that this\rparticular algorithm is the best choice for all possible scenarios - no such\ralgorithm exists. But hopefully we’ve covered the key ingredients which go into\ra state-of-the-art optimizer.\nIn the next installment of Machine Learning From Scratch,\rwe’ll explore neural networks and the backpropagation algorithm in\rparticular.\n\r","date":"December 27, 2018","href":"https://www.oranlooney.com/post/ml-from-scratch-part-2-logistic-regression/","thumbnail":"/post/ml-from-scratch-part-2-logistic-regression_files/lead.192x128.jpg","title":"ML From Scratch, Part 2: Logistic Regression"},{"content":"To kick off this series, will start with something simple yet foundational:\rlinear regression via ordinary least squares.\nWhile not exciting, linear regression finds widespread use both as a standalone\rlearning algorithm and as a building block in more advanced learning\ralgorithms. The output layer of a deep neural network trained for regression\rwith MSE loss, simple AR time series models, and the “local regression” part of\rLOWESS smoothing are all examples of linear regression being used as an\ringredient in a more sophisticated model.\nLinear regression is also the “simple harmonic oscillator” of machine learning;\rthat is to say, a pedagogical example that allows us to present\rdeep theoretical ideas about machine learning in a context that is not too\rmathematically taxing.\nThere is also the small matter of it being the most widely used supervised\rlearning algorithm in the world; although how much weight that carries I\rsuppose depends on where you are on the “applied” to “theoretical” spectrum.\nHowever, since I can already feel your eyes glazing over from such an\rintroductory topic, we can spice things up a little bit by doing something\rwhich isn’t often done in introductory machine learning - we can present the\ralgorithm that [your favorite statistical software here] actually uses to\rfit linear regression models: QR decomposition. It seems this is\rcommonly glossed over because it involves more linear algebra than can be\rgenerally assumed, or perhaps because the exact solution we will derive doesn’t\rgeneralize well to other machine learning algorithms, not even closely related variants\rsuch as regularized regression or robust regression.\nThe current paradigm in machine learning is to apply very powerful, very\rgeneral optimization algorithms that work for a wide variety of models and\rscale reasonably well to vast amounts of data. This allows researchers to\riterate rapidly on the structure of the model without needing to spend a lot of time\rcoming up with a clever algorithm which solves their special case efficiently.\rIt’s good software engineering; it avoids premature optimization and\rpromotes good separation of concerns. Still, history has shown that for\rany given optimization problem, there probably is a specialized algorithm that\rleverages the specifics of the problem to achieve an order of magnitude improvement in\rperformance. For example, John Platt’s Sequential Minimal Optimization\rbeat earlier, more general algorithms by such a wide margin that for a decade\r(1998-2009?) SVMs were one of the most promising approaches to machine\rlearning. Today (2019), the machine learning industry is in a kind of “rapid\rprototyping” mode, leveraging the flexibility and composability of deep neural\rnetworks to experiment with endless numbers of novel models. However, as our\runderstanding of which models work best for particular problems matures, the\rindustry will likely tip back in favor of researching specialized algorithms.\rIf we are interested in understanding machine learning from scratch we should\rbe prepared to study specialized algorithms when and where they arise\rnaturally.\nAnd after all, what’s a little linear algebra between friends?\nStatistical Motivation\rIn this section we will use statistical considerations to motivate the\rdefinition of a particular mathematical optimization problem. Once we have\rposed this problem, we will afterwards ignore the statistics altogether and\rfocus on numerical methods for solving the optimization problem.\nLet’s start by deriving the so-called normal equation from a statistical model.\rLet’s say that\r\\(X\\) is a random vector of length \\(m\\) and \\(Y\\) is a scalar random variable.\r\\(X\\) and \\(Y\\) are not independent, but have a joint probability\rdistribution \\(F(x, y; \\Theta, \\sigma)\\) parameterized by a non-random parameter\rvector \\(\\Theta\\), a non-negative scalar \\(\\sigma\\), and a random error term\r\\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). The model is:\n\\[ Y = X^T \\Theta + \\epsilon \\]\nNow suppose we sample \\(n\\) independent observations from \\(F\\). We place\rthese into a real-valued \\(n\\times m\\) matrix \\(\\mathbf{X}\\) and a\rreal-valued vector \\(\\mathbf{y}\\). Just to be absolutely clear, \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) are not\rrandom variables; they are the given data used to fit the model. We can then\rask, what is the likelihood of obtaining the data \\((\\mathbf{X}, \\mathbf{y})\\) given a\rparameter vector \\(\\Theta\\)? By rearranging our equation as \\(Y - X\\cdot\\Theta = \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\) and using the p.d.f. of the normal\rdistribution, we can see that:\n\\[ \\begin{align}\rL(\\Theta;\\mathbf{X},\\mathbf{y})\r\u0026amp; = P(\\mathbf{X},\\mathbf{y};\\Theta) \\\\\r\u0026amp; = \\prod_{i=1}^{n} P(\\mathbf{X}_i,\\mathbf{y}_i;\\Theta) \\\\\r\u0026amp; = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\text{exp}\\Big(\\frac{-(\\mathbf{y}_i - \\mathbf{X}_i^T\\Theta)^2}{2\\sigma^2} \\Big) \\\\\r\\end{align}\r\\]\nThat looks pretty awful, but there are a couple easy things we can do to make\rit a look a lot simpler. First, that constant term out front doesn’t matter at\rall: let’s just call it \\(C\\) or something. We can also take that\r\\(e^{-2\\sigma^2}\\) outside the product as \\(e^{-2N\\sigma^2}\\), which we’ll also\rstuff into the constant \\(C\\) because we’re only interested in \\(\\Theta\\) right now.\rFinally, we can take a log to get rid of the exponential and turn the product\rinto a sum. All together, we get the log-likelihood expression:\n\\[ \\begin{align}\r\\ell(\\Theta;\\mathbf{X},\\mathbf{y}) \u0026amp; = \\log L(\\Theta;\\mathbf{X},\\mathbf{y}) \\\\\r\u0026amp; = C - \\sum_{i=1}^N -(\\mathbf{y}_i - \\mathbf{X}^T_i\\Theta)^2 \\\\\r\u0026amp; = C - \\lVert\\mathbf{y} - \\mathbf{X}\\Theta\\rVert^2 \\\\\r\\end{align}\r\\]\nNow, because log is a monotonically increasing function, maximizing \\(\\ell\\) is the\rsame as maximizing \\(L\\). Furthermore, the constant \\(C\\) has no effect whatsoever\ron the location of the maximum. We can also remove the minus sign and consider\rthe problem as a minimization problem instead. Therefore our maximum\rlikelihood estimate of \\(\\Theta\\) for a given data set \\((X, y)\\) is simply:\n\\[ \\hat{\\Theta} \\triangleq \\underset{\\Theta}{\\text{argmin}} \\, \\lVert\\mathbf{y} - \\mathbf{X}\\Theta\\rVert^2 \\]\nIf statistics isn’t really your thing, I have some good news for you: we’re\rquits with it. Everything in this final equation is now a real-valued vector or\rmatrix: there’s not a random variable or probability distribution in sight.\rIt’s all over except for the linear algebra.\nWhat we did above was essentially a short sketch of the relevant parts of the Gauss-Markov\rtheorem. In particular, we’ve shown that the OLS solution to \\(\\mathbf{y} - \\mathbf{X}\\Theta\\) is the\rMaximum Likelihood Estimate for the parameters of the particular statistical\rmodel we started with. This isn’t true in general, but it is exactly true for linear regression\rwith a normally distributed error term. The full Gauss-Markov theorem proves a bunch of other\rnice properties: for example, it turns out this estimator is unbiased and we can even say\rthat it’s optimal in the sense that it is the best possible linear unbiased\restimator. But we won’t need these further theoretical results to implement a working model.\nIf there’s one thing you should remember, it’s this: the fact that the p.d.f.\rof the Gaussian distribution has the quadratic term \\((x -\\mu)^2\\) in the\rexponent is the reason why squared error is the “right” loss function for\rlinear regression. If the error of our linear model had a different\rdistribution, we’d have to make a different choice.\nOur key takeaway is that if it’s true that our response variable is related to\rour predictor variables by a linear equation plus a certain amount of random\rGaussian noise, then we can recover good, unbiased estimates of that linear\requations coefficients from nothing more than a finite number of data points\rsampled from the underlying distribution, and the way to actually calculate\rthose estimates is to solve the OLS problem for the data set.\n\rOrdinary Least Squares\rNote: for this next section, we’re going to be doing some light vector\rcalculus. I suggest you reference the matrix cookbook if any of the\rnotation or concepts aren’t familiar.\nLet’s call the right-hand side (the part we’re trying to minimize) \\(J\\). Then we\rhave:\n\\[ J(\\Theta) = \\lVert \\mathbf{y} - \\mathbf{X}\\Theta \\rVert^2 \\]\nAnd the problem is to minimize \\(J\\) with respect to \\(\\Theta\\). As optimization\rproblems go, this one is pretty well behaved: it’s continuous, quadratic, convex,\reverywhere continuously differentiable, and unconstrained. That’s a fancy way of saying that it’s\rshaped like a big, round bowl. A bowl has a unique lowest point and it can always be found simply\rby letting a marble roll down hill until it comes to rest exactly at the lowest point.\nBecause of these\rnice properties (and a very useful set of theorems called the KKT conditions)\rwe know that these properties guarentee that \\(J\\) has a unique global minimum and that\rwe can find the minimum - the bottom of the bowl - by finding the one place where the gradient\ris zero in all directions.\nNow, it may not be obvious at first how to\rtake the gradient of the squared norm of a vector, but recall that it is\rthe inner product of that vector with its dual:\n\\[ \\nabla_\\Theta J = \\nabla_\\Theta \\; (\\mathbf{y} - \\mathbf{X}\\Theta)^T(\\mathbf{y} - \\mathbf{X}\\Theta) \\]\nExpanding it out with FOIL:\n\\[ \\nabla_\\Theta J = \\nabla_\\Theta \\; (\r\\mathbf{y}^T \\mathbf{y} - (\\mathbf{X}\\Theta)^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X}\\Theta + \\Theta^T (\\mathbf{X}^T \\mathbf{X}) \\Theta )\r\\]\nIt’s obvious that \\(\\mathbf{y}^T \\mathbf{y}\\) is constant with respect to \\(\\Theta\\) so the first\rterm simply vanishes. It’s less obvious but also true that the next two terms\rare equal to each other - just remember that a J is a scalar, so those terms are\reach scalar, and the transpose of a scalar is itself. The final term is a\rquadratic form, and the general rule is $ x^T A x =\rA^T x + A x $ but because the product of a matrix with\ritself is always symmetric (\\(X^T X = (X^T X)^T\\)) we can use the simpler form\r\\(\\nabla x^T A x = 2 A x\\).\n\\[ \\nabla_\\Theta J = - 2 \\mathbf{X}^T \\mathbf{y} + 2 \\mathbf{X}^T \\mathbf{X} \\Theta \\]\nSetting this equal to zero at the minimum, which we will call \\(\\hat{\\Theta}\\), and dividing both sides by two, we get:\n\\[ \\mathbf{X}^T \\mathbf{X} \\hat{\\Theta} = \\mathbf{X}^T \\mathbf{y} \\tag{1} \\]\nThis is the so-called normal equation. The importance of this step is that we’ve reduced\rthe original optimization problem to a system of linear equations which may be solved purely\rby the methods of linear algegra. To see this, note that\rwe know \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), so the right hand side is a known vector, the left-hand side\ris a matrix times an unknown vector, so this is just the familiar equation for\rsolving for a particular solution to a system of equations \\(Ax = b\\).\nBecause \\(\\mathbf{X}^T \\mathbf{X}\\) is square and non-singular and therefore invertible, we\rcould just left-multiply both sides by its inverse to get an explicit closed\rform for \\(\\hat{\\Theta}\\):\n\\[ \\hat{\\Theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} X^T \\mathbf{y} \\]\nHowever, it turns out there is a faster and more numerically stable way of solving for\r\\(\\hat{\\Theta}\\) which relies on the QR Decomposition of the matrix \\(\\mathbf{X}\\).\n\rQR Decomposition\rSince we’re going to be both implementing and relying on the QR\rdecomposition in a minute, it’s worth making sure we understand how it works in\rdetail. A QR decomposition of a matrix square \\(A\\) is a product of an orthogonal\rmatrix \\(Q\\) and an upper-triangular matrix \\(R\\) such that \\(A = QR\\). It always\rexists and there’s a reasonably performant algorithm for calculating it. Why is\rit beneficial to put a matrix in this form? In short, because it makes it very easy to\rcompute solutions to systems of equations in matrix form \\(Ax = b\\); all we\rhave to do is compute \\(A = QR\\) and write the problem as \\(R x = Q^{-1} b\\)\rwhich is easy to compute. Let’s examine those two steps in more detail.\nWhy is \\(Q\\) easy to invert? Recall that \\(Q\\) is orthogonal which implies that\r\\(Q^{-1} = Q^T\\). Most linear algebra libraries don’t even have to explicitly\rcopy a matrix to take a transpose but simply set a flag that indicates that\rfrom now on it will operate on it row-wise instead of column-wise or vice\rversa. That means taking a transpose is free for all intents and purposes.\nWhy is \\(Rx = Q^T b\\) easy to solve? Well, the right-hand side is just a vector. R\ris upper triangular, so we can solve this with a technique called\rback-substitution. Back-substitution is easiest to explain with an example.\rConsider this system of equations in matrix form, where the matrix is\rupper-triangular:\n\\[\r\\begin{bmatrix}\r2 \u0026amp; 1 \u0026amp; 3 \\\\\\\r0 \u0026amp; 1 \u0026amp; 1 \\\\\r0 \u0026amp; 0 \u0026amp; 4 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rx_1 \\\\\rx_2 \\\\\rx_3 \\\\\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r2 \\\\\r2 \\\\\r8 \\\\\r\\end{bmatrix}\r\\]\nWe start on the bottom row, which is simply an equation \\(4x_3 = 8\\), so \\(x_3 = 2\\). The second row represents the equation \\(x_2 + x_3 =2\\), but we already know\r\\(x_3\\), so we can substitute that back in to get \\(x_2 - 2 = 0\\), so \\(x_2 = 0\\).\rThe top row is \\(2x_1 + x_2 + 3_x3 = 2x_1 + 6 = 2\\), so \\(x_1 = -2\\). This is\rback-substitution, and it should be clear that we can do this quickly and\refficiently for an upper-triangular matrix of any size. Furthermore, because we\rdo at most one division per row, this method is very numerically stable. (If\rthe matrix is ill-conditioned, we could still run into numerical error,\rbut this only occurs when the original data set \\(X\\) suffers from\rmulticollinearity.)\nSo hopefully you’re convinced by now that the \\(QR\\) form is desirable. But how do we\rcalculate \\(Q\\) and \\(R\\)? There are two parts to understanding the algorithm.\rFirst, note that the product of any two orthogonal matrices is itself\rorthogonal. Also, the identity matrix is orthogonal. Therefore, if we have a\rcandidate decomposition \\(A = QR\\) where \\(Q\\) is orthogonal (but R may not yet be\rsquare), then for any orthogonal matrix \\(S\\) we have \\(A = Q I R = Q S^T S R = (Q S^T) (S R) = Q\u0026#39; R\u0026#39;\\) where \\(Q\u0026#39; = Q S^T\\) and \\(R\u0026#39; = S R\\) is also a candidate\rdecomposition! This is the general strategy behind not just QR decomposition,\rbut behind many other decompositions in linear algebra: at each step we want to\rapply an orthogonal transformation designed to bring \\(R\\) closer to the desired\rform, while simultaneously keeping track of all the transformations applied so\rfar in a single matrix \\(Q\\).\nThat sets the rules and the goal of the game: we can apply any sequence of\rorthogonal transforms to a (square, non-singular) matrix \\(A\\) that will bring it\rinto upper triangular form. But what orthogonal transform will do that?\n\rHouseholder Reflections\rLet’s break it down into an even easier problem first. How would I make just\rone column of \\(A\\) zero below the diagonal? Or even more concretely, how would\rI make just the first column of \\(A\\) zero except for the first element?\nLet’s take a look at the “column view” of our matrix. It looks like this:\n\\[\r\\begin{bmatrix}\r\\vert \u0026amp; \\vert \u0026amp; \\vert \\\\\ra_1 \u0026amp; a_2 \u0026amp; a_3 \\\\\r\\vert \u0026amp; \\vert \u0026amp; \\vert\r\\end{bmatrix}\r\\]\nWe want \\(a_1\\) to be zero except for the first element. What does that mean?\rLet’s call our basis vectors \\(e_1 = [1\\, 0\\, 0]^T\\), \\(e_2 = [0\\, 1\\, 0]^T\\), \\(e_3 = [0\\, 0\\, 1]^T\\). Every vector in our space is a linear combination of these\rbasis vectors. So what it means for \\(a_1\\) to be zero except for the first\relement is that \\(a_1\\) is co-linear (in the same line) as \\(e_1\\): $ H a_i =\re_i$.\nWe’re going to do this with an orthogonal transformation. But orthogonal\rtransformations are length preserving. That means \\(\\alpha = ||a_1||\\).\rTherefore we need to find an orthogonal matrix that sends the vector \\(a_1\\) to\rthe vector \\(||a_1|| e_1\\). Note that any two vectors lie in a plane. We could\reither rotate by the angle between the vectors:\n\\[\r\\cos^{-1} \\frac{a_1 \\cdot e_1}{||a_1||}\r\\]\nor we can reflect across the line which bisects the two\rvectors in their plane. These two strategies are called the Givens\rrotation and the Householder reflection respectively. The rotation\rmatrix is slightly less stable, so we will use the Householder reflection.\nLet’s say that \\(v\\) is the unit normal vector of a plane; how would I reflect an\rarbitrary vector \\(x\\) across that plane? Well, if we subtracted \\(\\langle x, v \\rangle v\\) from \\(x\\), that would be a projection into the plane, right? So, if we\rjust keep going the same direction and for the same distance again, we’ll end\rup a point on the other side of the plane \\(x\u0026#39;\\). Both \\(x\\) and \\(x\u0026#39;\\) project to\rthe same point on the plane, and furthermore both are a distance \\(\\langle x, v \\rangle\\) from the plane. In other words, this operation is a reflection.\nThis diagram from Wikipedia illustrates this beautifully. Stare at it until you\rcan see that reflecting about the dotted plane sends \\(x\\) to \\(||x||e_1\\), and\rbelieve that \\(v\\) is a unit vector orthogonal to the dotted plane of\rreflection.\n\nBecause a reflection is a linear transformation, we can express it as a matrix,\rwhich we will call \\(H\\). Here is how we go from our geometric intuition to a\rmatrix:\n\\[\r\\begin{align}\rH x \u0026amp; \\triangleq x - 2 \\langle x, v \\rangle v \\\\\r\u0026amp; = x - 2v \\langle x, v \\rangle \\\\\r\u0026amp; = Ix - 2 v (v^T x) \\\\\r\u0026amp; = Ix - 2 (v v^T) x \\\\\r\u0026amp; = (I - 2 (v v^T)) x \\end{align}\r\\]\nHere, note that \\(v v^T\\) is the outer product of \\(v\\) with itself so is a\rsquare matrix with elements \\(v_i v_j\\). For example, if \\(v = [\\frac{1}{\\sqrt{2}} \\, \\frac{1}{\\sqrt{2}} \\, 0]^T\\) (the 45° line in the xy-plane) we get:\n\\[\rH = I - 2 v v^T =\r\\begin{bmatrix}\r1 \u0026amp; 0 \u0026amp; 0 \\\\\\\r0 \u0026amp; 1 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \\\\\r\\end{bmatrix} - 2\r\\begin{bmatrix}\r\\frac{1}{2} \u0026amp; \\frac{1}{2} \u0026amp; 0 \\\\\\\r\\frac{1}{2} \u0026amp; \\frac{1}{2} \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \\\\\r\\end{bmatrix} =\r\\begin{bmatrix}\r0 \u0026amp; -1 \u0026amp; 0 \\\\\\\r-1 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \\\\\r\\end{bmatrix} \\]\nWe now know how to define reflections which zero out the subdiagonal of a target columns,\rand we know how to construct orthogonal matrices which perform that reflection.\n\rImplementing the Lemmas\rGiven the above theoretical presentation, and copious inline comments, I hope\ryou will now be able to read and understand the following code:\ndef householder_reflection(a, e):\r\u0026#39;\u0026#39;\u0026#39;\rGiven a vector a and a unit vector e,\r(where a is non-zero and not collinear with e)\rreturns an orthogonal matrix which maps a\rinto the line of e.\r\u0026#39;\u0026#39;\u0026#39;\r# better safe than sorry.\rassert a.ndim == 1\rassert np.allclose(1, np.sum(e**2))\r# a and norm(a) * e are of equal length so\r# form an isosceles triangle. Therefore the third side\r# of the triangle is perpendicular to the line\r# that bisects the angle between a and e. This third\r# side is given by a - ||a|| e, which we will call u.\r# Since u lies in the plane spanned by a and e\r# its clear that u is actually orthogonal to a plane\r# equadistant to both a and ||a|| e. This is our\r# plane of reflection. We normalize u to v to # because a unit vector is required in the next step.\ru = a - np.sign(a[0]) * np.linalg.norm(a) * e v = u / np.linalg.norm(u)\r# derivation of the matrix form of a reflection:\r# x - 2\u0026lt;x, v\u0026gt;v ==\r# x - 2v\u0026lt;x, v\u0026gt; ==\r# Ix - 2 v (v^T x) ==\r# Ix - 2 (v v^T) x ==\r# (I - 2v v^T) x == H x\rH = np.eye(len(a)) - 2 * np.outer(v, v)\rreturn H\rWith the householder reflection in hand, we can implement an iterative\rversion of the QR decomposition algorithm, using the Householder reflection\ron each column in turn to transform A into an upper triangular matrix.\ndef qr_decomposition(A):\r\u0026#39;\u0026#39;\u0026#39;\rGiven an n x m invertable matrix A, returns the pair:\rQ an orthogonal n x m matrix\rR an upper triangular m x m matrix\rsuch that QR = A.\r\u0026#39;\u0026#39;\u0026#39;\rn, m = A.shape\rassert n \u0026gt;= m\r# Q starts as a simple identity matrix.\r# R is not yet upper-triangular, but will be.\rQ = np.eye(n)\rR = A.copy()\r# if the matrix is square, we can stop at m-1\r# because there are no elements below the pivot\r# in the last column to zero out. Otherwise we\r# need to do all m columns.\rfor i in range(m - int(n==m)):\r# we don\u0026#39;t actually need to construct it,\r# but conceptually we\u0026#39;re working to update\r# the minor matrix R[i:, i:] during the i-th\r# iteration. # the first column vector of the minor matrix.\rr = R[i:, i]\r# if r and e are already co-linear then we won\u0026#39;t\r# be able to construct the householder matrix,\r# but the good news is we won\u0026#39;t need to!\rif np.allclose(r[1:], 0):\rcontinue\r# e is the i-th basis vector of the minor matrix.\re = np.zeros(n-i)\re[0] = 1 # The householder reflection is only\r# applied to the minor matrix - the\r# rest of the matrix is left unchanged,\r# which we represent with an identity matrix.\r# Note that means H is in block diagonal form\r# where every block is orthogonal, therefore H\r# itself is orthogonal.\rH = np.eye(n)\rH[i:, i:] = householder_reflection(r, e)\r# QR = A is invariant. Proof:\r# QR = A, H^T H = I =\u0026gt; # Q H^T H R = A =\u0026gt;\r# Q\u0026#39; = Q H^T, R\u0026#39; = H R =\u0026gt;\r# Q\u0026#39; R\u0026#39; = A. QED.\r#\r# By construction, the first column of the # minor matrix now has zeros for all\r# subdiagonal matrix. By induction, we # have that all subdiagonal elements in\r# columns j\u0026lt;=i are zero. When i=N, R\r# is upper triangular. Q = Q @ H.T\rR = H @ R\rreturn Q, R\rThe last piece of the puzzle is back-substitution. This is straightforward and\ravailable in standard libraries, but to comply with the letter-of-law of the\r“from scratch” challenge we’ll implement our own version.\ndef solve_triangular(A, b):\r\u0026#39;\u0026#39;\u0026#39;\rSolves the equation Ax = b when A is an upper-triangular square matrix\rand b is a one dimensional vector by back-substitution. The length of b\rand the number of rows must match. Returns x as a one-dimensional numpy.ndarray\rof the same length as b.\rThis isn\u0026#39;t as micro-optimized as scipy.linalg.solve_triangular() but the\ralgorithm is the same, and the asymptotic time complexity is the same. \u0026#39;\u0026#39;\u0026#39;\r# starting at the bottom, the last row is just a_N_N * x = b_N\rn, m = A.shape\rx = b[(m-1):m] / A[m-1, m-1]\rfor i in range(m - 2, -1, -1):\rback_substitutions = np.dot(A[i, (i+1):], x)\rrhs = b[i] - back_substitutions\rx_i = rhs / A[i, i] # possible ill-conditioning warning?\rx = np.insert(x, 0, x_i)\rreturn x\rI won’t lie - that was a ton of linear algebra we just ploughed through. If you\rgot through it, or if you had the good sense to skim ahead until you found\rsomething that made sense, congratulations.\nBefore we move on to actually using our new functions, let’s spend some\rtime making sure everything up to this point is correct.\nclass QRTestCase(unittest.TestCase):\r\u0026#39;\u0026#39;\u0026#39;\rUnit tests for QR decomposition and its dependencies. \u0026#39;\u0026#39;\u0026#39;\rdef test_2d(self):\rA = np.array([[1,1], [0,1]])\rb = np.array([2,3])\rx = solve_triangular(A, b)\rassert_allclose(x, np.array([-1, 3]))\rdef test_solve_triangular(self):\rfor N in range(1, 20):\rA = np.triu(np.random.normal(size=(N, N)))\rx = np.random.normal(size=(N,))\rb = A @ x\rx2 = solve_triangular(A, b)\rassert_allclose(x, x2, atol=1e-5)\rdef test_solve_rect_triangular(self):\rfor N in range(1, 20):\rfor N2 in [1, 5, 100]:\rA = np.triu(np.random.normal(size=(N+N2, N)))\rx = np.random.normal(size=(N,))\rb = A @ x\rx2 = solve_triangular(A, b)\rassert_allclose(x, x2, atol=1e-5)\rdef test_reflection(self):\rx = np.array([1,1,1])\re1 = np.array([1,0,0])\rH = householder_reflection(x, e1)\rassert_allclose(H @ (sqrt(3)* np.array([1, 0, 0])), x, atol=1e-5)\rassert_allclose(H @ np.array([1,1,1]), sqrt(3) * e1, atol=1e-5)\rdef test_square_qr(self):\r# already upper triangular\rA = np.array([[2,1], [0, 3]])\rQ, R = qr_decomposition(A)\rassert_allclose(Q, np.eye(2))\rassert_allclose(R, A)\rN = 3\rQ = ortho_group.rvs(N) # generates random orthogonal matrices\rR = np.triu(np.random.normal(size=(N, N)))\rA = Q @ R\rQ2, R2 = qr_decomposition(Q @ R)\r# note that QR is not quite unique, so we can\u0026#39;t\r# just test Q == Q2, unfortunately.\rassert_allclose(Q2 @ R2, Q @ R, atol=1e-5)\rassert_allclose(np.abs(det(Q2)), 1.0, atol=1e-5)\rassert_allclose(R2[2, 0], 0, atol=1e-5)\rassert_allclose(R2[2, 1], 0, atol=1e-5)\rassert_allclose(R2[1, 0], 0, atol=1e-5)\rdef test_rect_qr(self):\rA = np.array([\r[2,1],\r[0,3],\r[4,5],\r[1,1],\r])\rQ, R = qr_decomposition(A)\rassert_allclose(R[1:, 0], np.zeros(A.shape[0]-1), atol=1e-5)\rassert_allclose(R[2:, 0], np.zeros(A.shape[0]-2), atol=1e-5)\rassert_allclose(Q @ R, A, atol=1e-5)\runittest.main(argv=[\u0026#39;\u0026#39;], exit=False)\rWith our trusty tools in hand, we’re ready to tackle linear regression properly.\n\rImplementing Linear Regression\rRecall that our problem was to solve the normal equation:\n\\[ X^T X \\hat{\\Theta} = X^T y \\]\nIf we now let \\(QR\\) be the QR-decomposition of \\(X\\):\n\\[ (R^T Q^T)(Q R) \\hat{\\Theta} = R^T Q^T y \\]\nSince \\(Q\\) is orthogonal, \\(Q^T Q = I\\) and we can simplify this to:\n\\[ R^T R \\hat{\\Theta} = R^T Q^T y \\]\nFor the next step, we have to assume the \\(R\\) is invertible. This is always the\rcase if our original \\(X\\) was free of multicollinearity. It is also equivalent\rto \\(X^T X\\) being invertible so the naive approach of taking \\((X^T X)^{-1}\\)\risn’t any stronger. Even gradient descent has issues with singular matrices\rbecause the problem is no longer strongly convex. There is a\rmethod based on SVD (singular value decomposition) which can handle linear\rregression in the presence of multicollinearity but it’s slower and in general\rthe whole problem is better handled by removing redundant features or adding\rregularization, neither of which are in scope for this article.\n\rA guy goes to the doctor and says, “Doctor, it hurts when I perform\rlinear regression on a dataset with strong or perfect multicollinearity.”\rThe doctor says, “don’t do that.”\n\rIn any case, let’s just assume that \\(R^{-1}\\) exists. We don’t actually have to\rcalculate it, though! The mere fact of its existence lets us left multiply both\rsides of the equation by \\((R^T)^{-1}\\) and cancel the \\(R^T\\) on both sides, leaving only:\n\\[ R \\hat{\\Theta} = Q^T y \\]\nBecause \\(R\\) is an upper triangular matrix, we can use our solve_triangular()\rfunction to solve this equation very quickly.\nThe final algorithm is deceptively simple. Compare the normal\requations derived above to the final two lines of the fit() method.\nclass LinearRegression:\rdef __init__(self, add_intercept=True):\rself.add_intercept = bool(add_intercept)\rdef _design_matrix(self, X):\rif self.add_intercept:\rX = np.hstack([ np.ones((X.shape[0], 1)), X])\rreturn X\rdef fit(self, X, y):\rX = self._design_matrix(X)\rQ, R = qr_decomposition(X)\rself.theta_hat = solve_triangular(R, Q.T @ y)\rdef predict(self, X):\rX = self._design_matrix(X)\rreturn X @ self.theta_hat\r\rTesting\rNote that while we follow the scikit-learn naming conventions, up to this point\rwe haven’t imported anything from sklearn. That’s in keeping with the “from\rscratch” challenge. However, to test the code, we are going to use a few\rsklearn and scipy dependencies.\nLet’s first grab a bunch of test-only dependencies and also\rgrab a copy of the famous Boston data set so we have a simple\rregression problem to play with.\n# testing purposes only\rfrom sklearn.datasets import load_boston\rimport matplotlib\rfrom matplotlib import pyplot as plt\r%matplotlib inline\rfrom numpy.linalg import det\rfrom scipy.stats import ortho_group\rimport unittest\rfrom numpy.testing import assert_allclose\rboston = load_boston()\rX_raw = boston.data\ry_raw = boston.target\r# shuffle the data to randomize the train/test split\rshuffle = np.random.permutation(len(y_raw))\rX_full = X_raw[shuffle].copy()\ry_full = y_raw[shuffle].copy()\r# 80/20 train/test split. train_test_split = int(0.8 * len(y_full))\rX_train = X_full[:train_test_split, :]\ry_train = y_full[:train_test_split]\rX_test = X_full[train_test_split:, :]\ry_test = y_full[train_test_split:]\rThe model is fit to the training set only. If it fits the\rtraining set pretty well we know it has learned the examples\rwe gave it; if it also fits the test set pretty well,\rwe know it’s done more than just memorize the examples given\rbut has also learned a more general lesson that it can apply\rto novel data that it’s never seen before.\nA good way to visualize model performance is to plot \\(y\\) vs. \\(\\hat{y}\\) - in\rother words, actual vs predicted. A perfect predictor would be a 45°\rdiagonal through the origin; random guessing would be a shapeless or circular\rcloud of points.\nmodel = LinearRegression()\rmodel.fit(X_train, y_train)\rdef goodness_of_fit_report(label, model, X, y):\ry_hat = model.predict(X)\r# predicted-vs-actual plot\rplt.scatter(x=y, y=y_hat, label=label, alpha=0.5)\rplt.title(\u0026quot;Predicted vs. Actual\u0026quot;)\rplt.xlabel(\u0026quot;Actual\u0026quot;)\rplt.ylabel(\u0026quot;Predictions\u0026quot;)\rplt.legend()\rmse = np.mean( (y - y_hat)**2 )\ry_bar = np.mean(y)\rr2 = 1 - np.sum( (y-y_hat)**2 ) / np.sum( (y-y_bar)**2 )\rprint(\u0026quot;{label: \u0026lt;16} mse={mse:.2f} r2={r2:.2f}\u0026quot;.format(**locals()))\rplt.figure(figsize=(16,6))\rplt.subplot(1, 2, 1)\rgoodness_of_fit_report(\u0026quot;Training Set\u0026quot;, model, X_train, y_train)\rplt.subplot(1, 2, 2)\rgoodness_of_fit_report(\u0026quot;Test Set\u0026quot;, model, X_test, y_test)\r\rTraining Set mse=21.30 r2=0.73\nTest Set mse=25.12 r2=0.75\n\rAnd in point of fact this linear regression model does reasonably well on both\rthe train and test set, with correlation scores around 75%. That means it’s\rable to explain about three-quarters of the variation that it finds in \\(y\\) from\rwhat it was able to learn about the relationship between \\(X\\) and \\(y\\).\nIt’s also a good idea to visualize actual responses \\(y\\) and predictions\r\\(\\hat{y}\\) as a function of the independent variables \\(X\\). In this case \\(X\\) is\r13-dimensional so hard to visualized fully, so we will simply choose a few\rrandom pairs of dimensions dimensions so we can work in 2D. If the model has\rlearned anything real about the relationship between \\(X\\) and \\(y\\), we should see\rtwo similar clouds of points for actual \\(y\\) and predicted \\(\\hat{y}\\).\nPrediction vs. Actual Scatterplot, training set and test set\n\rWe can also plot the actual and predicted response as a function of various\rpredictors to get a sense of whether or not our function is truly fitting the\rdata:\ny_hat = model.predict(X_train)\rplt.figure(figsize=(16,32))\rfor i in range(4, 8):\rplt.subplot(6, 2, i+1)\rplt.scatter(x=X_train[:, i], y=y_train, alpha=0.2, label=\u0026#39;Actual\u0026#39;)\rplt.scatter(x=X_train[:, i], y=y_hat, alpha=0.2, label=\u0026#39;Predicted\u0026#39;)\rplt.legend()\rplt.xlabel(boston.feature_names[i])\rplt.ylabel(\u0026quot;Response Variable\u0026quot;)\rPredicted vs. Actual over pairs of independent variables\n\rThe eyeball test confirms that this model is fitting the data rather well,\rjust as we’d expect when \\(r^2 = 0.75\\).\n\rConclusion\rThat was linear regression from scratch. There’s a lot more that could be said\rabout linear regression even as a black box predictive model: polynomial and\rinteraction terms, L1 and L2 regularization, and linear constraints on\rcoefficients come to mind.\nThere’s also a whole universe of techniques for doing statistical modeling and\rinference with linear regression: testing residuals for homoscedasticity,\rnormality, autocorrelation, variance inflation factors, orthogonal polynomial\rregression, Cook’s distance, leverage, Studentized residuals, ANOVA, AIC, BIC,\rOmnibus F-tests on nested models, etc., etc. Just to be clear, these aren’t\rvariations or generalization of linear regression (although there are tons\rof those too) these are just standard techniques for analyzing and\runderstanding linear regression models of the exact same form we calculated\rabove. The topic is very mature and a huge amount of auxiliary mathematical\rmachinery has been built up over the centuries (Gauss was studying OLS around 1800,\rand the problem is older than that.)\nHowever, if we go too deeply into linear regression, we won’t get a chance to\rexplore the rest of machine learning. So for the next part of the series, we\rwill switch our attention to logistic regression and use that as an\rexcuse to explore SGD in some detail. That will then serve as a jumping\roff point for our first “real” (or at least in fashion) machine learning algorithm in part 3: neural\rnetworks and backpropagation.\n\r","date":"November 29, 2018","href":"https://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/","thumbnail":"/post/ml-from-scratch-part-1-linear-regression_files/lead.192x128.png","title":"ML From Scratch, Part 1: Linear Regression"},{"content":"Motivation\r\r“As an apprentice, every new magician must prove to his own satisfaction, at\rleast once, that there is truly great power in magic.” - The Flying Sorcerers,\rby David Gerrold and Larry Niven\n\rHow do you know if you really understand something? You could just rely on\rthe subjective experience of feeling like you understand. This sounds\rplausible - surely you of all people should know, right? But this runs\rhead-first into in the Dunning-Kruger effect. Introspection is not a\rreliable guide to self-knowledge.\nA more objective criterion is suggested by this pithy quote:\n\r“What I cannot create, I do not understand.” - Richard Feynman\n\rThis is a very famous quote, but it’s not entirely unambiguous. If we’re going\rto use it as a guide, we’ll first have to break it down a little.\nThe most common interpretation might be, “what I cannot explain to a layperson\ror to a curious child, I do not understand.” Feynman unambiguously valued the\rability to explain complex physics in plain English, as exemplified in this\ranecdote:\n\rBefore the commercial announcement of the Connection Machine CM-1 and all of\rour future products, Richard would give a sentence-by-sentence critique of\rthe planned presentation. “Don’t say ‘reflected acoustic wave.’ Say [echo].”\rOr, “Forget all that ‘local minima’ stuff. Just say there’s a bubble caught\rin the crystal and you have to shake it out.” Nothing made him angrier than\rmaking something simple sound complicated. - Danny Hillis\n\rAs has often been remarked, explaining things well is often just as beneficial\rto the teacher as to the student; it helps reinforce ideas and builds intuition.\nIf that is all that Feynman had meant, though, why use the term “create” at\rall? Surely “explain” or “teach” is closer to the meaning discussed above. So\rwhile “explain in simple terms” is certainly part of it, “create” includes\rmore than just that. Feynman gives us a clue in this story from his\rautobiography:\n\r“During the conference I was staying with my sister in Syracuse. I brought the\rpaper home and said to her,”I can’t understand these things that Lee and Yang\rare saying. It’s all so complicated.\"\n“No,” she said, “what you mean is not that you can’t understand it, but that\ryou didn’t invent it. You didn’t figure it out your own way, from hearing the\rclue. What you should do is imagine you’re a student again, and take this paper\rupstairs, read every line of it, and check the equations. Then you’ll\runderstand it very easily.”\nI took her advice, and checked through the whole thing, and found it to be very\robvious and simple. I had been afraid to read it, thinking it was too\rdifficult.\" - Richard Feynman, Surely You’re Joking, Mr. Feynman!\n\rSo in the context of math or physics, “create” means something closer to\r“derive from first principles by hand.” This is a very strong criteria! If a\rperson could go into an empty office with a stack of scratch paper and a supply\rof sharp pencils, write down all first principles and proceed to derive every\rimportant theorem in their chosen field by hand then it must be conceded\rthat such a person has some real knowledge.\nIn the context of computer science and programming, “create” might mean\rsomething like, “write a program from scratch that implements the given\ralgorithm.” Since machine learning straddles the two, “create” means both: pose\ra machine learning problem mathematically, reduce the problem to some tractable\rform on paper, then write and implement an algorithm to produce a numerical\rapproximation of the answer.\nNow, if someone attempts this exercise, one of two things will happen. First,\rthey may succeed completely on their first try. If so, great! They’ve proved\rwhat they set out to prove. But more likely, they’ll\ronly succeed partially and get stuck at some point. Well, now they have the\ropportunity to correct a deficiency in their own understanding that they\rweren’t previously aware of, which is also a great outcome. After all, Feynman\rdidn’t go in empty-handed - he took the challenging paper with him, and surely\rreferenced it often. But at the end, his own notes would record his own\rcomplete derivation from start to finish and therefore serve as a testimonial\rto his own understanding.\n\rGround Rules\rIt was in the spirit of the above considerations that in the fall of 2018 I set\rmyself a goal: I would, over the course of the next year, derive and implement\ra representative sample of fundamental models and algorithms from machine\rlearning, entirely from scratch and (insofar as was possible) entirely from my\rown understanding. Where I found my understanding sufficient, this would be an\rexercise in recreational programming; where my understanding failed me, it would\rbe a chance to shore up the foundations.\nThis is possibly less insane than it may appear. Although there are aspects of\rmachine learning that are very technical, for the most part the\rimplementation of practical algorithms requires little more than some\rmoderately advanced statistics, a few semesters of linear algebra, some general\rfamiliarity with numerical optimization and of course basic programming skills.\nBecause an open-ended project like this has a tendency to get out of control,\rI also decided to set some ground rules to help keep things sane.\nFirst, mathematical derivations are in scope. This usually means posing and\rsolving an optimization problem of some form, such as MLE. This is\rstraight-forward for most of the algorithms on my list but could get a little\rhairy for things like backpropagation (which requires some fairly non-trivial\rmatrix calculus) or SVMs (which basically requires the entire theory of\rQuadradic Programming). In practice, the presentation of these\rderivations is bottlenecked by the necessity of typesetting the equations\rin \\(\\LaTeX\\), so these will typically be little more than sketches of the proofs.\nSecond, the algorithms used will be state-of-the-art, or at least reasonably\rso. For example, while we could solve linear regression with gradient\rdescent, it would be a bit of a cop-out. Instead, we’ll implement what modern\rstatistical software actually does under the hood. One particular consequence\rof this rule is that I will be implementing vectorized versions of the\ralgorithms whenever possible: while iterating over every example in the\rtraining set is often easier to understand, it’s also pretty far removed from\rthe realities of modern implementations which rely heavily on vectorization or\reven GPU acceleration for performance.\nThird, I will implement and test all algorithms on some data set. As the Agile\rcrowd would say, working software is the primary measure of progress. For\rconvenience, I will use Python 3 and allow myself numpy arrays… but not\rnumpy.linalg or other high-level libraries like scipy.optimize; matrix\rmultiplication is about the most complex operation we’ll let the libraries do\rfor us. I considered not using numpy at all, but it allows us to express\ralgorithms in vectorized notation and frankly the algorithms are both\rclearer and more realistic with it. This restriction only applies to the\rimplementation of the algorithm itself and excludes tests - I will routinely\ruse higher-level libraries (like numpy.linalg, pandas, scipy, or\rsklearn.datasets) when testing the algorithm.\nFourth and finally, I’ll be publishing write-ups as I go. I’ve found in\rpractice this can be more time consuming than the original exercise. However\rattempting to explain each algorithm in simple terms to a broad audience should\rhelp me to understand them a little better as well.\n\rProject Scope\rWhile I want to touch on every aspect of machine learning, there’s little point\rin implementing minor variations of basically the same algorithms over and\rover. Instead, let’s pick one or two representative algorithms from each\rcategory and leave it at that. We want to make sure that we get reasonable\rcoverage over the types of ML problems (supervised/unsupervised,\rregression/classification, etc.), as well as good coverage over the most\rimportant algorithms that crop up repeatedly in ML.\nHere’s a tentative list of algorithms I would like to tackle:\n\r\r\r\rProblem\rModel\rAlgorithm\rArticle\r\r\r\rRegression\rLinear Regression\rQR Decomposition\rPart 1\r\rClassification\rLogistic Regression\rGradient Descent\rPart 2\r\rClassification\rNeural Network\rBackpropagation\rPart 3\r\rClassification\rDecision Tree\rRecursive Partition\rPart 4\r\rClustering\rGaussian Mixture Model\rEM Algorithm\rPart 5\r\rDimension Reduction\rPrincipal Component Analysis\rQR Eigenvalue Algorithm\rPart 6\r\rRecommendation\rLow-Rank Matrix Approximation\rAlternating Projections\rTBD\r\rRegression\rGeneral Additive Models\rBackfitting\rTBD\r\rClassification\rSupport Vector Machines\rSMO Algorithm\rTBD\r\r\r\rOther candidates I considered but ultimately decided were out-of-scope:\n\rFactor Analysis - We already have PCA for dimensional reduction and GMM\ras an example of using the EM algorithm to solve for latent random variables.\rK-Means - We’ll do GMM instead, since k-means is just GMM with hard assignment.\rK-Nearest Neighbors - A naive algorithm is trivial while a serious\ralgorithm would mostly involve implementing a spatial index (such as\rR-Trees) which takes us pretty far afield from learning algorithms.\rEnsemble models - e.g. Random Forest or Boosted Trees. Not a good fit for the\r“from scratch” approach and can best be understood as “composing” two or more\rother mature models.\rCNN, RNN, etc. - We’ll do the vanilla deep neural network from scratch\rbut more advanced topologies are best explored with a framework with\rautomated differentiation.\rLearning-to-Rank - e.g. Bradley-Terry-Luce, etc. These can\rgenerally be reduced to logistic regression or viewed as latent variable\rmodels and solved with the EM algorithm.\rFelligi-Sunter Record Linkage - another take on the EM algorithm, and requiring to many\rprerequisites like Jaro-Winkler distance.\r\r\rBottom-Up Approach to Machine Learning\rIn the spirit of the Feynman technique, let’s spend a few minutes talking\rthrough the problem in plain English and see if we can understand why machine\rlearning seems to focus so heavily on a few mathematical techniques and\rapproaches; this, in turn, should make it clear why it’s worth understanding\rthese techniques in depth.\nThe problem, in the broadest possible terms, is to get a computer to learn how\rto do something. This is in contrast to traditional programming, where the\rcomputer does not usually “learn” anything, but follows a program written by a\rhuman programmer. Computers also aren’t very good at “doing” most things,\ralthough they are very good (and very fast!) at the few things they can do.\nSo, what are computers good at? In decreasing order (increasing by the amount\rof time it takes) computers can do the following:\nAddition and subtraction\n\rMultiplication\rDivision\rComparing two numbers to decide what to do next\rOther math functions like exp(), log(), sin(), cos(), etc.\rRemembering a billion numbers\rLooking something up in a file or database\rTalking to another computer over a network\r\rThis fairly standard set of costs actually leads directly to some\rimportant insights that guide research into practical machine learning.\nFirst, we want to restrict ourselves as much as possible to simple arithmetic.\rWhile we may occasionally allow ourselves a division or even, gasp, an\rexponentiation, we really want to stick to fast operations like addition,\rmultiplication, taking the greater of two numbers with max(a,b), or taking the sign of a number with sign(a).\nSecond, any “learning” we do should be in the form of updating a\rstructured set of numbers. We call these the “parameters” to distinguished them\rfrom the “data.” The parameters may be shaped like a vector, a matrix, or a\rtree, but if we want to combine parameters and data with simple arithmetic, then\rboth must ultimately be represented as data structures with numeric values.\nOn the other hand, we want to avoid representing learning as a formatted string\ror program. For example, the internal state of our learning algorithm could\rliterally be a a string describing a C program:\nfloat f(float* x) { float z = 42; if ( x[0] \u0026lt; 5 \u0026amp;\u0026amp; x[1] \u0026gt; 2 ) z -= 10; if ( x[2] \u0026gt; 7 || x[5] == 2 ) z += 3; for ( int i=6; i\u0026lt;11; i++ ) { z += x[i]; }\rif ( x[1] == 1 ) {\rfor ( int i=3; i\u0026lt;5; i++ ) { z -= 2 * x[i]; }\r}\rreturn z;\r}\rTo apply this to data, we would compile this C program and pass our data\rinto the function f(). To “learn”, the algorithm would add, remove, and\rmodify individual lines, characters, or perhaps syntactic statements or\rexpressions. This is sometimes called genetic programming.\rTo be 100% clear, this is only bad if we allow arbitrary programs\rinvolving AND, OR, NOT, if/else, while, for, intermediate\rvariables, and the like. Genetic programming can work well if the “genes” of\rthe program are very carefully designed. Indeed, it is sometimes used as\rthe “top level” learning algorithm in so-called automated machine learning\rframeworks such as TPOT. However, for the kind of fitting and optimization\rwe’re mainly interested in, genetic algorithms are hopelessly inefficient.\nWhy is learning an arbitrary program problematic? Does it simply not work?\rSurely any equation we write down could also be represented in a more general\rform as a program, and surely we could find that program by exhaustive\rbreadth-first search if necessary. And isn’t it also true that every program\rhas a Gödel number? So how is this fundamentally any different than\rlearning a set of numbers?\nThe problem isn’t that it doesn’t work, or that there’s anything wrong with\rthat approach in theory. The problem is simply that the space of programs we\rwould need to search is extremely large (the number of legal programs grows\rexponentially with the length of the program with very high fan-out), and it is\rexceedingly difficult to know if we’re getting “closer” to the right answer or\rnot. That’s a bad combination and means that “sufficient time” is often a lot\rlonger than we’re willing to wait.\nTo illustrate that second point, consider this (correct) program which finds\rthe greatest common divisor of a pair of numbers:\ndef gcd(x, y):\rwhile y != 0:\r(x, y) = (y, x % y)\rreturn x\rLet’s say that the \"def gcd(x, y)\" is fixed as part of the problem\rspecification. Then there’s literally not a single character, word, or symbol\rwe could change in the body of that function which would not make it incorrect.\rIf I change % to *, it doesn’t terminate, if I change y != 0 to y != 1\rit’s so completely wrong it can never return a correct solution even by\raccident, and so on. Therefore, in the space of all possible programs, this\rcorrect program is surrounded on all sides by wildly incorrect programs. That\rmeans that a greedy or even an evolutionary algorithm is unlikely to find this\relegant program. It is possible to find it via exhaustive breadth first search\r(where depth is the length of the program) but this is brute force and hard to\rscale.\nSo, in practical machine learning, we do not try to learn arbitrary programs,\rwe learn parameters for functions from some family of functions. For\rexample, let’s say a data point is represented as the vector \\(\\vec{x} \\in \\mathbb{R}^n\\) and our parameters are the vector \\(\\vec{p} \\in \\mathbb{R}^n\\).\rThen, keeping in mind that we mostly want to stick to arithmetic, the simplest\rthing we could do is a dot product between these two vectors:\n\\[ f(\\vec{x} ; \\vec{p} ) = \\vec{x} \\cdot \\vec{p} = \\sum_{i=1}^n x_i p_i \\]\nThat looks too simple to work, but in fact we’ll see in the next article in\rthis series, that it works surprisingly well for a very large class of\rproblems. Not for all problems of course; and throughout the series we will\rgradually add complexity to the representation. This will, in turn, create\rproblems for us in terms of fitting/training these more complex models. In\rparallel, we will develop ever more powerful techniques to deal with these\rproblems as they arise. In particular we will see again and again how a well\rchosen representation will allow us to find very fast algorithms for learning\roptimal parameters.\n\rConclusion\rNext time, we’ll start with linear regression, followed by logistic\rregression and some simple neural networks. As new articles\rare added, you can find them collected under the “from scratch” tag.\n\r","date":"November 11, 2018","href":"https://www.oranlooney.com/post/ml-from-scratch-part-0-introduction/","thumbnail":"/post/ml-from-scratch-part-0-introduction_files/lead.192x128.jpg","title":"ML From Scratch, Part 0: Introduction"},{"content":" Introduction Visualizing the results of a binary classifier is already a challenge, but having more than two classes aggravates the matter considerably.\nLet\u0026rsquo;s say we have $k$ classes. Then for each observation, there is one correct prediction and $k-1$ possible incorrect prediction. Instead of a $2 \\times 2$ confusion matrix, we have a $k^2$ possibilities. Instead of having two kinds of error, false positives and false negatives, we have $k(k-1)$ kinds of errors. And not all errors are created equal: just as we choose an optimal balance of false positives and false negatives depending on the cost associated to each, certain kinds of errors in a multiclass problem will be more or less acceptable. For example, mistaking a lion for a tiger may be acceptable, but mistaking a tiger for a bunny may be fatal.\nThe goal of visualizing multiclass classification results is to allow the user to quickly and accurately see which errors are occurring and to start developing theories about why those errors are occurring; usually this would be to assist the user during iterative model development, but could also be used, for example, to communicate the behavior of a final classifier to a non-specialized audience.\nIn this article I employ two basic strategies to try and meet these goals: data visualization techniques and algorithmic techniques. It\u0026rsquo;s worth a quick reminder about why data visualization is valuable at all. The human visual system is extremely good at picking up certain kinds of patterns (generally those that correspond to spatial relationships and color), but is completely unable to see other kinds of patterns (the digits of $\\pi$ coded as greyscale pixel brightness would look like pure noise) and worse yet has a tendency to see patterns in clouds of purely random data where none exist. A good visualization, then, ensures that any interesting structure in the underlying data will be presented in a way that is amenable to interpretation by the human visual system, while any irrelevant or statistically insignificant variation is suppressed.\nAlgorithmic techniques, on the other hand, do not rely on the human visual system\u0026rsquo;s ability to detect patterns, but automate the analysis that a human would have done anyway in some procedural way. Rather than merely making it easy to see where a relationship exists, an algorithmic solution would explicitly enumerate and rank the kinds of things the user is interested in. This approach can scale to large data sets much more efficiently, but requires us to trust the algorithm. Both data visualization and algorithmic techniques are useful in practice and are often best when combined.\nThe underlying problem is very open ended and I do not claim to have come up with any definitive solution, but I did find several novel and useful techniques that seem to me to be worth sharing.\nCase Study To explore the problem, we need some data and a toy classifier to work with.\nThe first step is to train and fit some reasonably good but not perfect classifier to some dataset that is reasonably amenable to classification but not linearly separable.\nTo meet these requirements, I chose the MNIST handwritten digit dataset; This data set is popular for testing multiclass classification algorithms and has the advantage of having very intuitive classes. The MNIST problem is to classify 28x28 gray scale images, which represent center and scaled images of handwritten digits, and assign them to one of ten classes, namely the digits 0 through 9. The MNIST data come pre-labeled and therefore ready to be fed into a supervised learning algorithm. Best of all, it is extremely easy to obtain in an R-friendly format due to its popularity.\nAs a preprocessing step, we will use T-SNE algorithm provided by the tsne package to reduce the 784 dimensions of the raw pixel data to just two dimensions, which we will simply call x and y.\nmnist_tsne \u0026lt;- tsne(as.matrix(mnist_r10000[,1:784])) xy \u0026lt;- as.data.frame(mnist_tsne) colnames(xy) \u0026lt;- c('x', 'y') xy$label \u0026lt;- mnist_r10000$Label  Next, we will apply a multinomial classifier from the nnet package (despite the name, the package actually provides MLP and multinomial log-linear models)\nmodel \u0026lt;- multinom( label ~ I(x^3) + I(y^3) + I((x^2)*y) + I(x*y^2) + I(x^2) + I(y^2) + x * y, data=xy, maxit=500 ) xy$prediction \u0026lt;- predict(model) hits_and_misses = xy[xy$label != xy$prediction | rep_len(c(TRUE,FALSE), nrow(xy)),]  This model does an OK but not stellar job of classifying digits, achieving an overall accuracy of about 95%. This is what we want - a higher quality model would have too few misses to analyze deeply, while a simpler model wouldn\u0026rsquo;t be realistic enough to make a good case study.\nOff-the-Shelf Pairs Plot A good place to start with any dataset is a so-called \u0026ldquo;pairs\u0026rdquo; plot: a grid of plots showing relationships between every every possible pair of columns. The GGally package provides a particularly high-quality pair plot, so let\u0026rsquo;s start with that.\nThis plot was easy to create, but most of the relationships turn out to be uninteresting, except in a negative sense: we can tell from the large amount of overlap between classes in the univariate kernel density plots that neither x nor y alone is able to classify digits very well. However, the leftmost plot in the middle column, which shows a scatter plot of x and y color-coded with the true class label, suggests that using both dimensions together with a non-linear classifier may be effective.\nScatter Plot To explore that further, let\u0026rsquo;s create a full-size scatter plot on x and y. To compactly and intuitively represent both the true class and the predicted class in the same plot, we will plot each point with the glyph representing the true class and a color representing the predicted class. To avoid overwhelming the plot, we plot only a random sample of 1,000 points.\nPro: I was able to pack an extra dimension into each point by using a glyph to represent each point. It\u0026rsquo;s easy to see that predictions form contiguous regions in the 2D space.\nCon: Misses can only be seen by carefully scanning the image for digits with the wrong color. Because only a sample of data is shown and there are relatively few misses, it is unclear exactly where the decision boundaries are.\n2D Kernel Density Plot To correct these defects, I next moved away from a scatter plot of the sample and looked for a way to visualize the underlying distribution. One way to do this is to compute a two-dimensional kernel density estimate from the underlying data and to use a contour plot to display the result. Essentially, we get a \u0026ldquo;hill\u0026rdquo; for the region where a particular class is prevalent. These regions look like concentric rings, although the rings are very far from circular. The probability that a given point belongs to any particular class is proportional to the number of rings of the right color that completely surround that point. Points which are in the intersection between two regions are ambiguous and this is where we should expect to see the most misclassifications.\ncenters \u0026lt;- xy %\u0026gt;% group_by(label) %\u0026gt;% summarise_all(funs(mean)) ggplot(xy[1:1000,], aes(x, y, color=label)) + geom_density_2d() + xlim(-120, 125) + ylim(-130, 110) + geom_text( data=centers, aes(x, y, color=label, label=label), fontface=\u0026quot;bold\u0026quot;, size=10, show.legend=FALSE ) + theme(legend.position=\u0026quot;bottom\u0026quot;) + guides(colour = guide_legend(nrow = 1)) + ggtitle(\u0026quot;t-SNE MNIST, 2D Kernel Density\u0026quot;)  Pro: the simple expedient of labelling each centroid with a large color coded digit makes works well and makes the color legend at the bottom almost unnecessary.\nThe graph is highly interpretable at a glance, and can also be used to give precise predictions if you are patient enough to count rings.\nIt is very easy to see where boundaries overlap and the classification model may be confused: not the large area of overlap between 3 and 5 on the left, for example. Such overlaps directly correspond to pairs of classes for which misclassification is common.\nCon: Directionality is rather unclear - if a point is in the overlap of 3 and 5, it is at risk of being misclassified - but will 3\u0026rsquo;s be misclassified as 5\u0026rsquo;s, vice versa, or both? Also, people do need at least some training to interpret contour plots, especially overlapping contour plots, which are not very common at all.\nHits and Misses Plot Returning to the scatterplot concept and striking out in a different direction, my next idea was to draw attention to misses by color coding by accuracy instead of by class; in the below plot: correct predictions are labeled in blue, incorrect in red.\nPro: this variation naturally calls the eye to the misses: Unlike our first scatterplot the misses now stand out vividly.\nIt many ways this trick is successful: we can immediately see at a glance that misclassifications do indeed tend to fall near the boundary of two clusters, and we also get a sense of where such misses tend to belong to one class or the other. Finally, we can also easily pick out examples of misclassifications buried deep within other clusters - such cases are perhaps very far beyond the current model to correctly classify and represent the irreducible error of the current approach.\nCon: Obviously we gave up the detailed information about the predicted class that we previously encoded into the color. Many of the criticisms directed at the previous scatterplot still apply here too.\nTree Plot While some of the above visualizations have succeeded at attracting attention to the signal in the noise, they cannot be said to have algorithmically extracted the relevant information. The best way I came up with for doing this was to use hierarchical clustering which is sometimes used for similar problems, such as finding correlation relationships in a data set.\nTo apply the algorithm to this problem, I defined classes as \u0026ldquo;closer\u0026rdquo; to each other each other the more often they are misclassified as each other. If the algorithm does its job then those classes which are most likely to be mistaken together will be close together on the resulting tree. (Graphical plots of tree structures have the slightly pretentious name of \u0026ldquo;dendrograms,\u0026rdquo; terminology I will never-the-less adopt for precision.)\nmiss_table \u0026lt;- table(misses$label, misses$prediction) sym_miss_table \u0026lt;- as.matrix(prop.table(miss_table + t(miss_table))) diag(sym_miss_table) \u0026lt;- 0.07 sym_dist_table \u0026lt;- round(0.07 - sym_miss_table,4) miss_dist \u0026lt;- as.dist(round(0.07 - sym_miss_table,4)) plot(hclust(miss_dist, method=\u0026quot;ward.D\u0026quot;))  Pro: I am very pleased to note that this clustering is fundamentally successful: it correctly pairs 3 with 5, 4 with 9, and so on. These are the same patterns we observed in the less rigorous analysis above, but we no longer have to rely on eyeballing the graph and making a subjective judgement. The clustering algorithm is explicitly telling us that those are the most prevalent relationships.\nCon: The above relationships are symmetric (as is required by the definition of a metric.) The use of a symmetric metric was in turn a requirement of the agglomerative clustering algorithm we used. We will need a fundamentally different approach for deal with directionality.\nHeat Map The dendrogram does obscure some of the raw data about the frequency of misclassifications, however. A standard way to have our cake and eat it to \u0026ndash; to show both the algorithmic clusters and underling data in the same visualization \u0026ndash; is to use a heatmap for the raw data, and attach the dendrograms to the rows and columns.\nheatmap(sym_dist_table)  Pro: If you look closely, you\u0026rsquo;ll see that both the row and column dendrograms are in fact the same dendrogram from before, now being used to order the rows and columns of a heat map. This brings the 10 classes into a roughly block matrix form where squares along the diagonal indicate groups of classes that may be mistaken for one another. But the heatmap shows much more than this - we can see the isolated, bright red squares along the diagonal in the upper right, representing the easily classifiable cases. We can see not just pairs, but larger groups - the 3-5-8 group in the lower left stands out as a 3x3 block of related classes.\nCon: I am very pleased with this visualization, and feel the only thing lacking is the directionality information we had to discard in order to fit our data into the hierarchical clustering mold. Let\u0026rsquo;s address that next.\nDirected Graph Let\u0026rsquo;s address the directionality issue now by returning to the asymmetric results matrix, before we applied the symmetry condition, and instead interpret it as the adjacency matrix of a directed graph. Then the classes will be the nodes of the graph and the edges will indicate common misclassifications. We can use the igraph package to visualize this digraph.\nlibrary(igraph) plot( graph_from_adjacency_matrix(t(miss_table \u0026gt; 12), mode='directed'), main=\u0026quot;Digraph: Real -\u0026gt; Mistaken Prediction\u0026quot;)  Pro: It makes certain imbalances in misclassification quite evident: while a 5 might be misclassified as a 9, a 9 will almost never be misclassified as a 5. Such imbalances can be found simply by looking for edges with only one arrow.\nCon: Quite difficult to explain to a lay audience. No real sense of relative probability of each type of error. This graphic does not stand by itself, but may be a useful companion to the heatmap if directionality is present and relevant. While not necessarily a bad thing in and of itself, it does mean that we discarded directionality information.\nDeep Dive into Misses The above hierarchy suggests a strong relationship between the classes 3 and 5. We can explore this in depth by taking a random sample of such misses and plotting them in full.\nSome of these errors are more forgivable than others, but it\u0026rsquo;s clear that the multinomial algorithm is struggling when a digit is written in such a way as to shift critical features by a few pixels. An algorithm that didn\u0026rsquo;t look at all 784 pixels at once but zoomed in and looked for certain features or patterns in a translation invariant way would do a much better job\u0026hellip; While I\u0026rsquo;m not too interested in the particulars of the toy problem, the fact that way to improve the model is immediately leaps to mind just by looking at a few examples of misses suggest that this kind of deep dive is a useful diagnostic supplement.\nConclusion Performing hierarchical clustering on the $k \\times k$ confusion matrix and displaying the results as a dendrogram was very successful at algorithmically finding real relationships between classes but hides directionality information. However, this can be supplemented with a digraph if directionality is important. I also found that presenting the dendrograms together with a heat map is an excellent way to visualize both the structure and raw results of a multiclass classification algorithm. Finally, I found that even a few concrete examples of each type of hit or miss went a long way towards providing insights about which cases the classifier could handle and which it could not.\n","date":"August 23, 2018","href":"https://www.oranlooney.com/post/viz-tsne/","thumbnail":"/post/viz-tsne_files/lead.192x128.jpg","title":"Visualizing Multiclass Classification Results"},{"content":" Craps is a suprisingly fair game. I remember calculating the probability of winning craps for the first time in an undergraduate discrete math class: I went back through my calculations several times, certain there was a mistake somewhere. How could it be closer than $\\frac{1}{36}$?\n(Spoiler Warning If you haven\u0026rsquo;t calculated these odds for yourself then you may want to do so before reading further. I\u0026rsquo;m about to spoil it for you rather thoroughly in the name of exploring a more general case. A full solution may also be found, for example, in the book Fifty Challenging Problems in Probability with Solutions.)\nIt\u0026rsquo;s so close to fair, in fact, that I actually had to check to see if the inventor had been a mathematician or statistician; and while he seemed like a very colorful character there\u0026rsquo;s little in his biography to suggest he had any deep knowledge of mathematics.\nConcretely, the exact odds are $\\frac{244}{495} \\approx 49.29\\%$: That\u0026rsquo;s less than 0.71% away from perfectly fair \u0026ndash; and of course no casino would ever host a game that was perfectly fair. (Note that this means that the house edge is 1.4%, similar to Blackjack but without requiring the memorization of large tables of optimal moves. Any player who isn\u0026rsquo;t able to play Blackjack perfectly will probably do better at the craps table.)\nTo appreciate how remarkable that is, note that fairest (\u0026ldquo;fairest\u0026rdquo; meaning the closest to 50% while still strictly less) outcome you can achieve with a single roll of a 1d20 is 45%, or even using two 10-sided die to roll 1d100 you can only get to 49%. And it seems like it should be utterly hopeless with just two six-sided die, because the probabilities of all outcomes are all multiples of $\\frac{1}{36} \\approx 2.78\\%$. There just doesn\u0026rsquo;t seem like there\u0026rsquo;s enough granularity to get to $49.29\\%$. But with the clever design of the \u0026ldquo;on point\u0026rdquo; rule, it seems we can get there with just two ordinary six-sided die.\nIts so close to 50% it\u0026rsquo;s actually a bit of a pain to test empirically: according to G*Power3 you would need to play 54,000 games of craps to be reasonably sure that you are safe to reject the null hypothesis that the true odds were in fact 50\u0026frasl;50.\nWe also note that the game appears to have been constructed by a very deliberate choice of numbers: 2, 3, 7, 11, and 12. We can imagine de Marigny sitting in front of a 19th century fireplace, drinking brandy and rolling dice, taking notes, occasionally scratching out a 4 and replacing it with a 3, little by little adjusting his rules to try and find the perfect game.\nSo the question of the day is this: is there anything unique or special about this particular assignment of numbers? In particular, does de Marigny\u0026rsquo;s particular assignment of numbers to outcomes result in the fairest possible craps-like game?\nAnd above all \u0026ndash; is it possible to do better? To put it another way: could we choose different numbers such that the probability of winning our variant is greater than $\\frac{244}{ 495}$ while still being strictly less than $\\frac{1}{2}$?\nTo answer this question we\u0026rsquo;ll need a little bit of math and little bit of Python.\nDefinitions Let\u0026rsquo;s start by laying out the original rules of craps.\nIn the game of craps a \u0026ldquo;roll\u0026rdquo; is always the sum of two random six-sided die. The individual dice results never matter: for example, 2,3 always leads to the same as outcome as 1,4 because they both sum to 5.\nA game of craps has two phases: A \u0026ldquo;come out\u0026rdquo; phase for the first dice roll, and an \u0026ldquo;on point\u0026rdquo; phase for all subsequent rolls. In the \u0026ldquo;come out\u0026rdquo; phase, if the player rolls a 7 or an 11, he or she immediately wins; this called a \u0026ldquo;natural.\u0026rdquo; If the player rolls a 2, a 3, or 12, he or she immediately loses; this is called a \u0026ldquo;craps.\u0026rdquo; For any other roll, the first dice roll becomes the \u0026ldquo;point\u0026rdquo;. The point number is fixed by that first \u0026ldquo;come out\u0026rdquo; roll and remains the same until the end of the game. The player then repeatedly rolls until one of two things happen: either they roll the \u0026ldquo;point\u0026rdquo; and win, or they roll a 7 and lose; this is called \u0026ldquo;seven-out\u0026rdquo;.\nLet\u0026rsquo;s do a complete example. A player starts a new game. On his first roll - the \u0026ldquo;come out\u0026rdquo; roll - he rolls a 5. This is not 7 or 11 so he doesn\u0026rsquo;t immediately win, and it\u0026rsquo;s not 2, 3, or 12 so he doesn\u0026rsquo;t immediately lose. Instead he is now \u0026ldquo;on point\u0026rdquo; and enters the second phase of the game. His next roll is an 11. Because we are no longer in the come out phase, 11 has no special meaning. The only two numbers that matter are his \u0026ldquo;point\u0026rdquo; (5) and to the \u0026ldquo;seven-out\u0026rdquo; (7). Since 11 is neither of these, he rolls again. This next roll is a 5, so he wins the game.\nParameterization Now that we have a good understanding of the rules, let\u0026rsquo;s try to generalize the game. A full generalization would not have any specific magic numbers hard-coded in, but would instead treat all numbers used in the rules as parameters.\nIt\u0026rsquo;s clear that parameters have different roles. In the \u0026ldquo;come out\u0026rdquo; phase, 7 and 11 are the \u0026ldquo;naturals\u0026rdquo; while 2, 3, and 12 are the \u0026ldquo;craps\u0026rdquo;.; In \u0026ldquo;on point\u0026rdquo; phase 7 is the \u0026ldquo;out\u0026rdquo; that causes a loss in the \u0026ldquo;on point\u0026rdquo; phase. There is no particular parameter for the \u0026ldquo;point\u0026rdquo; because this is decided by the \u0026ldquo;come out\u0026rdquo; roll.\nIt\u0026rsquo;s also clear there are some natural constraints. We cannot assign a number to be both a natural and a craps. There should be at least one natural and at least one craps \u0026ndash; part of the excitement of the game is that it is possible to win or lose on every roll. With some careful thought it can also be seen that the \u0026ldquo;out\u0026rdquo; must be the same as one of the naturals or the craps \u0026ndash; if this were not the case it would be possible to enter the \u0026ldquo;come out\u0026rdquo; phase with your \u0026ldquo;point\u0026rdquo; the same as the \u0026ldquo;out\u0026rdquo; which would be fatally ambiguous. And finally, if every possible roll resulted in either a \u0026ldquo;natural\u0026rdquo; or a \u0026ldquo;craps\u0026rdquo; then it wouldn\u0026rsquo;t be possible to enter the \u0026ldquo;on point\u0026rdquo; phase \u0026ndash; and really wouldn\u0026rsquo;t be very craps like.\nTherefore a game is \u0026ldquo;craps-like\u0026rdquo; and may be called a \u0026ldquo;craps-variant\u0026rdquo; if it is defined by a partition of the integers from 2 to 12 into three non-empty sets: $N$, $C$, and $P$, plus a single parameter $o \\in N \\cup C$. With this parameterization, every craps-variant has the same rules.\nLet $\\mathcal{D}(s)$ be the uniform distribution over the set of integers from 1 to $s$. Let $(r_i)$ be an infinite sequence of i.i.d. random variables where $r_i \\sim \\mathcal{D}(6) + \\mathcal{D}(6)$. Then we define the game of craps and all its variants as the parameterized family of functions:\n\\[ \\text{craps}(r; C,N,o)= \\begin{cases} \\text{win} \u0026 \\text{ if } r_1 \\in N \\\\ \\text{lose} \u0026 \\text{ if } r_1 \\in C \\\\ \\text{onpoint}(r, 2; o) \u0026 \\text{ otherwise } \\end{cases} \\]\nThe function $\\text{onpoint}()$ is defined recursively as:\n\\[ \\text{onpoint}(r,i; o) = \\begin{cases} \\text{win} \u0026 \\text{ if } r_i = r_1 \\\\ \\text{lose} \u0026 \\text{ if } r_i = o \\\\ \\text{onpoint}(r,i+1; o) \u0026 \\text{ otherwise } \\end{cases} \\]\nThese formalisms make it clear that partitioning the numbers 2-12 into three non-empty sets is at the heart of the combinatorics of craps variants. We\u0026rsquo;ll need a way to count and generate such partitions if we want to search the space of all possible craps-variants. The functional definition of the craps game will simply serve as a guide to the Python implementation, which will perhaps be much easier to read.\nComputation To compute probabilities exactly we will use the fractions package to precisely represent rational numbers.\nimport fractions from fractions import Fraction  We can also precompute p.m.f. of $\\mathcal{D}(6) + \\mathcal{D}(6)$ and represent the map of outcomes to probabilities as a Python dict:\np_roll = { total: Fraction(6-abs(total-7), 36) for total in range(2,12+1)}  {2: Fraction(1, 36), 3: Fraction(1, 18), 4: Fraction(1, 12), 5: Fraction(1, 9), 6: Fraction(5, 36), 7: Fraction(1, 6), 8: Fraction(5, 36), 9: Fraction(1, 9), 10: Fraction(1, 12), 11: Fraction(1, 18), 12: Fraction(1, 36)}  The support of a random variable is the set for which it has non-zero probability; in other words, all possible outcomes. It is convenient to have this as separate variable since we we will need to refer to it several times.\nroll_support = list(p_roll.keys())  Next we will define a class which represents a single craps variant. The parameters will be instance members and the function p_win() will calculate the exact probability $P(\\text{craps}(;N,S,o) = \\text{win})$. (In general the prefix p_ will denote \u0026ldquo;probability of\u0026rdquo;.) Note this is not a Monte Carlo simulation or an approximation: using Fraction() and summing over the finite support gives us exact probabilities.\nclass Craps: def __init__(self, craps=[2, 3, 12], naturals=[7, 11], out=7): self.craps = craps self.naturals = naturals self.out = out def p_win(self): return sum( p_roll[total] * self.p_come_out(total) for total in roll_support) def p_come_out(self, total): if total in self.craps: return 0 elif total in self.naturals: return 1 else: return self.p_when_on_point(total) def p_when_on_point(self, point): p_out = p_roll[self.out] p_point = p_roll[point] return p_point / (p_out + p_point)  (I\u0026rsquo;ve omitted a few double underscore methods to keep things high level; check the original notebook for the full code listing.)\nWhat\u0026rsquo;s important here are the methods p_win(), p_come_out(), and p_when_on_point() which closely correspond our mathematically definitions and in fact correspond to calculating the various conditional probabilities of each phase. Note that even though the game itself can go on forever and the we defined our game mathematically using infinite recursion, the expectation value calculated in p_come_out() does not require infinite recursion, or any recursion at all. We need only take the ratio of the probability of the point and the probability of rolling the out.\nOriginal Game With our class defined, we can test it by feeding it the original parameters and verifying that it returns $\\frac{244}{495}$.\np = Craps(craps=[2, 3, 12], naturals=[7, 11], out=7).p_win() p_original = p p, float(p)  (Fraction(244, 495), 0.49292929292929294)  Which is what we expect.\nGenerating Variants We next turn our attention to generating all possible variants. This is a two step process: generate all 3-partitions of $[2,12]$ to obtain $C$, $N$, and $P$, and then pick our \u0026ldquo;out\u0026rdquo; from the set $C \\cup N$. Note that the members of a partition are non-empty by definition so there will always be at least one craps, one natural, and one number which will advance us into the \u0026ldquo;on point phase.\u0026rdquo;\nWe are going to be generating a lot of partitions so it behooves us to have a reasonable performant implementation. Knuth\u0026rsquo;s algorithm u is very fast and memory efficient but very far outside the scope of this article; so for now just note that its a generator expression yielding lists of lists to represent partitions and that it yields every valid partition exactly once before stopping.\n# Knuth's algorithm to partition ns into m sets. def algorithm_u(ns, m): # 83 lines of code omitted # yields partitions as lists of lists of integers  We wrap the partition generator in one extra layer to pick the \u0026ldquo;out\u0026rdquo;:\ndef generate_craps_variants(): for craps, naturals, _ in algorithm_u(roll_support, 3): for out in craps + naturals: yield Craps(craps, naturals, out)  Investigation The first question that comes to mind is \u0026ldquo;How many craps variations are there?\u0026rdquo;\nsum( 1 for _ in generate_craps_variants() )  229858  Just shy of a quarter million. Note that while all of these games are unique under our definition, they exhibit several kinds of symmetry. For example, if two games are the same except one has an out of 5 and the other an out of 9, they will always have the exact same probability winning the whole game because 5 and 9 have the same probability. And there are many cases where we could exchange say a 3 in the naturals with an 11 in the craps set and once again get a game with the exact same probability of winning. So in some sense our quarter million is overcounting. However these symmetries are quite complex so we will leave that for some future article.\nIn any case, a quarter million is no obstacle to explicit enumeration. If you would like to see the full list, here it is in compressed format. And of course you could always generate them for yourself in a few minutes from the notebook code.\nTo answer the original question posed, we need to rank these games according to the rule \u0026ldquo;closest to 50% while still strictly less.\u0026rdquo; In Python we use the decorate-sort-undecorate idiom.\ndef decorate_with_scores(craps_variants): one_half = Fraction(1, 2) for craps_variant in craps_variants: p = craps_variant.p_win() score = (one_half - p) if p \u0026lt; one_half else 1 - p yield (score, p, craps_variant) scored_variants = list(decorate_with_scores(generate_craps_variants())) sorted_variants = sorted(scored_variants)  Let\u0026rsquo;s just take a peek at the top 20:\nfor score, p, craps_variant in sorted_variants[:20]: print(\u0026quot;{} : {} {:.4f}%\u0026quot;.format(craps_variant, p, 100*float(p)))  craps: 2,8,10,12 naturals: 3,7 out: 10 : 5039/10080 49.9901% craps: 2,6,10,12 naturals: 3,7 out: 10 : 5039/10080 49.9901% craps: 2,4,8,12 naturals: 3,7 out: 4 : 5039/10080 49.9901% craps: 2-3,7,12 naturals: 4,8 out: 4 : 5039/10080 49.9901% craps: 2-3,7,12 naturals: 4,6 out: 4 : 5039/10080 49.9901% craps: 2,4,6,12 naturals: 3,7 out: 4 : 5039/10080 49.9901% craps: 2-3,6 naturals: 4,12 out: 4 : 5039/10080 49.9901% craps: 2,4,7 naturals: 3,6,12 out: 4 : 5039/10080 49.9901% craps: 2,4,7 naturals: 3,8,12 out: 4 : 5039/10080 49.9901% craps: 2-3,8 naturals: 4,12 out: 4 : 5039/10080 49.9901% craps: 2,7,10 naturals: 3,6,12 out: 10 : 5039/10080 49.9901% craps: 2,7,10 naturals: 3,8,12 out: 10 : 5039/10080 49.9901% craps: 2-3,6,11 naturals: 4,8,10 out: 8 : 3563/7128 49.9860% craps: 2-3,6,11 naturals: 4,8,10 out: 6 : 3563/7128 49.9860% craps: 2-3,8,11 naturals: 4,6,10 out: 6 : 3563/7128 49.9860% craps: 2-3,8,11 naturals: 4,6,10 out: 8 : 3563/7128 49.9860% craps: 2,6-7 naturals: 3,5,8-9 out: 8 : 1511/3024 49.9669% craps: 2,6-7 naturals: 3,5,8-9 out: 6 : 1511/3024 49.9669% craps: 2,7-8 naturals: 3,5-6,9 out: 6 : 1511/3024 49.9669% craps: 2,7-8 naturals: 3,5-6,9 out: 8 : 1511/3024 49.9669%  OK, wow, I wasn\u0026rsquo;t expecting that. We\u0026rsquo;re seeing lots of variations that are beating the original game by a wide margin. So the existence question is settled - there absolutely do exist craps variants which are much closer to fair than the original.\nIn fact we note that the top 12 games all achieved the same remarkable score of 49.99%. This relates back to the symmetries I mentioned earlier: in general, craps-variants will belong to an equivalence class of similar games reachable through swaps and reflections about 7 and all games in an equivalence class will have the same probability of winning. But more on that in a future article.\nThe next obvious question is how many craps variants are more fair than the original game?\nsum(p \u0026gt; p_original and p \u0026lt; Fraction(1,2) for _, p, _ in scored_variants)/len(scored_variants)  0.01621000791793194  So of all possible craps variants, only 1.6% are better than the original. So in this sense the original game is exceptionally fair - if I picked a variant at random, then 98.4% is would be as fair or less fair then the original.\nHowever the best possible variants are really quite extraordinarily fair: $\\frac{5039}{10080} \\approx 49.99\\%$ Less than 1 part in 10,000. It\u0026rsquo;s pretty amazing, really, what we can do with a pair of dice and a few simple rules.\nEmpirical Confirmation  Beware of bugs in the above code; I have only proved it correct, not tried it.\n Donald Knuth   The above code was careful to calculate exact probabilities. This was much faster and more accurate than a Monte Carlo simulation; indeed it would have been exorbitant to run a simulation for a quarter million variants and multiple hypothesis testing would have made it very difficult to search for extreme variants such as the \u0026ldquo;fairest\u0026rdquo; variant. However, it\u0026rsquo;s always nice to see empirical results that confirm our theoretical calculations and in this case the simulation code itself is obvious and straight-forward:\nimport random class CrapsSimulation: def __init__(self, craps=[2, 3, 12], naturals=[7, 11], out=7): self.craps = craps self.naturals = naturals self.out = out def reset(self): self.point = None self.roll_log = [] def roll(self): roll = random.randint(1, 6) + random.randint(1, 6) self.roll_log.append(roll) return roll def play(self): self.reset() come_out_roll = self.roll() if come_out_roll in self.craps: return 0 elif come_out_roll in self.naturals: return 1 else: self.point = come_out_roll return self.play_on_point() def play_on_point(self): while True: roll = self.roll() if roll == self.out: return 0 if roll == self.point: return 1  Now we can simply play the game 100 million times to confirm our results. (To get 4 significant figures, we need an error term on the order of $10^{-4}$ but the error terms deceases with the square root of the number of trials so we need about $10^8$ trials to get 4 sig figs.)\nn = int(1e8) original_game = CrapsSimulation(craps=[2, 3, 12], naturals=[7, 11], out=7) fairest_variant = CrapsSimulation(craps=[2, 8, 10, 12], naturals=[3, 7], out=10) outcomes = [] for sim in [original_game, fairest_variant]: win_loss_record = [ sim.play() for _ in range(n) ] p_win = sum(win_loss_record) / len(win_loss_record) outcomes.append(p_win)  [0.49290737, 0.49991584]  Yep - that\u0026rsquo;s the 49.29% and 49.99% we expect for the original and fairest variant respectively.\nConclusion Craps is quite a fair game compared to other common casino games and even compared to 98% of possible craps variants. Nevertheless, my conjecture that craps had been explicitly designed as a maximally fair game was thoroughly disproved when we were able to construct thousands of variants that were all fairer than the original. Some of these were much fairer than the original, deviating from perfectly fair by less than one game in 10,000. While this does take some of the shine off the original game, I am even more impressed with the power of the craps rule set to generate hyper-granular probabilities with only two die.\nOne aspect of craps variants that we didn\u0026rsquo;t explore in detail but which seems promising is the symmetry group of craps variants. Variants can be divided into equivalence classes based which variants can reach each other using only outcome preserving operations. We observe that equivalence classes vary in size quite a bit; for example the original game is part of an equivalence class class with two members while the fairest variant is part of an equivalence class with 12 members, and many other sizes are possible. It seems a moderate and amusing challenge to characterize this group and count the number of equivalence classes of games, but I will have to leave that to a future article.\n","date":"July 11, 2018","href":"https://www.oranlooney.com/post/craps-game-variants/","thumbnail":"/post/craps-game-variants_files/lead.192x128.jpg","title":"Craps Variants"},{"content":"This post is part of a series on complex number functionality in the\rR programming language. You may want to read Part I before continuing if\ryou are not already comfortable with the basics.\nIn Part I of this series, we dipped our toes in the water by explicitly\rcreating some complex numbers and showing how they worked with the most basic\rmathematical operators, functions, and plots.\nIn this second part, we’ll take a more in-depth look at some scenarios where\rcomplex numbers arise naturally – where they are less of a choice an more\rof a necessity. R doesn’t hesitate to return complex numbers from standard\rfunctions when they are the most natural and idiomatic representation, so you\rshould be prepared to deal with that.\nComplex Roots and Eigenvalues\r\rSome problems are specific to complex numbers, some problems can be made\reasier by a complex representation, and some problems have complex numbers\rthrust upon them.\n– William Shakespeare, 12 + 5i Night\n\rOne such case that is of interest to statisticians and scientists (I’m\rassuming you’re not using R for embedded systems or game development) is\rsolving the eignproblem for a non-symmetric matrix.\nNow, if your only exposure to eigenvalues is through PCA, you might not\reven be aware that eigenvalues are usually complex numbers… even when\rthe original matrix is comprised only of real numbers! However\rPCA is actually a very special case: a covariance matrix is always\ra symmetric, positive-definite, real-valued matrix, therefore its\reigenvalues are always positive real numbers.\nHowever, there are plenty of situations in statistics where a non-symmetric\rmatrix arises naturally and the eigenvalues can give us deep insight into\rthe problem. Two such are Markov Chains and AR models. Let’s\ronly look at a simple example of an AR model - that will suffice to\rdemonstrate R’s complex number functionality in this domain.\nLet’s start by constructing a small time series that exhibits very strong\rautocorrelation. To get some interesting behavior, I will give it a strongly\rpositive one day correlation, but then reverse it the next day. This should\rgive us both decay and oscillations.\nset.seed(43)\rt_0 \u0026lt;- zoo(rnorm(n=100))\rt_1 \u0026lt;- lag(t_0, k=1, na.pad=TRUE)\rt_2 \u0026lt;- lag(t_0, k=2, na.pad=TRUE)\rt_3 \u0026lt;- lag(t_0, k=3, na.pad=TRUE)\rt \u0026lt;- na.omit(t_0 + 0.7*t_1 - 0.2*t_2 + 0.2*t_3)\rplot(t, type=\u0026#39;l\u0026#39;)\rtitle(\u0026#39;Time Series With Autocorrelation\u0026#39;)\rpacf(t) # Partial Autocorrelation Plot\rNext we construct the model. While I normally recommend the forecast\rpackage, we’ll just use the built-in ar() function today.\nar_model \u0026lt;- ar(t)\rar_model\r# # Call:\r# ar(x = t)\r# # Coefficients:\r# 1 2 3 4 5 # 0.5078 -0.4062 0.3481 -0.3960 0.2462 # # Order selected 5 sigma^2 estimated as 1.19\rThat’s roughly what we’d expect based on how we constructed the time series and\rwhat we saw on the partial autocorrelation plot: A strong positive\rautocorrelation at lag one, a slightly less strong negative autocorrelation at\rlag 2, then some harmonics.\nar_coefs \u0026lt;- ar_model$ar # coefficients(ar_model) doesn\u0026#39;t work, IDK why\rroots \u0026lt;- polyroot( c(1,-ar_coefs) )\rroots\r# [1] 0.7158218+1.1364815i -0.6823253+0.9974625i -0.6823253-0.9974625i\r# [4] 0.7158218-1.1364815i 1.5417367+0.0000000i\rplot(\r1/roots, ylim=c(-1,1), asp=1,\rmain=\u0026quot;Inverse AR Roots\u0026quot;,\rpanel.first=c(\rlines(complex(modulus=1, argument=0.01*2*pi)^(0:100), col=\u0026#39;grey\u0026#39;),\rabline(h=0, col=\u0026#39;grey\u0026#39;),\rabline(v=0, col=\u0026#39;grey\u0026#39;)\r)\r)\rJust to be clear, we’re plotting the inverse roots, so we’d expect them to be\rinside the unit circle if the process is stationary.\n(Just as an Easter egg, we also used complex numbers to plot the unit circle.\rIf you’re not sure how that worked, just remember that multiplying complex\rnumbers adds their arguments – their angle with the x-axis – together.)\nJust from looking at the roots and observing that some are far from the real\raxis, we can also say that this time series will experience a back-and-forth\roscillations as each day tries to “correct” for the previous day. If the\rinfluence of history merely decayed away smoothly and exponentially, all the\rroots would have been close to the real axis. (It’s a common misconception that\rhow long effects last is related to the order of the model; when in fact even\ran AR(1) model can have a very long memory if it has its root close to 1.)\nPlotting the inverse roots of ARIMA models is standard practice because it can\rhelp you diagnose non-stationary series and near unit roots, both\rof which can ruin the predictive power and interpretability of a model. There’s\rno getting away from the fact that a polynomial of degree two or higher might\rhave complex roots.\nBut there’s another way of looking at an AR model - as a discrete linear\rdynamical system. Let’s call the value of our at the \\(n\\)-th step \\(t_n\\).\rThen we can define our state vectors to be\n\\[\r\\boldsymbol{t}_n = \\begin{bmatrix}\rt_n \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\rt_{n-4} \\\\\r\\end{bmatrix}\r\\]\nIn other words, we just stack \\(t_n\\) with it’s first four lags. That may not\rseem like an improvement, but now we can write\n\\[\r\\boldsymbol{t}_{n+1} =\\boldsymbol{F} \\boldsymbol{t}_n\r\\]\nor more explicitly:\n\\[\r\\begin{bmatrix}\rt_{n+1} \\\\\rt_{n} \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\r\\end{bmatrix} = \\boldsymbol{F}\r\\begin{bmatrix}\rt_n \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\rt_{n-4} \\\\\r\\end{bmatrix}\r\\]\nwhere \\(\\boldsymbol{F}\\) is the “forward time evolution” matrix. This basically\rsays we can always compute the state of our time series at the next time step\rby applying a linear operator to the previous state. And in fact, we already\rhave a good idea what the matrix \\(\\boldsymbol{F}\\) should look like. For one\rthing, it’s clear that the four lagged components can simply be grabbed from\rthe old state by shifting down by one:\n\\[\r\\begin{bmatrix}\rt_{n+1} \\\\\rt_{n} \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\r\\end{bmatrix} = \\begin{bmatrix}\r. \u0026amp; . \u0026amp; . \u0026amp; . \u0026amp; . \\\\\r1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rt_n \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\rt_{n-4} \\\\\r\\end{bmatrix}\r\\]\nAnd from the coefficients of the AR(1) model we built before, we know that\r\\(t_n\\) can be expressed as a linear sum of \\(t_{n-1}\\) through \\(t_{n-4}\\):\n\\[\r\\begin{bmatrix}\rt_{n+1} \\\\\rt_{n} \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\r\\end{bmatrix} = \\begin{bmatrix}\r0.508 \u0026amp; -0.406 \u0026amp; 0.348 \u0026amp; -0.396 \u0026amp; 0.246 \\\\\r1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\\r\\end{bmatrix}\r\\begin{bmatrix}\rt_n \\\\\rt_{n-1} \\\\\rt_{n-2} \\\\\rt_{n-3} \\\\\rt_{n-4} \\\\\r\\end{bmatrix}\r\\]\nSo now that, we’ve determined the linear operator \\(\\boldsymbol{F}\\) for our\rdynamic system, we can ask what happens to the system 2 time-steps into the\rfuture, then 3, and so on. It should be clear that we can simply apply\r\\(\\boldsymbol{F}\\) again and again to determine any future state, so that in\rgeneral the state at time \\(n\\) is\n\\[\r\\boldsymbol{t}_n = \\boldsymbol{F}^n \\boldsymbol{t}_0\r\\]\nBut raising a matrix to a power is particularly easy if we know its\reigenvalues. Let’s say \\(\\boldsymbol{F} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}\\) is the eigen-decomposition, where \\(\\boldsymbol{Q}\\) is an\rorthogonal matrix and \\(\\boldsymbol{\\Lambda}\\) is the diagonal matrix of\reigenvalues. Then\n\\[\r\\boldsymbol{F}^2 = \\boldsymbol{F} \\boldsymbol{F} =\r\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}\r\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}\r= \\boldsymbol{Q} \\boldsymbol{\\Lambda}^2 \\boldsymbol{Q}^{-1}\r\\]\nThis clearly generalizes to any power by induction. Also, raising a diagonal\rmatrix to a power is completely trivial: you simply raise each independent\relement to its power.\n\\[\r\\boldsymbol{\\Lambda}^n = \\begin{bmatrix}\r\\lambda_1^n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; \\lambda_2^n \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; \\lambda_3^n \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\lambda_4^n \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\lambda_5^n\r\\end{bmatrix}\r\\]\nA few things are immediately obvious. Each eigenvalue is a complex number; so\rif its norm is less than 1 it will tend to 0 as \\(n\\) increases, or if its norm\ris greater than 1 it will tend to \\(\\infty\\), or if its norm is exactly 1 it will\ralways be exactly 1. Furthermore, if the eigenvalue is real, it will always be\rreal, but if it is not real then it will rotate about the origin by a fixed\rangle with every time step. Thus, it will exhibit some kind of oscillation with\ra frequency determined by its argument. Each eigenvalue will behave\rindependently, but if every eigenvalue has norm less than 1, then the system\ras a whole will converge to a steady state at 0.\nSo now that I’ve hopefully impressed upon you the importance of eigenvalues is\runderstanding the dynamics of our system, let’s actually compute them. And,\rjust for fun let’s compare them to the roots of the lag polynomial from above.\nar_matrix \u0026lt;- matrix( nrow=5, ncol=5, byrow=TRUE, c(\r0.5078, -0.4062, 0.3481, -0.3960, 0.2462, 1, 0, 0, 0, 0,\r0, 1, 0, 0, 0,\r0, 0, 1, 0, 0,\r0, 0, 0, 1, 0))\rar_eigen \u0026lt;- eigen(ar_matrix)\rdf \u0026lt;- t(rbind(\rdata.frame(t(sort(1/roots))), data.frame(t(sort(ar_eigen$values)))))\rcolnames(df) \u0026lt;- c(\u0026quot;Inverse AR(5) Roots\u0026quot;, \u0026quot;Time Evolution Eigenvalues\u0026quot;)\r\r\rInverse AR(5) Roots\rTime Evolution Eigenvalues\r\r\r\r-0.467 + 0.683i\r-0.467 - 0.683i\r\r-0.467 - 0.683i\r-0.467 + 0.683i\r\r0.397 + 0.630i\r0.397 - 0.630i\r\r0.397 - 0.630i\r0.397 + 0.630i\r\r0.649 - 0.000i\r0.649 + 0.000i\r\r\r\rHey, wait just a minute here! What are you trying to pull here, buddy? Those\rare (to within numerical precision) exactly the same as the inverse roots!\nYes, it’s true. This is very obvious if we plot them together:\nplot(\rar_eigen$values, ylim=c(-1,1), xlim=c(-1,1),\rasp=1,\rcex=2,\rmain=\u0026quot;Inverse AR Roots\u0026quot;,\rpanel.first=c(\rlines(complex(modulus=1, argument=0.01*2*pi)^(0:100), col=\u0026#39;grey\u0026#39;),\rabline(h=0, col=\u0026#39;grey\u0026#39;),\rabline(v=0, col=\u0026#39;grey\u0026#39;)\r)\r)\rpoints(\r1/roots, pch=4,\rcex=2,\rcol=\u0026#39;red\u0026#39;\r)\rThey are exactly the same. You’re welcome to prove this for yourself by writing\rdown the characteristic polynomial for a matrix in this form and verifying it’s\rthe exact same polynomial we found the roots for in the AR formulation of the\rproblem.\nIn fact, you can see the many parallels in the two approaches: in one analysis,\rwe said that an AR model would only be stationary if all its inverse roots were\rinside the unit circle, in the other we said the dynamic system would converge\rto a steady state at the origin. Different language, indeed two historically\rdifferent mathematical treatments, but the same conclusions. In both cases we\rfound that the system was characterized by a sequence of 5 complex numbers, and\rthat both the norm and the argument of each number meaningfully impacted the\rbehavior of the system. And so on.\nThere’s no escaping it: those 5 complex numbers are the best way to\runderstand this system, and any sufficiently sophisticated approach will lead\rus to this same conclusion.\nLet’s just take a moment to realize what happened to us here: we started from a\rdata set entirely comprised of real numbers, built a model with real number\rvalues for all parameters but in the end we still had to understand our model\rin terms of complex numbers.\nThe hard truth is that the real numbers are not closed under many interesting\rand natural operations… if you work with real numbers long enough, you’ll\reventually find yourself in the complex plane.\nLuckily, R really does have excellent support for complex numbers – if nothing\relse, I hope I’ve familiarized you with some of that functionality.\n\r","date":"June 30, 2018","href":"https://www.oranlooney.com/post/complex-r-part-2/","thumbnail":"/post/complex-r-part-2_files/lead.192x128.jpg","title":"Complex Numbers in R, Part II"},{"content":"R, like many scientific programming languages, has first-class support for\rcomplex numbers. And, just as in most other programming languages, this\rfunctionality is ignored by the vast majority of users.\nYet complex numbers can often offer surprisingly elegant formulations and\rsolutions to problems. I want to convince you that familiarizing yourself with\rR’s excellent complex number functionality is well worth the effort and will\rpay off in two different ways: first by showing you how they are so\ramazingly useful you’ll want to go out of your way to use them, and then by\rshowing you how they are so common and fundamental to modern analysis that you\rcouldn’t avoid them if you wanted to.\nPythagorean Triples\rLet’s start with a problem which could be solved in other ways, but is\rgreatly simplified by the introduction of complex numbers that it almost seems\rmagical.\nA Pythagorean triple is an integer solution to the Pythagorean equation:\n\\[\ra^2 + b^2 = c^2 \\quad\\quad a,b,c \\in \\mathbb{N}^+ \\tag{1}\r\\]\nYou probably learned at least one of these in school – the famous 3, 4, 5\rtriangle:\nIn general Diophantine equations – which require integer solutions –\rcan be quite hard to solve, so it might surprise you to hear that it’s almost\rtrivially easy to write down an infinite of Pythagorean triples. Well, it’s\reasy if we use complex numbers, anyway.\nA Gaussian integer is a complex number where both the real and imaginary parts\rare integers. The set of Gaussian integers is denoted by \\(\\mathbb{Z}[i]\\) and\ris defined as:\n\\[\r\\mathbb{Z}[i] = \\{ x + iy \\mid x,y \\in \\mathbb{Z} \\} \\tag{2}\r\\]\nSo one way of stating the problem of finding all Pythagorean triples is to find\rall Gaussian integers which are an integer distance away from the origin. The\rdistance of a complex number from the origin is called its “norm” and denoted\r\\(\\lVert z \\rVert\\). We will call the set of Pythagorean triples \\(T\\) and define\rit as:\n\\[\rT = \\{ z \\in \\mathbb{Z}[i] \\mid \\lVert z \\rVert \\in \\mathbb{Z} \\} \\tag{3}\r\\]\nNow, in general the norm of Gaussian integer will be the square root of an\rinteger (the integer \\(x^2 + y^2\\) to be precise.) Therefore if we square a\rGaussian integer, it will have an integer norm and therefore represent a\rPythagorean triple!\n\\[\r\\forall z \\in \\mathbb{C}, z \\in \\mathbb{Z}[i] \\implies z^2 \\in T \\tag{4}\r\\]\nSo that’s a pretty good start: just a few minutes work, and we’ve already found\ran infinite number of Pythagorean triples, and we have a computationally\rtrivial way of constructing new triples: we simply pick any two positive\rintegers \\(x\\) and \\(y\\) and then square the complex number \\(x + iy\\).\nBefore address the more difficult question of whether or not we’ve found all\rpossible Pythagorean triples using this construction, let’s switch over to R\rand write some code to capture our solution so far.\n\rGaussian Integers in R\rOur algorithm first requires us to pick pairs of positive integers. Just to be\rthorough, we’ll take all such pairs up to an arbitrary threshold.\nNow, if we wanted just one or two complex numbers, we could use the literal syntax:\ntriples \u0026lt;- c( 3+4i, 5+12i, 9+12i )\rBut since want to construct them in bulk, we’ll use the complex() constructor. This\rconstructor is vectorized: by passing in two vectors of equal length we can\rconstruct a one-dimensional vector of complex numbers.\nn = 400\rgrid \u0026lt;- expand.grid(u=1:(2*n), v=1:(2*n))\rgrid \u0026lt;- grid[ grid$u \u0026gt; grid$v, ]\rgaussian_integers \u0026lt;- complex(real=grid$u, imaginary=grid$v)\rPer the theoretical discussion above, we can generate Pythagorean triples by\rsimply squaring these. All primitive math functions in R work just as well on\rcomplex numbers: exp, log, sin, cosand of course the power operator ^:\ntriples \u0026lt;- gaussian_integers^2\r# display the 10 with the smallest norm\rcat( triples[order(Mod(triples))][1:10], sep=\u0026quot;\\n\u0026quot;)\r# 3+4i\r# 8+6i\r# 5+12i\r# 15+8i\r# 12+16i\r# 7+24i\r# 24+10i\r# 21+20i\r# 16+30i\r# 35+12i\rDid it work? We’re certainly seeing some familiar pairings there, like \\(5+12i\\)\rwhich maps to well-known triple \\((5,12,13)\\). To visualize them, we can simply\rpass our complex vector to R’s plot() function – it will conveniently plot\rthem in the complex plane for us!\ntriples \u0026lt;- triples[ Re(triples) \u0026lt;= n \u0026amp; Im(triples) \u0026lt;= n ]\r# helper function to colorize complex points by their angle.\rargcolor \u0026lt;- function(z) hsv(Arg(z)*2/pi, s=0.9, v=0.8)\rplot(\rtriples, col=argcolor(triples),\rpch=20,\rxlim=c(0,n),\rylim=c(0,n),\rmain=paste(\u0026quot;Squared Gaussian Integers Up to\u0026quot;, n)\r)\rNow it turns out that our algorithm does not, in fact, generate all possible\rtriples. For example, multiples are missing: if \\((3,4,5)\\) is a triple, then\r\\((6,8,10)\\) should be a triple, and \\((9,12,15)\\) should be a triple, and so on.\rSo we have to expand our set to have all multiples.\nmultiples \u0026lt;- lapply(1:(floor(n/3)), function(m) triples*m)\rtriples \u0026lt;- unique(do.call(c, multiples))\rIt also turns out that in the special case where both integers are even we can\rdivide by two and get a new triple that was missed by the initial net we cast.\rBut that’s the end of the special cases – with this final rule in place, we’re\rnow guaranteed to hit every Pythagorean triple.\nhalves \u0026lt;- triples[ Re(triples) %% 2 == 0 \u0026amp; Im(triples) %% 2 == 0 ] / 2\rtriples \u0026lt;- unique(c(triples, halves))\rNow all we need to is clean up duplicates and duplicate along the mirror line\rof symmetry…\ntriples \u0026lt;- triples[ Re(triples) \u0026lt;= n \u0026amp; Im(triples) \u0026lt;= n]\rtriples \u0026lt;- c(triples, complex(real=Im(triples), imaginary=Re(triples)))\r..and we’re finally ready to visualize the real solution.\nplot(triples, col=argcolor(triples), pch=20)\rtitle(paste(\u0026quot;All Pythagorean Triples Up to\u0026quot;, n))\r\rA Closer Look\rThat’s too many to really understand, although there are definitely\rpatterns emerging. Let’s zoom in and just plot a small region,but with more\rdetail.\nsmall_n = 25\rsmall_triples \u0026lt;- triples[ Re(triples) \u0026lt; small_n \u0026amp; Im(triples) \u0026lt; small_n ]\rsmall_triples \u0026lt;- small_triples[ order(Mod(small_triples), decreasing=TRUE) ]\r# plot points\rplot(\rsmall_triples, pch=20,\rylim=c(0,small_n), xlim=c(0,small_n),\rylab=\u0026quot;b\u0026quot;, xlab=\u0026quot;a\u0026quot;)\r# add triangles. Can\u0026#39;t rely on automatic complex plane plotting here.\rsegments(\rRe(small_triples), Im(small_triples), 0, 0, col=argcolor(small_triples))\rsegments(\rRe(small_triples), Im(small_triples), Re(small_triples), 0, col=argcolor(small_triples))\rsegments(\rRe(small_triples), 0, 0, 0, col=argcolor(small_triples))\r# points again, so that they\u0026#39;re in the foreground.\rpoints(small_triples, pch=20, col=argcolor(triples), cex=1)\r# text label for the points\rtext(\rx=small_triples + 1i, cex=0.8,\rlabels=paste0(\r\u0026quot;(\u0026quot;, Re(small_triples), \u0026quot;,\u0026quot;,\rIm(small_triples),\r\u0026quot;,\u0026quot;,\rMod(small_triples),\r\u0026quot;)\u0026quot;\r)\r)\rtitle(paste(\u0026quot;Pythagorean Triples Up to\u0026quot;, small_n))\rOn the zoomed in view we can see each Pythagorean triple represented as a right\rtriangle; that the integer multiples of solutions form a series of\rsimilar triangles; and that there’s a strong symmetry with every triple\r\\((a,b,c)\\) having a partner \\((b,a,c)\\) which is its mirror reflection about the\rlike \\(y=x\\).\nFrom the zoomed out view we can see that the region close to either the\rx-axis or the y-axis is essentially devoid of solutions and that it looks\ras if triples actually get less dense as we move away from the origin.\nBy the way, this last observation about triples thinning out as we move away from the\rorigin can be understood and quantified by once again using the complex plane.\rTriples are more or less the squares of Gaussian integers; we can say the\rnumber of triples with norm less than \\(r\\) is roughly proportional to the number\rof Gaussian integers in the first quadrant and inside a circle with radius\r\\(\\sqrt{r}\\), which is roughly proportional to the area of the quarter-circle of\rradius \\(\\sqrt{r}\\), which is \\(\\frac{\\pi r}{4}\\) or very roughly just \\(r\\).\n\rNext Time\rIn this first part of a planned series on complex numbers in R, we dipped our\rtoes in the water by explicitly creating some complex numbers and manipulating them.\rWe demonstrated the most important functions for working specificly with\rcomplex numbers such as Re(), Im(), Mod(), Arg(), and complex() but\rwe emphasized that most built-in functions such as exp() and operators such\ras * and ^ work correctly with complex numbers and implement the natural\ranalytic continuation of their equivalents on the real numbers. Finally, we\rshowcased R’s ability to plot on the complex plane.\nNext time in Part II, we will discuss in more depth some scenarios\rwhere complex numbers arise naturally from the problem itself and cannot\rbe reasonably avoided, while continuing to demostrate more advanced\raspects of R’s complex number functionalty.\n\r","date":"June 17, 2018","href":"https://www.oranlooney.com/post/complex-r/","thumbnail":"/post/complex-r_files/lead.192x128.png","title":"Complex Numbers in R, Part I"},{"content":"Last week my boss stopped by and dropped a brand spanking new iPad on my desk. \u0026quot;Make our application work on this,\u0026quot; he commanded. \u0026quot;You have two days before we demo it at the trade show.\u0026quot; Madness? No, these are web apps! You see, for the last couple years we've been working exclusively on AJAX applications: web pages stuffed with so much JavaScript they look and feel like desktop apps. It's harder than writing desktop software, but if you pull it off you get an application that can be run anywhere, instantly. So, I'm not a \u0026quot;real\u0026quot; iPhone/iPad developer; I've never even seen the dev kit. I just do web apps. Although maybe \u0026quot;just\u0026quot; isn't the right word. We had a 60,000 line application running on a completely new platform the same week it launched. Can your app do that? We already supported Safari, so you'd expect it to work on the iPad too... and it did, mostly, except for a couple of bugs. Flipping the iPad on its side would mess up the layout, double clicks didn't work, stuff like that. Here are the problems we hit, and what we did to solve them. Browser Detect To fix a browser-specific JavaScript bug you need to know when you're on that browser. Detecting the iPad is easy: the iPad user agent contains the unique string \u0026quot;iPad\u0026quot;, so we checked for that. // a function to parse the user agent string; useful for // detecting lots of browsers, not just the iPad. function checkUserAgent(vs) { var pattern = new RegExp(vs, 'i'); return !!pattern.test(navigator.userAgent); } if ( checkUserAgent('iPad') ) { // iPad specific stuff here } Simple enough. Orientation Change Flipping the iPad would also mess up our layouts. iPads use an accelerometer to detect if they're being held vertically or horizontally (or upside down.) From the browser's point of view it looks like the window was resized from 1024x768 to 768x1024; and you get the normal window resize event you'd expect. We use ExtJS, and their Layout framework usually does a great job handling the window resize event on normal browsers. It knew that something had happened but didn't recalculate all the sizes correctly. (To be fair, we use some pretty complicated layouts.) This is probably do to subtle timing issues or some such. I didn't waste much time figuring out what was going on, because there's a way to cut this Gordian knot: the iPad specific orientationchange event. window.onorientationchange = function() { alert(window.orientation); } window.orientation is one of 0, 90, -90, or 180. 0 and 180 are portrait; 90 and -90 are landscape. The onorientationchange event fires whenever it changes. For us, it was sufficient to say viewport.doLayout() on orientation change; that gave ExtJS the hint it needed to get the sizes right. Double Click Safari on the iPad co-opts the double click event for its own use (a local zoom.) You can listen for the dblclick event in JavaScript... it just never fires. That was a bit of a problem, because we'd consistently allowed the user to double click stuff (a row in a grid, for example) to jump to a more detailed view. We settled on a two finger touch gesture to emulate the double click: touch an item in one place and tap it in another. Tapping with two figures simultaneously also works. When I say emulate, I mean that literally: when we detect one of these double touches, we actually tell the DOM to fire a dblclick event. That way, the solution worked everywhere and we didn't have to track down every single place we registered a dblclick listener. document.body.addEventListener('touchstart', function(e) { touch = e.touches[0]; if ( !touch ) return; var me = document.createEvent(\u0026quot;MouseEvents\u0026quot;); me.initMouseEvent('dblclick', true, true, window, 1, // detail / mouse click count touch.screenX, touch.screenY, touch.clientX, touch.clientY, false, false, false, false, // key modifiers 0, // primary mouse button null // related target not used for dblclick event ); touch.target.dispatchEvent(me); }); Unfortunately, this code is too simple. You see, both the iPad and iPhone use \u0026quot;two finger scrolling\u0026quot; for scrollable regions (elements with CSS overflow: scroll;) within a website. There's no way to turn this off, and it doesn't even show a scrollbar: it's two finger scrolling or nothing. If you use the above code as is, the dblclick event will fire as soon as the user tries to use two finger scrolling. We ran into this because we have grids with thousands of rows to scroll from where the user can double click a row to go to a detailed view. To fix this problem, we had to make sure we only fired an emulated dblclick event if the user quickly double tapped a row, not if they pressed and dragged. So instead we only set up a timer in the touchstart event and actually fired the dblclick from touchend. This requires more code, but lets us handle \u0026quot;double clicking\u0026quot; inside of scrollable regions. Nitpicking Pretty much every reviewer has mentioned the iPad's \u0026quot;gorgeous screen,\u0026quot; but it's just a 1024x768 display at fairly low DPI, so I'm not sure what that's about. We test everything down to 800x600 so we were ok, usability-wise, but it sure looks a lot better on a large monitor. We also embed Google maps, using them as backdrops to display geographic data, and the web experience for Google maps is somewhat diminished compared to the full fledged Google Maps iPad app. They still work, though. Safari on the iPad and iPhone doesn't fire mouse events on elements it doesn't consider \u0026quot;clickable\u0026quot;. Registering a click event listener, even if it doesn't do anything, will allow the element to receive the full set of mouse events. Aftermath We made the two day deadline. Did it work? Man, people were stopping by our booth just for the chance to touch an iPad. This was less than two weeks after it launched; there was a lot of curiosity. We use a bunch of data visualization like Google maps and charts, and it was just stunning. It was like a thick, juicy slice of future, served of a piece of future toast. I can't say for sure it generated a sales lead, but it sure brought in some serious foot traffic. But what do you expect? Web apps can and should run anywhere, even on stuff that hasn't been invented yet.  ","date":"April 28, 2010","href":"https://www.oranlooney.com/post/apparently-ipad-developer/","thumbnail":"/post/apparently-ipad-developer/lead.192x128.jpg","title":"So, Apparently I'm an iPad Developer Now"},{"content":"Update 2017-10-23: This article and code library have not kept up with the rapidly changing JavaScript landscape and are now hopelessly out of date. First came non-enumerable properties, and with ES2015 came the introduction of classes, proxies, symbols, and anonymous functions, all of which break the below logic. I'm afraid I no longer know how to fully copy the full menagerie of JavaScript objects while preserving relative references, and it's quite possible that no one else knows either. Nevertheless, the below essay may be of interest if you're interesting in the purely theoretical aspects of deep copying, which can be demonstrated just as well in JavaScript as any other language, so long as you don't go asking tricky questions about the newer features. I've been interested in writing a generic deep copy algorithm in JavaScript for a while. The simple way to make a deep copy in JavaScript is to JSON-serialize and deserialize it (described below) but this approach is very limited. A while ago it occurred to me that that it should be possible to write a fully generic implementation in JavaScript thanks to the language's design. This is interesting because most languages can't do this. In Java, for example, the Cloneable interface is fundamentally shallow because it doesn't pass through enough information to allow cycles to be handled. The Serializable interface does handle cycles but also imposes a huge burden on the user: at best, it's difficult, tedious and error-prone to implement, at worst it may be impossible to serialize an object to disk where it would be simple to create an in-memory copy. Using Serializable for deep copies is a hack. The truth is, Java just doesn't have a generic deep copy mechanism. This is true for many languages. So it would be pretty cool if we could write one for JavaScript, huh? The Three Copies This essay presents a recursive deep copy algorithm for JavaScript that can handle cycles. It works with all non-object types and the standard classes: Arrays, Objects, and Dates, as well as HTML DOM Nodes. It can be extended with custom behavior for any class. It's published under the LGPL, which means you can include it in your open-source or commercial software without charge. Source: olooney/deep-copy-js. The script installs three functions in the namespace owl, all variants on copying an object: copy(), clone(), and deepCopy(). Example usage: john = { name: 'John Smith', hobbies: ['surfing', 'diving'] }; // clone john2 = owl.clone(john); clone() uses JavaScript's built-in prototype mechanism to create a cheap, shallow copy of a single Object. It is described in detail in a separate essay. It's used internally by copy() and deepCopy() but won't be mentioned here again. // shallow copy john3 = owl.copy(john); // john and john3 have separate names, // but share the same hobbies Array: john.hobbies === john3.hobbies; copy() makes a shallow, non-recursive copy of a single object. This implementation is interesting because it handles native types and correctly copies objects created by a user-defined class. I've written about user-defined classes elsewhere and you can read the source code for details on how that works. Shallow copy() is only included for contrast and won't be mentioned here again. // deep copy john4 = owl.deepCopy(john); There we go! deepCopy() is the entry point for the deep copy algorithm. Every member is recursively deep copied: // john and john4 have separate hobby arrays john.hobbies !== john4.hobbies // which can be manipulated separately: john4.hobbies.push('sailing'); john.hobbies.length === 2; john4.hobbies.length === 3; If there are cyclic references: john = { name: 'John Smith', hobbies: ['surfing', 'diving'], friends: [] }; bob = { name: 'Bob Boston', hobbies: ['rowing', 'surfing'], friends: [ john ] } john.friends.push(bob); they'll be handled correctly; the algorithm will not go into an infinite loop, and the set of copied objects will have the same graph structure, including cycles, as the original: john2 = owl.deepCopy(john); bob2 = john.friends[0]; // bob was included in the deep copy, // so now we have another bob. bob2 !== bob; // john2 and bob2 have the same cyclic // relationship as john and bob. bob2.friends[0] === john2;  How It Works At the heart, there's a recursive algorithm that descends through the graph of objects, copying each one. As it goes, it makes a record of each object that it copies, in the form of an ordered pair: [original, copy]. If it ever sees an object it has already copied before, it does not perform a second deep copy, but immediately returns to the copy it already made. Detecting objects that have already been copied is the key to avoiding infinite recursion. Using the same copy as the first time is the key to preserving cycles. We only keep track of previously copied objects over a single pass of the algorithm, where a single pass is a single call to the global deepCopy() algorithm. If you call deepCopy() on the same object later, it won't remember that it's been copied before, and you'll get yet another copy of it. However, the global deepCopy() algorithm is reentrant because a different object is created to keep track of each pass instead of using static data. This isn't terribly important because JavaScript is single-threaded but could still prevent a subtle bug someday. Unfortunately, we have to use an array to keep track of the [original, copy] pairs, and we have to search through that array linearly each time. Objects are unordered (o1 \u0026lt; o2 and o2 \u0026lt; o1 always return false for any two Objects o1 and o2) can't be used as keys in some kind of lookup Object, and don't expose a unique address or id that could be ordered. This is unfortunate because it means the algorithm as a whole is O(n2) when it could be O(n log(n)) if Objects could be ordered or hashed in some way, but I just don't think that's possible. We also keep track of our current \u0026quot;depth:\u0026quot; the number of times deepCopy() has recursively called back into itself. If that hits the max depth of 256, it will abort and throw an Error. You can change the depth limit by passing in a second argument to deepCopy(): john2 = owl.deepCopy(john, 5); You'll probably want to reduce this to detect errors early, rather than increase it. To paraphrase Bill Gates, 256 levels should be enough for anyone. In fact, except when copying DOM nodes, you probably won't get out of the single digits. Exactly how we copy a given object depends on its class. This is handled by a set of objects called \u0026quot;copiers\u0026quot; that are responsible for copying specific kinds of objects. For each object that we copy, we determine which copier to use and delegate all the specifics of copying to it. New copiers can be added at any time. This makes the algorithm extensible and customizable. For more details on the implementation, please refer to the source code directly. Registering Copiers Copier implementations are provided for standard JavaScript classes and types. The mechanism is extensible: you can add copiers for your own classes. As an example, let's take a look at the Array copier: // Array copier deepCopy.register({ canCopy: function(source) { return ( source instanceof Array ); }, create: function(source) { return new source.constructor(); }, populate: function(deepCopy, source, result) { for ( var i=0; i\u0026lt;source.length; i++) { result.push( deepCopy(source[i]) ); } return result; } }); Every copier must have the three methods show here, and can be added to the registry using deepCopy.register() as shown. Copiers registered later are checked first and therefore have higher priority. Let's examine the three methods in turn. canCopy() returns a Boolean indicating if this copier is able to handle a given object. It is invoked for each copier in the registry , starting with the most recently registered copier and working backwards until one of them returns true. Only if it returns true will the other two methods be called. Typically this will be an instanceof check as shown, but any logic is allowed. create() returns a new object of the appropriate type. This is important, because the hidden internal prototype of each object can only be set at creation. You can perform other setup here if you choose, but you are not allowed to perform a recursive deep copy. The reason for this is simple: until you return the copy, the algorithm will not be able to record the [original, copy] pair needed to handle cycles. Use this method only to initialize a new, empty object of the correct class and leave all other initialization until populate(). populate() is called immediately after create(). It is passed a reference to a deepCopy() function... but this is not the global deepCopy() function. Instead, it a closure that is aware of the previous copies and can avoid cycles; otherwise, it is the same. populate() is also passed the original object and the empty copy made by create(). Use this function to recursively deep copy members. There are copiers already registered for Objects, Arrays, Dates, and HTML DOM Nodes. The algorithm handles non-object types automatically, since these are all copy-by-value by definition. The Default Copier The default, generic deep copy does a reasonable job of copying objects of user-defined classes too. They are detected by this test: obj instanceof obj.constructor This requires you to have correctly overridden the constructor property, and to have actually used the new operator on that constructor to obtain the object. This will be true if you're following the advice from my essay on Classes and Objects in JavaScript, or using any standard framework to define your classes. If that condition is met, then the constructor's prototype is cloned, and the object's instance properties are deep copied, one by one, into the clone. The result should be an object of the same class as the original with a deep copy of all instance data. Static (class/prototype level) data is not deep copied, but shared, just as it is shared between normal instances of the class. The copy will be an instance of the original's class. However, the constructor is NOT called. This often, but not always, results in a high-quality, low-overhead copy. You can register a custom copier to handle cases where this default behaviour is not correct. The generic object copier is always called for objects of type \u0026quot;object\u0026quot; if no other more specific copier claims to be able to copy the object. Notice that it preserves the class, methods, and static members of the object and only copies the instance-level members of the object. My earlier essays on clone() and Classes and Objects might help you understand exactly what's going on here, but the point is that it will \u0026quot;just work\u0026quot; for most classes: you don't need to register a custom copier for every, or even most, of your own classes. FAQ  Q: I don't think I need any of this stuff. I just want the Arrays in my Objects and the Objects in my Arrays to be copied too. Is there an easier way? A: Yes. If the data structure you want to copy can be serialized to JSON, then you can make a deep copy by serializing and deserializing it. For example, using JSON.stringify(), write var b = JSON.parse(JSON.stringify(a));  The limitation of this approach is that you won't be able to handle reference cycles, user-defined classes, or standard Date objects (Date isn't part of the JSON standard.) The advantage is that it's very reliable and doesn't introduce any new dependencies since it's universally available across modern browsers. \u0026nbsp; Q: How do I know if a class needs a custom copier? A: Look for special constructor behavior or uniqueness conditions, or for properties that should not be deep copied. For example, a class with a unique id would need a custom copier that generated a new id for the copy. Or, the object itself might be some globally unique Singleton. Or, it might also register itself with some global manager in the constructor. Or, it might have a reference to some shared object, like document.body, that you don't want to pull into the copy. Basically, the deep copy works best on native types and simple classes, which are mostly data, with maybe a few methods to access that data. For example, a Point(x, y) class, with a few methods like length(), or a Rectangle(a, b) class defined by two Points and having methods like area(). That would deep copy just fine. But a fancy class like Ext.Window, which register with an global Ext.WindowMgr to manage their relative z-indexes, would need a custom copier. \u0026nbsp; Q: How are Functions copied? A: They aren't: deepCopy() just returns a reference to the original function. A Function's behaviour is immutable: you can't change the code of a function after initial creation, and there's no reason to make a copy of an immutable object. It is possible to set properties on a function though, so in that sense functions are mutable. This isn't very common, and when it is used, such as for prototype for class constructors, the correct behavior is usually still to not copy. If you really want to copy a function though, you can use something like the wrap() function explained this essay. \u0026nbsp; Q: Singletons are classes that should only exist once \u0026mdash; for example, a cache or a registry. How can I make deepCopy() respect singletons? A: Register a copier for the Singleton class that returns the original object from create() and does nothing in populate(). Here is the complete pattern for a class called MySingleton: owl.deepCopy.register({ canCopy: function(obj) { return obj instanceof MySingleton; }, create: function(obj) { return obj; } }); Q: My class requires a collection to be passed into the constructor, so it's impossible to break the copying up into two stages. A: It's always possible because all properties are public in JavaScript except for the hidden prototype. You can change every property of an object after creation. I suppose there might be native-code objects (objects provided by the browser, not based on Object) that can't be deep copied, but I don't know of any. Some classes can't be copied via their public interface, though. This is why copying behavior is typically left up to each class to implement. Here, to avoid namespace conflicts, we put it in a separate copier, but it really is logically part of the class. If that bothers you, just think of the copier as a friend of the class. \u0026nbsp; Q: Why doesn't the copier for DOM Nodes just call cloneNode(true)? Wouldn't that be a deep copy? A: cloneNode(true) wouldn't preserve the reference structure with the rest of the copy. Suppose you were implementing round corners with several nested divs and had an object keeping track of them all: roundyCornerBox = { outer: outerDiv, header: headerDiv, footer: footerDiv, body: contentP };  In the original, header, footer, and body are children of outer. That needs to be true of the copy, too, and wouldn't be if we used cloneNode(true). \u0026nbsp; Q: I'm keeping data around as custom properties on HTML DOM Nodes, and I've noticed this doesn't get copied. Why not? A: Nodes have hundreds of properties and there's no way to distinguish between custom and standard ones. Since cloneNode() doesn't preserve custom properties, we'd need to do hundreds of checks per element. Since most elements don't have custom properties it seems kind of wasteful. However, some JavaScript frameworks rely on this. So, you can implement this yourself, by adding something like this to populate(): for ( var key in source ) { if ( !(key in result) ) { result[ deepCopy(key) ] = deepCopy( source[key] ); } }  ","date":"November 25, 2009","href":"https://www.oranlooney.com/post/deep-copy-javascript/","thumbnail":"/post/deep-copy-javascript/lead.192x128.jpg","title":"Deep Copy in JavaScript"},{"content":"  se-man-tic (si-man\u0026rsquo;tik) adj. \u0026nbsp; \u0026nbsp; 1. Of or relating to meaning, especially meaning in language.\n Programming destroys meaning. When we program, we first replace concepts with symbols and then replace those symbols with arbitrary codes \u0026mdash; that\u0026rsquo;s why it\u0026rsquo;s called coding.\nAt its worst programming is write-only: the program accomplishes a task, but is incomprehensible to humans. See, for example, the story of Mel. Such a program is correct, yet at the same time meaningless.\nSemantic Functions The opposite of write-only programming is semantic programming: writing code that has meaning encoded into it. Let\u0026rsquo;s take an example from C: strcpy(). Instead of calling strcpy(), you could write this:\n while( *p++ = *q++ );  A good programmer will be able to puzzle out what you meant if he or she is familiar with pointers and null-terminated strings. If instead we call strcpy(), equivalent code is executed but we\u0026rsquo;ve also made in clear that we mean to copy a string. strcpy() is more than just a block of code to execute; it is also a symbol with meaning, and when we use it we add meaning to the program.\nThis becomes important when the implementation is not perfect, as it never is. Consider this alternative code snippet:\n while( *q ) *p++ = *q++;  Which behaves identically to the above snippet for almost all inputs. Which one is correct? What did the programmer intend? Did he or she do it on purpose? There\u0026rsquo;s no way to know from the code; all semantic meaning is gone.\nFunctions like strcpy() are semantic symbols, and as such allow you to inject meaning directly into your code\u0026mdash;not into the comments or even the variable names, but the structure of the code itself.\nLikewise, you can create new semantic symbols by writing functions that do one \u0026mdash; and only one \u0026mdash; thing and giving them a descriptive names.\n Functions should be short and sweet, and do just one thing. - Linus Torvalds\n The Linux Coding Style guide contains good, practical advice on how to write meaningful functions. A program built out of such semantic functions will be more meaningful, hence more readable and understandable.\nSemantic Methods In the object-oriented world, semantic programing means providing methods with good semantics. An Array class doesn\u0026rsquo;t need to provide a method for accessing the last element, because programmers could simply write:\n myArray[myArray.length-1]  and still be guaranteed constant-time access. But this isn\u0026rsquo;t semantic; you\u0026rsquo;re saying how to do something, instead of what you want to do, programming procedurally instead of declaratively. It would be better to be able to write:\n myArray.last()  How is it better, you may ask? Well, how should this code behave for an empty array? With the last() method that decision is encapsulated in the Array class. Even functions \u0026ldquo;too simple to screw up\u0026rdquo; can have edge cases anyone can miss when slamming out \u0026ldquo;just one line of code.\u0026rdquo;\nThe client is busy trying to solve their own problem. Having to write a even a simple algorithm (take the length, subtract one, get the element at that index) to get the last element is a distraction. That shoud be SEP: Somebody Else\u0026rsquo;s Problem. (Specifically, Array\u0026rsquo;s implementer.)\nAnother example is the .empty() method provided for containers in the C++ STL. Why not simply compare .length() to 0? Because not all containers can compute their length in constant time. list\u0026lt;\u0026gt;, which is implemented as a bi-directionally linked-list, must walk from the start node to the end code in to determine its own length. However, to determine if it contains at least one node takes only constant time.\nThe principle is the same: let the client define what to do, and let the object figure out how to do it. The \u0026ldquo;how\u0026rdquo; might be different between different implementations of the interface, and it might change over time; therefore it should be encapsulated in the object. By providing the semantic .empty() method, STL contains encapsulate that behavior and provide practical performance and maintainence advantages.\nTo enable the client to declare what needs to be done without any how, we need to write semantic methods, like .last() and .empty(), that have clear, meaningful responsibilities. Doing so makes it easier for programmers to learn the object\u0026rsquo;s API, makes the client code simplier and more declarative, and improves encapsulation.\nImplementing Semantic Objects So, semantic methods are useful to the object\u0026rsquo;s client, because they don\u0026rsquo;t have to think about implementations, but can simply say what they want and let the object provide it. That\u0026rsquo;s useful to the client, but doesn\u0026rsquo;t it impose a burden on the classes implementor?\nNo. The reverse is the case; providing semantic methods gives the implementor great freedom to change the underlying implementation and prevents the object from being pushed into a passive, \u0026ldquo;data\u0026rdquo; role.\nSuppose a class provides various \u0026ldquo;get\u0026rdquo; methods \u0026mdash; getName(), getAge(), getGender(), and so on \u0026mdash; but no getDescription() method. The client can certainly construct a string representation of the object, but the class has no control over it\u0026hellip; in particular it has no way to update those cobbled-together descriptions when the class changes and new fields are added. In general, we never want the client to have to write procedural code acting on our object, and should provide semantic methods they can call instead.\n(C++ programmers may be familar with the idea of writing non-method, non-friend functions to provide convenience functionality that can be implemented in terms of the classes public interface. That\u0026rsquo;s fine; the important thing is to provide the semantics along with the object, so what I\u0026rsquo;ve said about methods applies here too.)\nProgramming destroys meaning. However, this destruction does not need to be wholesale. With a little thought we can preserve much of the meaning. Such \u0026ldquo;semantic code\u0026rdquo; can be understood, re-used, debugged, and modified far more easily than \u0026ldquo;write-only code.\u0026rdquo;\n","date":"April 30, 2008","href":"https://www.oranlooney.com/post/semantic-code/","thumbnail":"/post/semantic-code_files/lead.192x128.jpg","title":"Semantic Code"}]