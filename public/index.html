<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OranLooney.com</title>
  <meta property="og:title" content="OranLooney.com" />
  <meta name="twitter:title" content="OranLooney.com" />
  <meta name="author" content="Oran Looney"/>

  <meta name="generator" content="Hugo 0.42.1" />
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="OranLooney.com" />
  <link href="/index.xml" rel="feed" type="application/rss+xml" title="OranLooney.com" />
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous" async></script>

  <script defer src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" integrity="sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc" crossorigin="anonymous" async></script>

  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-2535855-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-2535855-1');
  </script>

 
</head>

<body>
<header class="site-header">
  <nav class="site-navi">
    <a href="/" class="site-title">OWL</a>
    <ul class="site-navi-items">
      <li class="site-navi-item">
        <a href="/tags/" title="Article Tags"><i class="fa fa-tag"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/archives/" title="Article Archives"><i class="fa fa-archive"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/quotes/" title="Favorite Quotes"><i class="fas fa-quote-right"></i></a>
      </li>
      <li class="site-navi-item">
        <a href="/about/" title="About Me"><i class="fa fa-info-circle"></i></a>
      </li>
    </ul>
    
  <ul class="author-social">
    <li><a href="//honeycode.tumblr.com/" target="_blank" title="Honeycode Microblog">
      <svg class="svg-inline--fa fa-w-12" aria-hidden="true" data-prefix="fab" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="20 50 350 450" data-fa-i2svg="">
<path fill="currentColor" d="M193 97l86.6 50v100l-86.6 50l-86.6 -50v-100zM285 255l86.6 50v100l-86.6 50l-86.6 -50v-100zM100 255l86.6 50v100l-86.6 50l-86.6 -50v-100z"></path>
</svg>
    </a></li>
    <li><a href="//linkedin.com/in/oran-looney" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a></li>
    <li><a href="https://github.com/olooney" target="_blank"  title="github"><i class="fab fa-github"></i></a></li>
    <li><a href="https://stackoverflow.com/users/273231/olooney" target="_blank" title="StackOverflow"><i class="fab fa-stack-overflow"></i></a></li>
    <li><a href="https://stats.stackexchange.com/users/48250/olooney" target="_blank" title="CrossValidated"><i class="fa fa-flask"></i></a></li>

    <li>
      <a href="https://www.librarything.com/catalog.php?view=olooney&amp;offset=0&amp;shelf_rows=10&amp;previousOffset=0&amp;shelf=shelf" target="_blank" title="LibraryThing">
	    <i class="fas fa-book-reader"></i>
	  </a>
	</li>
    
  </ul>

  </nav>
</header>


  <div class="main" role="main">
    <section class="list home-list">
      <article class="article">
        <a href="/post/eight-billion/" class="article-titles">
          <h2 class="article-title">Eight Billion People</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 14, 2022</time></li>
          <li class="article-meta-tags">
            <a href="/tags/future/">
              <i class="fas fa-tag"></i>
              Future
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/eight-billion/"><img src="/post/eight-billion_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Today is the last day when the number of people alive will start with a seven. Sometime late Tuesday afternoon, or perhaps early Wednesday morning, the population will cross the eight billion mark. When I was a kid, the number they taught us in school was five billion. By the time I was in college, it was up to six, and a decade ago it hit seven.
Now it&rsquo;s eight.
        </div>
        <div class="article-readmore"><a href="/post/eight-billion/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-6-pca/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 6: Principal Component Analysis</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>September 16, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-6-pca/"><img src="/post/ml-from-scratch-part-6-pca_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In the previous article in this series we distinguishedbetween two kinds of unsupervised learning (cluster analysis and dimensionalityreduction) and discussed the former in some detail. In this installment we turnour attention to the later.
In dimensionality reduction we seek a function \(f : \mathbb{R}^n \mapsto \mathbb{R}^m\) where \(n\) is the dimension of the original data \(\mathbf{X}\) and\(m\) is less than or equal to \(n\). That is, we want to map some high dimensionalspace into some lower dimensional space.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-6-pca/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/slow-fibonacci/" class="article-titles">
          <h2 class="article-title">A Seriously Slow Fibonacci Function</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>July 6, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/slow-fibonacci/"><img src="/post/slow-fibonacci_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          I recently wrote an article which was ostensibly about the Fibonacciseries but was really about optimization techniques. I wanted to follow up onits (extremely moderate) success by going in the exact opposite direction:by writing a Fibonacci function which is as slow as possible.
This is not as easy as it sounds: any program can trivially be made slower,but this is boring. How can we make it slow in a fair and interesting way?
        </div>
        <div class="article-readmore"><a href="/post/slow-fibonacci/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-5-gmm/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 5: Gaussian Mixture Models</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>June 5, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-5-gmm/"><img src="/post/ml-from-scratch-part-5-gmm_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Consider the following motivating dataset:
Unlabled Data
 It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. The question is: is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-5-gmm/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/adaptive-basis-functions/" class="article-titles">
          <h2 class="article-title">Adaptive Basis Functions</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>May 21, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/adaptive-basis-functions/"><img src="/post/adaptive-basis-functions_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Today, let me be vague. No statistics, no algorithms, no proofs. Instead,we’re going to go through a series of examples and eyeball a suggestiveseries of charts, which will imply a certain conclusion, without actuallyproving anything; but which will, I hope, provide useful intuition.
The premise is this:
For any given problem, there exists learned featured representationswhich are better than any fixed/human-engineered set of features, even oncethe cost of the added parameters necessary to also learn the new features into account.
        </div>
        <div class="article-readmore"><a href="/post/adaptive-basis-functions/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-4-decision-tree/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 4: Decision Trees</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>March 1, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-4-decision-tree/"><img src="/post/ml-from-scratch-part-4-decision-tree_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          So far in this series we’ve followed one particular thread: linear regression-&gt; logistic regression -&gt; neural network. This is a very natural progression ofideas, but it really represents only one possible approach. Today we’ll switchgears and look at a model with completely different pedigree: the decisiontree, sometimes also referred to as Classification and RegressionTrees, or simply CART models. In contrast to the earlier progression,decision trees are designed from the start to represent non-linear features andinteractions.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-4-decision-tree/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/fibonacci/" class="article-titles">
          <h2 class="article-title">A Fairly Fast Fibonacci Function</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 19, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/c&#43;&#43;/">
              <i class="fas fa-tag"></i>
              C&#43;&#43;
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/fibonacci/"><img src="/post/fibonacci_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          A common example of recursion is the function to calculate the \(n\)-th Fibonacci number:
def naive_fib(n):if n &lt; 2:return nelse:return naive_fib(n-1) + naive_fib(n-2)This follows the mathematical definition very closely but it’s performance isterrible: roughly \(\mathcal{O}(2^n)\). This is commonly patched up with dynamicprogramming. Specifically, either the memoization:
from functools import lru_cache@lru_cache(100)def memoized_fib(n):if n &lt; 2:return nelse:return memoized_fib(n-1) + memoized_fib(n-2)or tabulation:
        </div>
        <div class="article-readmore"><a href="/post/fibonacci/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-3-backpropagation/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 3: Backpropagation</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 3, 2019</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-3-backpropagation/"><img src="/post/ml-from-scratch-part-3-backpropagation_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In today’s installment of Machine Learning From Scratch we’ll build on the logistic regression from last time to create a classifier which is able to automatically represent non-linear relationships and interactions between features: the neural network. In particular I want to focus on one central algorithm which allows us to apply gradient descent to deep neural networks: the backpropagation algorithm. The history of this algorithm appears to be somewhat complex (as you can hear from Yann LeCun himself in this 2018 interview) but luckily for us the algorithm in its modern form is not difficult - although it does require a solid handle on linear algebra and calculus.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-3-backpropagation/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-2-logistic-regression/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 2: Logistic Regression</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>December 27, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-2-logistic-regression/"><img src="/post/ml-from-scratch-part-2-logistic-regression_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          In this second installment of the machine learning from scratchwe switch the point of view from regression to classification: instead ofestimating a number, we will be trying to guess which of 2 possible classes agiven input belongs to. A modern example is looking at a photo and deciding ifits a cat or a dog.
In practice, its extremely common to need to decide between \(k\) classes where\(k &gt; 2\) but in this article we’ll limit ourselves to just two classes - theso-called binary classification problem - because generalizations to manyclasses are usually both tedious and straight-forward.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-2-logistic-regression/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-1-linear-regression/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 1: Linear Regression</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 29, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-1-linear-regression/"><img src="/post/ml-from-scratch-part-1-linear-regression_files/lead.192x128.png" class="article-image" /></a>
        <div class="article-content">
          To kick off this series, will start with something simple yet foundational:linear regression via ordinary least squares.
While not exciting, linear regression finds widespread use both as a standalonelearning algorithm and as a building block in more advanced learningalgorithms. The output layer of a deep neural network trained for regressionwith MSE loss, simple AR time series models, and the “local regression” part ofLOWESS smoothing are all examples of linear regression being used as aningredient in a more sophisticated model.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-1-linear-regression/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/ml-from-scratch-part-0-introduction/" class="article-titles">
          <h2 class="article-title">ML From Scratch, Part 0: Introduction</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 11, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/from-scratch/">
              <i class="fas fa-tag"></i>
              From Scratch
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/ml-from-scratch-part-0-introduction/"><img src="/post/ml-from-scratch-part-0-introduction_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Motivation“As an apprentice, every new magician must prove to his own satisfaction, atleast once, that there is truly great power in magic.” - The Flying Sorcerers,by David Gerrold and Larry Niven
How do you know if you really understand something? You could just rely onthe subjective experience of feeling like you understand. This soundsplausible - surely you of all people should know, right? But this runshead-first into in the Dunning-Kruger effect.
        </div>
        <div class="article-readmore"><a href="/post/ml-from-scratch-part-0-introduction/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/viz-tsne/" class="article-titles">
          <h2 class="article-title">Visualizing Multiclass Classification Results</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>August 23, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/r/">
              <i class="fas fa-tag"></i>
              R
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/machine-learning/">
              <i class="fas fa-tag"></i>
              Machine Learning
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/visualization/">
              <i class="fas fa-tag"></i>
              Visualization
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/viz-tsne/"><img src="/post/viz-tsne_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Introduction Visualizing the results of a binary classifier is already a challenge, but having more than two classes aggravates the matter considerably.
Let&rsquo;s say we have $k$ classes. Then for each observation, there is one correct prediction and $k-1$ possible incorrect prediction. Instead of a $2 \times 2$ confusion matrix, we have a $k^2$ possibilities. Instead of having two kinds of error, false positives and false negatives, we have $k(k-1)$ kinds of errors.
        </div>
        <div class="article-readmore"><a href="/post/viz-tsne/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/craps-game-variants/" class="article-titles">
          <h2 class="article-title">Craps Variants</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>July 11, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/statistics/">
              <i class="fas fa-tag"></i>
              Statistics
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/craps-game-variants/"><img src="/post/craps-game-variants_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Craps is a suprisingly fair game. I remember calculating the probability of winning craps for the first time in an undergraduate discrete math class: I went back through my calculations several times, certain there was a mistake somewhere. How could it be closer than $\frac{1}{36}$?
(Spoiler Warning If you haven&rsquo;t calculated these odds for yourself then you may want to do so before reading further. I&rsquo;m about to spoil it for you rather thoroughly in the name of exploring a more general case.
        </div>
        <div class="article-readmore"><a href="/post/craps-game-variants/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/complex-r-part-2/" class="article-titles">
          <h2 class="article-title">Complex Numbers in R, Part II</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>June 30, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/r/">
              <i class="fas fa-tag"></i>
              R
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/complex-r-part-2/"><img src="/post/complex-r-part-2_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          This post is part of a series on complex number functionality in theR programming language. You may want to read Part I before continuing ifyou are not already comfortable with the basics.
In Part I of this series, we dipped our toes in the water by explicitlycreating some complex numbers and showing how they worked with the most basicmathematical operators, functions, and plots.
In this second part, we’ll take a more in-depth look at some scenarios wherecomplex numbers arise naturally – where they are less of a choice an moreof a necessity.
        </div>
        <div class="article-readmore"><a href="/post/complex-r-part-2/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/complex-r/" class="article-titles">
          <h2 class="article-title">Complex Numbers in R, Part I</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>June 17, 2018</time></li>
          <li class="article-meta-tags">
            <a href="/tags/r/">
              <i class="fas fa-tag"></i>
              R
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/math/">
              <i class="fas fa-tag"></i>
              Math
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/complex-r/"><img src="/post/complex-r_files/lead.192x128.png" class="article-image" /></a>
        <div class="article-content">
          R, like many scientific programming languages, has first-class support forcomplex numbers. And, just as in most other programming languages, thisfunctionality is ignored by the vast majority of users.
Yet complex numbers can often offer surprisingly elegant formulations andsolutions to problems. I want to convince you that familiarizing yourself withR’s excellent complex number functionality is well worth the effort and willpay off in two different ways: first by showing you how they are soamazingly useful you’ll want to go out of your way to use them, and then byshowing you how they are so common and fundamental to modern analysis that youcouldn’t avoid them if you wanted to.
        </div>
        <div class="article-readmore"><a href="/post/complex-r/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/apparently-ipad-developer/" class="article-titles">
          <h2 class="article-title">So, Apparently I&#39;m an iPad Developer Now</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 28, 2010</time></li>
          <li class="article-meta-tags">
            <a href="/tags/web/">
              <i class="fas fa-tag"></i>
              Web
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/javascript/">
              <i class="fas fa-tag"></i>
              JavaScript
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/apparently-ipad-developer/"><img src="/post/apparently-ipad-developer/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Last week my boss stopped by and dropped a brand spanking new iPad on my desk. &quot;Make our application work on this,&quot; he commanded. &quot;You have two days before we demo it at the trade show.&quot; Madness? No, these are web apps! You see, for the last couple years we've been working exclusively on AJAX applications: web pages stuffed with so much JavaScript they look and feel like desktop apps. It's harder than writing desktop software, but if you pull it off you get an application that can be run anywhere, instantly.
        </div>
        <div class="article-readmore"><a href="/post/apparently-ipad-developer/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/deep-copy-javascript/" class="article-titles">
          <h2 class="article-title">Deep Copy in JavaScript</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>November 25, 2009</time></li>
          <li class="article-meta-tags">
            <a href="/tags/javascript/">
              <i class="fas fa-tag"></i>
              JavaScript
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/deep-copy-javascript/"><img src="/post/deep-copy-javascript/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          Update 2017-10-23: This article and code library have not kept up with the rapidly changing JavaScript landscape and are now hopelessly out of date. First came non-enumerable properties, and with ES2015 came the introduction of classes, proxies, symbols, and anonymous functions, all of which break the below logic. I'm afraid I no longer know how to fully copy the full menagerie of JavaScript objects while preserving relative references, and it's quite possible that no one else knows either.
        </div>
        <div class="article-readmore"><a href="/post/deep-copy-javascript/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/post/semantic-code/" class="article-titles">
          <h2 class="article-title">Semantic Code</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 30, 2008</time></li>
          <li class="article-meta-tags">
            <a href="/tags/python/">
              <i class="fas fa-tag"></i>
              Python
            </a>&nbsp;
          </li>
        </ul>
        <a href="/post/semantic-code/"><img src="/post/semantic-code_files/lead.192x128.jpg" class="article-image" /></a>
        <div class="article-content">
          se-man-tic (si-man&rsquo;tik) adj. &nbsp; &nbsp; 1. Of or relating to meaning, especially meaning in language.
 Programming destroys meaning. When we program, we first replace concepts with symbols and then replace those symbols with arbitrary codes &mdash; that&rsquo;s why it&rsquo;s called coding.
At its worst programming is write-only: the program accomplishes a task, but is incomprehensible to humans. See, for example, the story of Mel. Such a program is correct, yet at the same time meaningless.
        </div>
        <div class="article-readmore"><a href="/post/semantic-code/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
    </section>
    

  </div>



<div class="site-footer">
  <div class="copyright">
	  &copy; Copyright 2022 Oran Looney
  </div>
  <ul class="site-footer-items">
    <li class="site-footer-item-rsslink">
      <a href="/index.xml" type="application/rss+xml" target="_blank" title="RSS">
        <i class="fas fa-rss"></i>
      </a>
    </li>
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
  </div>
</div>
<script src="/js/script.js"></script>
<script src="/js/custom.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ["\\[","\\]"] ],  // ['$$','$$'], 
    processEscapes: true,
    processEnvironments: true
  },
  // Center justify equations in code and markdown cells. Elsewhere
  // we use CSS to left justify single line equations in code cells.
  displayAlign: 'center',
  "HTML-CSS": {
    styles: {'.MathJax_Display': {"margin": 0}},
    linebreaks: { automatic: true }
  },
  TeX: { extensions: ["color.js"] }
});
</script>


<link rel="stylesheet"
	href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
  hljs.configure({
    languages: ['python', 'r', 'javascript']
  })
  hljs.initHighlightingOnLoad()
</script>



</body>
</html>
